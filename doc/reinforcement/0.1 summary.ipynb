{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "2 steps <br>\n",
    "    1 Iterative Policy Evaluation <br>\n",
    "    2 Policy Improvement <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v += action_prob * (reward + discount_factor * prob * V[next_state])\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            \n",
    "            # **** update value function ****\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "        break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reshaped Grid (rounded) Value Function:\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-1. -2. -2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)\n",
    "print(\"\\nReshaped Grid (rounded) Value Function:\")\n",
    "print(np.around(v.reshape(env.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    def one_step_lookahead(state, V):\n",
    "        Q = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            # we know probability of transition a->s' so this one is model base\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                Q[a] += reward + discount_factor * prob * V[next_state]\n",
    "        return Q\n",
    "    \n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            action_values = one_step_lookahead(s, V)\n",
    "            best_a = np.argmax(action_values)\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            \n",
    "            # **** update policy from one step look ahead ****\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 1 2]\n",
      " [2 0 1 0]]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -1. -1. -1.]\n",
      " [-1. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = policy_improvement(env)\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    def one_step_lookahead(state, V):\n",
    "        Q = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                Q[a] += reward + discount_factor * prob* V[next_state]\n",
    "        return Q\n",
    "    \n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            Q = one_step_lookahead(s, V)\n",
    "            best_action_value = np.max(Q)\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            \n",
    "            # **** update Value from best action of one step look ahead ****\n",
    "            V[s] = best_action_value        \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        A = one_step_lookahead(s, V)\n",
    "        best_action = np.argmax(A)\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, v = value_iteration(env)\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0):\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        # update V\n",
    "        for i, episode_step in enumerate(episode):\n",
    "            state = episode_step[0]\n",
    "            G = sum([x[2]*(discount_factor**idx_step) for idx_step, x in enumerate(episode[i:])])\n",
    "            returns_sum[state] += G\n",
    "            returns_count[state] += 1.0\n",
    "            V[state] = returns_sum[state] / returns_count[state]\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/1000.Reshaped Grid Value Function:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -16.14189944, -20.26352396, -21.23388849],\n",
       "       [-13.79787234, -18.71111111, -20.38071895, -20.59163701],\n",
       "       [-19.71331828, -20.69535415, -18.38797814, -13.85984848],\n",
       "       [-22.37629221, -20.07776905, -13.52729384,   0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = mc_prediction(lambda _: np.random.choice([0,1,2,3]), env, num_episodes=1000)\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "v_array = [x[1] for x in sorted(v.items())]\n",
    "np.array(v_array).reshape(env.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo with Epsilon-Greedy Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "#         best_action = np.argmax(Q[observation])\n",
    "        best_action = np.random.choice(np.flatnonzero(Q[observation] == Q[observation].max()))\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return np.random.choice(np.arange(len(A)), p=A)\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Q_to_V(Q):\n",
    "    V = np.array([0]*env.nS)\n",
    "    for state, action_value in Q.items():\n",
    "        V[state] = np.max(action_value)\n",
    "    return V.reshape(env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # update Q\n",
    "        for i, episode_step in enumerate(episode):\n",
    "            state, action, _ = episode_step\n",
    "            sa_pair = (state, action)\n",
    "            G = sum([x[2]*(discount_factor**idx_step) for idx_step, x in enumerate(episode[i:])])\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = mc_control_epsilon_greedy(env, num_episodes=20, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  -1, -13, -15],\n",
       "       [ -1, -15, -11,  -4],\n",
       "       [ -5, -18, -20,  -1],\n",
       "       [-36, -38,  -1,   0]])"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_Q_to_V(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD SARSA with Epsilon-Greedy Policies\n",
    "n-step = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "\n",
    "        for t in range(10000000):\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_state)\n",
    "            \n",
    "            # TD Update\n",
    "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "            td_delta = td_target - Q[state][action] #'TD error'\n",
    "            Q[state][action] += alpha * td_delta\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            action = next_action\n",
    "            state = next_state        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/200."
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -2],\n",
       "       [-1, -2, -2,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = sarsa(env, 200)\n",
    "convert_Q_to_V(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD Q-Learning with Epsilon-Greedy Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        state = env.reset()\n",
    "        for t in range(10000000):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            \n",
    "            # 1. update\n",
    "#             td_delta = td_target - Q[state][action]\n",
    "#             Q[state][action] += alpha * td_delta\n",
    "            # 2. another term with same result \n",
    "            Q[state][action] = (1-alpha)*Q[state][action] + alpha*td_target\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/200."
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -2, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = q_learning(env, 200)\n",
    "convert_Q_to_V(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T  o  o  o\n",
      "o  o  o  o\n",
      "o  o  o  o\n",
      "x  o  o  T\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()\n",
    "state = env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa n-step [my own code] this code is wrong see p.147"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_n_steps(env, num_episodes, n_steps, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "\n",
    "        done = False\n",
    "        while True:\n",
    "            steps = []\n",
    "            for n in range(n_steps):\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_action = policy(next_state)\n",
    "                steps.append((state, action, reward))\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                action = next_action\n",
    "                state = next_state\n",
    "            \n",
    "            for i, step in enumerate(steps):\n",
    "                td_target = sum([(discount_factor**idx_step)*x[2] for idx_step, x in enumerate(steps[i:i+n_steps])])\n",
    "                td_target += Q[next_state][next_action] # add the last action-value\n",
    "                \n",
    "                current_state, current_action, _ = steps[i]\n",
    "                td_error = td_target - Q[current_state][current_action]\n",
    "                Q[current_state][current_action] += alpha * td_error\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/200."
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -2, -3],\n",
       "       [ 0, -2, -2, -2],\n",
       "       [-2, -2, -2,  0],\n",
       "       [-3, -2, -1,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = sarsa_n_steps(env, 200, 4)\n",
    "convert_Q_to_V(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sasa TD(λ) [my own code]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_lamba(env, num_episodes, lamb, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "\n",
    "        done = False\n",
    "        steps = []\n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_state)\n",
    "            steps.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            action = next_action\n",
    "            state = next_state\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            G = sum([(discount_factor**idx_step)*x[2] for idx_step, x in enumerate(steps[i:])])\n",
    "            current_state, current_action, _ = steps[i]\n",
    "            td_error = G - Q[current_state][current_action]\n",
    "            Q[current_state][current_action] += alpha * td_error\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200/200."
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -2, -3],\n",
       "       [-1, -5, -3, -4],\n",
       "       [-2, -3, -2,  0],\n",
       "       [-3, -2, -1,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = sarsa_lamba(env, 200, 0.9)\n",
    "convert_Q_to_V(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eligibility traces (from David's slide p.29)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](media/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_eligibility_traces(env, num_episodes, lamb, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    Q = np.zeros((env.nS, env.action_space.n))\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        E = np.zeros((env.nS, env.action_space.n))\n",
    "        state = env.reset()\n",
    "        action = policy(state)\n",
    "\n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = policy(next_state)\n",
    "            \n",
    "            td_error = (reward+discount_factor*Q[next_state][next_action])\\\n",
    "                        - Q[state][action]\n",
    "            E[state][action] = E[state][action] + 1\n",
    "            \n",
    "            Q = Q + alpha*td_error*E\n",
    "            E *= discount_factor*lamb\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            action = next_action\n",
    "            state = next_state\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Q_numpy_to_V(Q):\n",
    "    V = np.array([0]*env.nS)\n",
    "    for state in range(len(Q)):\n",
    "        V[state] = np.max(Q[state])\n",
    "    return V.reshape(env.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = sarsa_eligibility_traces(env, 2, 0.1)\n",
    "convert_Q_numpy_to_V(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

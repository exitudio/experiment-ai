{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.table_base import TableBase, ApproximationBase\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -2, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -2, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _= self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.Q[state_][action_]\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -2, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9, lambd=0.1):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma, lambd)\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_state = 14# np.array([x for x in range(16)])\n",
    "raw_state%4, np.floor(raw_state/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.60148382, -0.57171768, -0.54195154, -0.51218545],\n",
       "       [-0.54788327, -0.58922839, -0.62383676, -0.65844512],\n",
       "       [-0.49428269, -0.54139221, -0.5769729 , -0.61158127],\n",
       "       [-0.44068211, -0.48940787, -0.53010905, -0.56471741]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "env = GridworldEnv()\n",
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.5, alpha=0.001, gamma=.9):\n",
    "        num_state = 2 # width * height\n",
    "        super().__init__(env, \n",
    "                         num_state, \n",
    "                         env.action_space.n, \n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "        self.state_rage = env.observation_space.n ** (1/num_state)\n",
    "        \n",
    "    def convert_to_2_dimension_state(self, state):\n",
    "        return np.array([math.floor(state/self.state_rage), state%self.state_rage], dtype=int)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.convert_to_2_dimension_state(self.env.reset())\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            state_ = self.convert_to_2_dimension_state(state_)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_, action_)\n",
    "            self.update_weight(td_target, self.approximate_q(state, action))\n",
    "#             td_error = td_target - self.approximate_q(state, action)\n",
    "#             self.weight -= self.alpha * td_error * self.feature(state, action)\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = self.convert_to_2_dimension_state(state)\n",
    "            qs = []\n",
    "            for action in range(self.env.action_space.n):\n",
    "                qs.append(self.approximate_q(convert_state, action).item())\n",
    "            V[state] = np.array(qs).max()\n",
    "        return V.reshape(self.env.shape)\n",
    "        \n",
    "s = SarsaApproximation(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [1,0]\n",
    "a = np.array([[1,2],[3,4]])\n",
    "a[c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 9.0\n",
      "episode: 1 reward: 11.0\n",
      "episode: 2 reward: 10.0\n",
      "episode: 3 reward: 11.0\n",
      "episode: 4 reward: 11.0\n",
      "episode: 5 reward: 10.0\n",
      "episode: 6 reward: 11.0\n",
      "episode: 7 reward: 10.0\n",
      "episode: 8 reward: 8.0\n",
      "episode: 9 reward: 10.0\n",
      "episode: 10 reward: 8.0\n",
      "episode: 11 reward: 11.0\n",
      "episode: 12 reward: 9.0\n",
      "episode: 13 reward: 9.0\n",
      "episode: 14 reward: 9.0\n",
      "episode: 15 reward: 11.0\n",
      "episode: 16 reward: 11.0\n",
      "episode: 17 reward: 10.0\n",
      "episode: 18 reward: 10.0\n",
      "episode: 19 reward: 9.0\n",
      "episode: 20 reward: 10.0\n",
      "episode: 21 reward: 10.0\n",
      "episode: 22 reward: 11.0\n",
      "episode: 23 reward: 8.0\n",
      "episode: 24 reward: 9.0\n",
      "episode: 25 reward: 9.0\n",
      "episode: 26 reward: 10.0\n",
      "episode: 27 reward: 9.0\n",
      "episode: 28 reward: 10.0\n",
      "episode: 29 reward: 8.0\n",
      "episode: 30 reward: 10.0\n",
      "episode: 31 reward: 13.0\n",
      "episode: 32 reward: 8.0\n",
      "episode: 33 reward: 9.0\n",
      "episode: 34 reward: 11.0\n",
      "episode: 35 reward: 9.0\n",
      "episode: 36 reward: 10.0\n",
      "episode: 37 reward: 11.0\n",
      "episode: 38 reward: 10.0\n",
      "episode: 39 reward: 11.0\n",
      "episode: 40 reward: 9.0\n",
      "episode: 41 reward: 11.0\n",
      "episode: 42 reward: 12.0\n",
      "episode: 43 reward: 9.0\n",
      "episode: 44 reward: 9.0\n",
      "episode: 45 reward: 15.0\n",
      "episode: 46 reward: 10.0\n",
      "episode: 47 reward: 16.0\n",
      "episode: 48 reward: 11.0\n",
      "episode: 49 reward: 19.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "import random\n",
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.001, gamma=.9):\n",
    "        super().__init__(env,\n",
    "                         env.observation_space.shape[0],\n",
    "                         env.action_space.n,\n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_, action_)\n",
    "            self.update_weight(td_target, self.approximate_q(state, action))\n",
    "#             td_error = td_target - self.approximate_q(state, action)\n",
    "#             self.weight -= self.alpha * td_error * self.feature(state, action)\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaApproximation(env, 50)\n",
    "s.train(True)\n",
    "# s.convert_Q_to_V()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

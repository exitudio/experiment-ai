{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning) <br>\n",
    "[another one](https://github.com/vikasjiitk/Deep-RL-Mountain-Car/blob/master/MCqlearn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approximation_v_base import ApproximationVBase, ExperienceReplay\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-1, -1,  0,  0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9):\n",
    "        super().__init__(env, num_episodes, \"epsilon_greedy\", epsilon, alpha, gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _= self.env.step(action)\n",
    "            action_ = self.policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1, -1, -2],\n",
       "       [-1, -2, -2, -1],\n",
       "       [-1, -2, -1, -1],\n",
       "       [-2, -2, -1,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9, lambd=0.1):\n",
    "        super().__init__(env, num_episodes, \"epsilon_greedy\", epsilon, alpha, gamma, lambd)\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = self.policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 510)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldEnv2DState(GridworldEnv):\n",
    "    def __init__(self, shape=[4, 4]):\n",
    "        super().__init__(shape)\n",
    "        \n",
    "    def convert_to_2_dimension_state(self, state):\n",
    "        return np.array([math.floor(state/4), state%4], dtype=int)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.convert_to_2_dimension_state(super(GridworldEnv, self).reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super(GridworldEnv, self).step(action)\n",
    "        state = self.convert_to_2_dimension_state(state)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "env = GridworldEnv2DState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -1.0\n",
      "episode: 1 reward: -39.0\n",
      "episode: 2 reward: -11.0\n",
      "episode: 3 reward: -5.0\n",
      "episode: 4 reward: -2.0\n",
      "episode: 5 reward: -2.0\n",
      "episode: 6 reward: -3.0\n",
      "episode: 7 reward: -19.0\n",
      "episode: 8 reward: -3.0\n",
      "episode: 9 reward: -3.0\n",
      "episode: 10 reward: -2.0\n",
      "episode: 11 reward: -3.0\n",
      "episode: 12 reward: -4.0\n",
      "episode: 13 reward: -4.0\n",
      "episode: 14 reward: 0.0\n",
      "episode: 15 reward: -2.0\n",
      "episode: 16 reward: -5.0\n",
      "episode: 17 reward: -4.0\n",
      "episode: 18 reward: -5.0\n",
      "episode: 19 reward: -3.0\n",
      "episode: 20 reward: -4.0\n",
      "episode: 21 reward: -1.0\n",
      "episode: 22 reward: -3.0\n",
      "episode: 23 reward: -2.0\n",
      "episode: 24 reward: -4.0\n",
      "episode: 25 reward: -2.0\n",
      "episode: 26 reward: 0.0\n",
      "episode: 27 reward: 0.0\n",
      "episode: 28 reward: -2.0\n",
      "episode: 29 reward: 0.0\n",
      "episode: 30 reward: -2.0\n",
      "episode: 31 reward: -5.0\n",
      "episode: 32 reward: -1.0\n",
      "episode: 33 reward: -5.0\n",
      "episode: 34 reward: -1.0\n",
      "episode: 35 reward: -3.0\n",
      "episode: 36 reward: 0.0\n",
      "episode: 37 reward: -3.0\n",
      "episode: 38 reward: -3.0\n",
      "episode: 39 reward: -3.0\n",
      "episode: 40 reward: -4.0\n",
      "episode: 41 reward: 0.0\n",
      "episode: 42 reward: -2.0\n",
      "episode: 43 reward: -2.0\n",
      "episode: 44 reward: 0.0\n",
      "episode: 45 reward: -1.0\n",
      "episode: 46 reward: -4.0\n",
      "episode: 47 reward: -5.0\n",
      "episode: 48 reward: 0.0\n",
      "episode: 49 reward: -1.0\n",
      "[0 0] [-0.87132955 -3.3201706  -1.4656886  -0.4598508 ]\n",
      "[0 1] [-2.4847233 -4.135795  -1.8376215 -1.0001509]\n",
      "[0 2] [-5.5117497 -3.6280937 -2.7716877 -1.8890069]\n",
      "[0 3] [-8.575933  -2.755424  -3.784681  -2.7001388]\n",
      "[1 0] [-1.0014015 -3.8165958 -2.2321725 -1.4097228]\n",
      "[1 1] [-2.5504618 -4.617696  -2.7769437 -1.8970761]\n",
      "[1 2] [-5.5617967 -4.297784  -3.7363129 -2.6947317]\n",
      "[1 3] [-8.70873   -3.6990983 -4.718719  -3.4467812]\n",
      "[2 0] [-1.892564  -4.404025  -3.5475469 -2.9435399]\n",
      "[2 1] [-2.7001839 -5.1993837 -3.9101362 -2.7412136]\n",
      "[2 2] [-5.6179576 -4.997469  -4.8366303 -3.425762 ]\n",
      "[2 3] [-8.776967  -4.4110293 -5.779968  -4.1172757]\n",
      "[3 0] [-2.7459414 -4.8531923 -4.9509234 -4.337423 ]\n",
      "[3 1] [-3.457851  -5.689809  -5.257214  -4.0136533]\n",
      "[3 2] [-5.759476  -5.5922647 -5.930047  -4.1873226]\n",
      "[3 3] [-8.832312  -5.1225595 -6.882553  -4.8552747]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.45985079, -1.00015092, -1.88900685, -2.70013881],\n",
       "       [-1.00140154, -1.89707613, -2.69473171, -3.44678116],\n",
       "       [-1.89256406, -2.70018387, -3.42576194, -4.11727571],\n",
       "       [-2.7459414 , -3.45785093, -4.18732262, -4.85527468]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaApproximation(ApproximationVBase):\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 num_state, \n",
    "                 num_action, \n",
    "                 num_episodes, \n",
    "                 num_experience=100, \n",
    "                 epsilon=0.01, \n",
    "                 alpha=0.008, \n",
    "                 gamma=.9):\n",
    "        super().__init__(env, \n",
    "                         num_state, \n",
    "                         num_action, \n",
    "                         num_episodes, \n",
    "                         \"epsilon_greedy\",\n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "        self.initialize()\n",
    "        if num_experience==1:\n",
    "            self.update_experience = self.update_step_by_step_experience\n",
    "        else:\n",
    "            self.experience_replay = ExperienceReplay(num_experience) \n",
    "            self.update_experience = self.update_experience_replay\n",
    "    \n",
    "    def update_step_by_step_experience(self, state, action, reward, state_, action_, done):\n",
    "        if done:\n",
    "            td_target = torch.Tensor(np.array(reward))\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "        predict_q = self.approximate_q(state)[action]\n",
    "        self.update_weight(td_target, predict_q)\n",
    "    \n",
    "    def update_experience_replay(self, state, action, reward, state_, action_, done):\n",
    "        def get_target(state, action, reward, state_, action_, done):\n",
    "            if done:\n",
    "                td_target = torch.Tensor(np.array(reward))\n",
    "            else:\n",
    "                td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "            predict_q = self.approximate_q(state)[action]\n",
    "            return td_target, predict_q\n",
    "\n",
    "        self.experience_replay.remember(state, action, reward, state_, action_, done)\n",
    "        targets, predict_qs = self.experience_replay.get_batch(get_target)\n",
    "        self.update_weight(targets, predict_qs)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = self.policy(state_)\n",
    "            self.update_experience(state, action, reward, state_, action_, done)\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = env.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.approximate_q(convert_state).detach().numpy())\n",
    "            V[state] = self.approximate_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "        \n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "s.train(True)\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nn by Q from Table base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.36118457 -1.3198457  -0.2557702  -0.22230437]\n",
      "[0 1] [-1.511564  -2.1404922 -1.3061261 -1.1788965]\n",
      "[0 2] [-2.4676168 -2.8936074 -2.0234046 -1.9880375]\n",
      "[0 3] [-3.3136873 -3.579819  -2.4548166 -2.7299285]\n",
      "[1 0] [-1.2686487 -1.6585399 -1.5670905 -1.4436761]\n",
      "[1 1] [-1.6204808 -1.786285  -1.8365965 -1.4560344]\n",
      "[1 2] [-1.9652414 -2.3225915 -1.9482744 -1.7251934]\n",
      "[1 3] [-2.5179617 -2.9341543 -2.1117406 -2.2142372]\n",
      "[2 0] [-1.9817442 -2.006702  -2.524445  -2.5173528]\n",
      "[2 1] [-1.7604569 -1.700321  -2.151839  -2.006669 ]\n",
      "[2 2] [-1.5198797 -1.619311  -1.7397889 -1.4716948]\n",
      "[2 3] [-1.7550889 -2.0477638 -1.7674761 -1.6426393]\n",
      "[3 0] [-2.5028756 -2.3395753 -3.1446683 -3.3754904]\n",
      "[3 1] [-2.1944273 -1.8976816 -2.7189476 -2.8086221]\n",
      "[3 2] [-1.6339544 -1.3893665 -2.010995  -1.9795097]\n",
      "[3 3] [-1.3282132 -1.2867506 -1.5140728 -1.3920003]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.22230437, -1.17889655, -1.98803747, -2.45481658],\n",
       "       [-1.26864874, -1.45603442, -1.72519338, -2.11174059],\n",
       "       [-1.98174417, -1.70032096, -1.47169483, -1.64263928],\n",
       "       [-2.33957529, -1.89768159, -1.38936651, -1.28675056]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_weight(td_target, s.approximate_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 13.0\n",
      "episode: 1 reward: 17.0\n",
      "episode: 2 reward: 21.0\n",
      "episode: 3 reward: 25.0\n",
      "episode: 4 reward: 123.0\n",
      "episode: 5 reward: 102.0\n",
      "episode: 6 reward: 30.0\n",
      "episode: 7 reward: 8.0\n",
      "episode: 8 reward: 46.0\n",
      "episode: 9 reward: 62.0\n",
      "episode: 10 reward: 78.0\n",
      "episode: 11 reward: 84.0\n",
      "episode: 12 reward: 115.0\n",
      "episode: 13 reward: 124.0\n",
      "episode: 14 reward: 121.0\n",
      "episode: 15 reward: 131.0\n",
      "episode: 16 reward: 154.0\n",
      "episode: 17 reward: 182.0\n",
      "episode: 18 reward: 179.0\n",
      "episode: 19 reward: 255.0\n",
      "episode: 20 reward: 205.0\n",
      "episode: 21 reward: 178.0\n",
      "episode: 22 reward: 13.0\n",
      "episode: 23 reward: 19.0\n",
      "episode: 24 reward: 208.0\n",
      "episode: 25 reward: 155.0\n",
      "episode: 26 reward: 457.0\n",
      "episode: 27 reward: 206.0\n",
      "episode: 28 reward: 220.0\n",
      "episode: 29 reward: 178.0\n",
      "episode: 30 reward: 196.0\n",
      "episode: 31 reward: 500.0\n",
      "episode: 32 reward: 192.0\n",
      "episode: 33 reward: 14.0\n",
      "episode: 34 reward: 18.0\n",
      "episode: 35 reward: 16.0\n",
      "episode: 36 reward: 12.0\n",
      "episode: 37 reward: 67.0\n",
      "episode: 38 reward: 101.0\n",
      "episode: 39 reward: 362.0\n",
      "episode: 40 reward: 224.0\n",
      "episode: 41 reward: 248.0\n",
      "episode: 42 reward: 403.0\n",
      "episode: 43 reward: 97.0\n",
      "episode: 44 reward: 95.0\n",
      "episode: 45 reward: 129.0\n",
      "episode: 46 reward: 169.0\n",
      "episode: 47 reward: 186.0\n",
      "episode: 48 reward: 203.0\n",
      "episode: 49 reward: 216.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=env.observation_space.shape[0],\n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       num_experience=512,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.007, \n",
    "                       gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

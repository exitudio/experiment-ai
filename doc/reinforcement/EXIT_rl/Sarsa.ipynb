{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning) <br>\n",
    "[another one](https://github.com/vikasjiitk/Deep-RL-Mountain-Car/blob/master/MCqlearn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approximation_base import ApproximationBase, ExperienceReplay\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _= self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1, -2, -2],\n",
       "       [-1, -1, -2, -1],\n",
       "       [-2, -2, -1, -1],\n",
       "       [-2, -1, -1,  0]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9, lambd=0.1):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma, lambd)\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 510)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldEnv2DState(GridworldEnv):\n",
    "    def __init__(self, shape=[4, 4]):\n",
    "        super().__init__(shape)\n",
    "        \n",
    "    def convert_to_2_dimension_state(self, state):\n",
    "        return np.array([math.floor(state/4), state%4], dtype=int)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.convert_to_2_dimension_state(super(GridworldEnv, self).reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super(GridworldEnv, self).step(action)\n",
    "        state = self.convert_to_2_dimension_state(state)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "env = GridworldEnv2DState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -16.0\n",
      "episode: 1 reward: -2.0\n",
      "episode: 2 reward: 0.0\n",
      "episode: 3 reward: -60.0\n",
      "episode: 4 reward: -2.0\n",
      "episode: 5 reward: -3.0\n",
      "episode: 6 reward: -17.0\n",
      "episode: 7 reward: -3.0\n",
      "episode: 8 reward: -1.0\n",
      "episode: 9 reward: -2.0\n",
      "episode: 10 reward: -27.0\n",
      "episode: 11 reward: -1.0\n",
      "episode: 12 reward: -45.0\n",
      "episode: 13 reward: -2.0\n",
      "episode: 14 reward: -2.0\n",
      "episode: 15 reward: -1.0\n",
      "episode: 16 reward: -7.0\n",
      "episode: 17 reward: 0.0\n",
      "episode: 18 reward: -4.0\n",
      "episode: 19 reward: -14.0\n",
      "episode: 20 reward: -3.0\n",
      "episode: 21 reward: -7.0\n",
      "episode: 22 reward: -4.0\n",
      "episode: 23 reward: -3.0\n",
      "episode: 24 reward: -5.0\n",
      "episode: 25 reward: -1.0\n",
      "episode: 26 reward: -2.0\n",
      "episode: 27 reward: -3.0\n",
      "episode: 28 reward: -5.0\n",
      "episode: 29 reward: -2.0\n",
      "episode: 30 reward: -3.0\n",
      "episode: 31 reward: 0.0\n",
      "episode: 32 reward: -8.0\n",
      "episode: 33 reward: -2.0\n",
      "episode: 34 reward: -2.0\n",
      "episode: 35 reward: -2.0\n",
      "episode: 36 reward: -4.0\n",
      "episode: 37 reward: -4.0\n",
      "episode: 38 reward: -2.0\n",
      "episode: 39 reward: -3.0\n",
      "episode: 40 reward: -1.0\n",
      "episode: 41 reward: -7.0\n",
      "episode: 42 reward: -3.0\n",
      "episode: 43 reward: -1.0\n",
      "episode: 44 reward: -3.0\n",
      "episode: 45 reward: -2.0\n",
      "episode: 46 reward: -2.0\n",
      "episode: 47 reward: -3.0\n",
      "episode: 48 reward: -1.0\n",
      "episode: 49 reward: -3.0\n",
      "[0 0] [-0.9707987  -3.6587846  -5.53193     0.00771327]\n",
      "[0 1] [-1.9138908 -3.0300162 -6.424612  -1.0171376]\n",
      "[0 2] [-2.8844216 -2.9337869 -7.41348   -1.918012 ]\n",
      "[0 3] [-3.685647  -3.2796798 -8.43903   -2.7321608]\n",
      "[1 0] [-1.0369213 -7.22295   -5.1740003 -4.5813665]\n",
      "[1 1] [-1.93394   -6.2048283 -5.816093  -6.0247564]\n",
      "[1 2] [-2.8102856 -5.5416107 -6.744933  -6.512431 ]\n",
      "[1 3] [-3.5265906 -5.328963  -7.6716623 -6.7441864]\n",
      "[2 0] [-2.1221893 -5.548255  -4.3646746 -4.6836443]\n",
      "[2 1] [-2.6665037 -4.3652678 -4.855232  -6.7434654]\n",
      "[2 2] [-3.3955095 -3.7103286 -5.569265  -7.9988832]\n",
      "[2 3] [-4.112443  -4.3235607 -6.646349  -9.244602 ]\n",
      "[3 0] [-3.278517  -2.94083   -3.3916168 -4.210126 ]\n",
      "[3 1] [-3.6649215 -1.8822128 -3.9136386 -6.287854 ]\n",
      "[3 2] [-4.2318964  -0.96589684 -4.48998    -8.138945  ]\n",
      "[3 3] [-4.89779   -0.5161816 -5.201546  -9.248049 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00771327, -1.01713765, -1.91801202, -2.73216081],\n",
       "       [-1.03692126, -1.93394005, -2.81028557, -3.52659059],\n",
       "       [-2.12218928, -2.66650367, -3.39550948, -4.11244297],\n",
       "       [-2.94082999, -1.88221276, -0.96589684, -0.51618159]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 num_state, \n",
    "                 num_action, \n",
    "                 num_episodes, \n",
    "                 num_experience=100, \n",
    "                 epsilon=0.01, \n",
    "                 alpha=0.008, \n",
    "                 gamma=.9):\n",
    "        super().__init__(env, \n",
    "                         num_state, \n",
    "                         num_action, \n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "        if num_experience==1:\n",
    "            self.update_experience = self.update_step_by_step_experience\n",
    "        else:\n",
    "            self.experience_replay = ExperienceReplay(num_experience) \n",
    "            self.update_experience = self.update_experience_replay\n",
    "    \n",
    "    def update_step_by_step_experience(self, state, action, reward, state_, action_, done):\n",
    "        if done:\n",
    "            td_target = torch.Tensor(np.array(reward))\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "        predict_q = self.approximate_q(state)[action]\n",
    "        self.update_weight(td_target, predict_q)\n",
    "    \n",
    "    def update_experience_replay(self, state, action, reward, state_, action_, done):\n",
    "        def get_target(state, action, reward, state_, action_, done):\n",
    "            if done:\n",
    "                td_target = torch.Tensor(np.array(reward))\n",
    "            else:\n",
    "                td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "            predict_q = self.approximate_q(state)[action]\n",
    "            return td_target, predict_q\n",
    "\n",
    "        self.experience_replay.remember(state, action, reward, state_, action_, done)\n",
    "        targets, predict_qs = self.experience_replay.get_batch(get_target)\n",
    "        self.update_weight(targets, predict_qs)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            self.update_experience(state, action, reward, state_, action_, done)\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = env.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.approximate_q(convert_state).detach().numpy())\n",
    "            V[state] = self.approximate_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "        \n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "s.train(True)\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nn by Q from Table base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.39613077 -0.03714038  0.13694015 -0.3392189 ]\n",
      "[0 1] [-1.6346174 -1.6044319 -1.0119783 -1.3353143]\n",
      "[0 2] [-2.2975006 -2.5693913 -1.4769734 -2.0587354]\n",
      "[0 3] [-2.7724636 -3.3577216 -1.7400581 -2.6508527]\n",
      "[1 0] [-1.3514197 -1.4272399 -1.3393623 -1.5845333]\n",
      "[1 1] [-1.5627629 -1.4453161 -1.2919213 -1.5040332]\n",
      "[1 2] [-1.8155991 -1.8279312 -1.2653239 -1.7701101]\n",
      "[1 3] [-2.1938133 -2.4766972 -1.4244022 -2.2850974]\n",
      "[2 0] [-2.0016148 -2.2187886 -2.046148  -2.2909245]\n",
      "[2 1] [-1.6934061 -1.5435145 -1.548365  -1.8205878]\n",
      "[2 2] [-1.5272017 -1.1294053 -1.0528605 -1.4390197]\n",
      "[2 3] [-1.6833905 -1.4549936 -0.9649601 -1.63746  ]\n",
      "[3 0] [-2.3912418 -2.6372817 -2.4151344 -2.8316402]\n",
      "[3 1] [-2.0485532 -1.9206715 -1.946922  -2.3814125]\n",
      "[3 2] [-1.6947435 -1.2301654 -1.3014444 -1.7872014]\n",
      "[3 3] [-1.4059881 -0.7419831 -0.7568294 -1.3104312]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.13694015, -1.01197827, -1.47697341, -1.74005806],\n",
       "       [-1.33936226, -1.29192126, -1.26532388, -1.42440224],\n",
       "       [-2.00161481, -1.54351449, -1.0528605 , -0.9649601 ],\n",
       "       [-2.39124179, -1.92067146, -1.23016536, -0.74198312]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_weight(td_target, s.approximate_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 12.0\n",
      "episode: 1 reward: 22.0\n",
      "episode: 2 reward: 21.0\n",
      "episode: 3 reward: 10.0\n",
      "episode: 4 reward: 17.0\n",
      "episode: 5 reward: 34.0\n",
      "episode: 6 reward: 66.0\n",
      "episode: 7 reward: 67.0\n",
      "episode: 8 reward: 19.0\n",
      "episode: 9 reward: 47.0\n",
      "episode: 10 reward: 65.0\n",
      "episode: 11 reward: 9.0\n",
      "episode: 12 reward: 23.0\n",
      "episode: 13 reward: 11.0\n",
      "episode: 14 reward: 16.0\n",
      "episode: 15 reward: 40.0\n",
      "episode: 16 reward: 42.0\n",
      "episode: 17 reward: 42.0\n",
      "episode: 18 reward: 26.0\n",
      "episode: 19 reward: 59.0\n",
      "episode: 20 reward: 96.0\n",
      "episode: 21 reward: 28.0\n",
      "episode: 22 reward: 26.0\n",
      "episode: 23 reward: 37.0\n",
      "episode: 24 reward: 32.0\n",
      "episode: 25 reward: 43.0\n",
      "episode: 26 reward: 52.0\n",
      "episode: 27 reward: 104.0\n",
      "episode: 28 reward: 158.0\n",
      "episode: 29 reward: 149.0\n",
      "episode: 30 reward: 164.0\n",
      "episode: 31 reward: 135.0\n",
      "episode: 32 reward: 99.0\n",
      "episode: 33 reward: 229.0\n",
      "episode: 34 reward: 14.0\n",
      "episode: 35 reward: 16.0\n",
      "episode: 36 reward: 330.0\n",
      "episode: 37 reward: 207.0\n",
      "episode: 38 reward: 179.0\n",
      "episode: 39 reward: 175.0\n",
      "episode: 40 reward: 500.0\n",
      "episode: 41 reward: 33.0\n",
      "episode: 42 reward: 16.0\n",
      "episode: 43 reward: 9.0\n",
      "episode: 44 reward: 29.0\n",
      "episode: 45 reward: 15.0\n",
      "episode: 46 reward: 21.0\n",
      "episode: 47 reward: 22.0\n",
      "episode: 48 reward: 36.0\n",
      "episode: 49 reward: 50.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=env.observation_space.shape[0],\n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       num_experience=128,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.007, \n",
    "                       gamma=.99)\n",
    "s.train(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

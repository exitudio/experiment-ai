{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning) <br>\n",
    "[another one](https://github.com/vikasjiitk/Deep-RL-Mountain-Car/blob/master/MCqlearn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approximation_base import ApproximationBase, ExperienceReplay\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _= self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1, -2, -2],\n",
       "       [-1, -1, -2, -1],\n",
       "       [-2, -2, -1, -1],\n",
       "       [-2, -1, -1,  0]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9, lambd=0.1):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma, lambd)\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 510)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldEnv2DState(GridworldEnv):\n",
    "    def __init__(self, shape=[4, 4]):\n",
    "        super().__init__(shape)\n",
    "        \n",
    "    def convert_to_2_dimension_state(self, state):\n",
    "        return np.array([math.floor(state/4), state%4], dtype=int)\n",
    "    \n",
    "    def reset(self):\n",
    "        return self.convert_to_2_dimension_state(super(GridworldEnv, self).reset())\n",
    "    \n",
    "    def step(self, action):\n",
    "        state, reward, done, info = super(GridworldEnv, self).step(action)\n",
    "        state = self.convert_to_2_dimension_state(state)\n",
    "        return state, reward, done, info\n",
    "    \n",
    "env = GridworldEnv2DState()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got numpy.int64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-049c1c8f5c8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                        \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.008\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                        gamma=.9)\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_Q_to_V\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-049c1c8f5c8b>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/approximation_base.py\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Not support multiple max a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproximate_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/approximation_base.py\u001b[0m in \u001b[0;36mapproximate_q\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapproximate_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got numpy.int64)"
     ]
    }
   ],
   "source": [
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, \n",
    "                 env, \n",
    "                 num_state, \n",
    "                 num_action, \n",
    "                 num_episodes, \n",
    "                 num_experience=100, \n",
    "                 epsilon=0.01, \n",
    "                 alpha=0.008, \n",
    "                 gamma=.9):\n",
    "        super().__init__(env, \n",
    "                         num_state, \n",
    "                         num_action, \n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "        if num_experience==1:\n",
    "            self.update_experience = self.update_step_by_step_experience\n",
    "        else:\n",
    "            self.experience_replay = ExperienceReplay(num_experience) \n",
    "            self.update_experience = self.update_experience_replay\n",
    "    \n",
    "    def update_step_by_step_experience(self, state, action, reward, state_, action_, done):\n",
    "        if done:\n",
    "            td_target = torch.Tensor(np.array(reward))\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "        predict_q = self.approximate_q(state)[action]\n",
    "        self.update_weight(td_target, predict_q)\n",
    "    \n",
    "    def update_experience_replay(self, state, action, reward, state_, action_, done):\n",
    "        def get_target(state, action, reward, state_, action_, done):\n",
    "            if done:\n",
    "                td_target = torch.Tensor(np.array(reward))\n",
    "            else:\n",
    "                td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "            predict_q = self.approximate_q(state)[action]\n",
    "            return td_target, predict_q\n",
    "\n",
    "        self.experience_replay.remember(state, action, reward, state_, action_, done)\n",
    "        targets, predict_qs = self.experience_replay.get_batch(get_target)\n",
    "        self.update_weight(targets, predict_qs)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            self.update_experience(state, action, reward, state_, action_, done)\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = env.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.approximate_q(convert_state).detach().numpy())\n",
    "            V[state] = self.approximate_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "        \n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "s.train(True)\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nn by Q from Table base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.39613077 -0.03714038  0.13694015 -0.3392189 ]\n",
      "[0 1] [-1.6346174 -1.6044319 -1.0119783 -1.3353143]\n",
      "[0 2] [-2.2975006 -2.5693913 -1.4769734 -2.0587354]\n",
      "[0 3] [-2.7724636 -3.3577216 -1.7400581 -2.6508527]\n",
      "[1 0] [-1.3514197 -1.4272399 -1.3393623 -1.5845333]\n",
      "[1 1] [-1.5627629 -1.4453161 -1.2919213 -1.5040332]\n",
      "[1 2] [-1.8155991 -1.8279312 -1.2653239 -1.7701101]\n",
      "[1 3] [-2.1938133 -2.4766972 -1.4244022 -2.2850974]\n",
      "[2 0] [-2.0016148 -2.2187886 -2.046148  -2.2909245]\n",
      "[2 1] [-1.6934061 -1.5435145 -1.548365  -1.8205878]\n",
      "[2 2] [-1.5272017 -1.1294053 -1.0528605 -1.4390197]\n",
      "[2 3] [-1.6833905 -1.4549936 -0.9649601 -1.63746  ]\n",
      "[3 0] [-2.3912418 -2.6372817 -2.4151344 -2.8316402]\n",
      "[3 1] [-2.0485532 -1.9206715 -1.946922  -2.3814125]\n",
      "[3 2] [-1.6947435 -1.2301654 -1.3014444 -1.7872014]\n",
      "[3 3] [-1.4059881 -0.7419831 -0.7568294 -1.3104312]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.13694015, -1.01197827, -1.47697341, -1.74005806],\n",
       "       [-1.33936226, -1.29192126, -1.26532388, -1.42440224],\n",
       "       [-2.00161481, -1.54351449, -1.0528605 , -0.9649601 ],\n",
       "       [-2.39124179, -1.92067146, -1.23016536, -0.74198312]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=2, \n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_weight(td_target, s.approximate_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 12.0\n",
      "episode: 1 reward: 22.0\n",
      "episode: 2 reward: 21.0\n",
      "episode: 3 reward: 10.0\n",
      "episode: 4 reward: 17.0\n",
      "episode: 5 reward: 34.0\n",
      "episode: 6 reward: 66.0\n",
      "episode: 7 reward: 67.0\n",
      "episode: 8 reward: 19.0\n",
      "episode: 9 reward: 47.0\n",
      "episode: 10 reward: 65.0\n",
      "episode: 11 reward: 9.0\n",
      "episode: 12 reward: 23.0\n",
      "episode: 13 reward: 11.0\n",
      "episode: 14 reward: 16.0\n",
      "episode: 15 reward: 40.0\n",
      "episode: 16 reward: 42.0\n",
      "episode: 17 reward: 42.0\n",
      "episode: 18 reward: 26.0\n",
      "episode: 19 reward: 59.0\n",
      "episode: 20 reward: 96.0\n",
      "episode: 21 reward: 28.0\n",
      "episode: 22 reward: 26.0\n",
      "episode: 23 reward: 37.0\n",
      "episode: 24 reward: 32.0\n",
      "episode: 25 reward: 43.0\n",
      "episode: 26 reward: 52.0\n",
      "episode: 27 reward: 104.0\n",
      "episode: 28 reward: 158.0\n",
      "episode: 29 reward: 149.0\n",
      "episode: 30 reward: 164.0\n",
      "episode: 31 reward: 135.0\n",
      "episode: 32 reward: 99.0\n",
      "episode: 33 reward: 229.0\n",
      "episode: 34 reward: 14.0\n",
      "episode: 35 reward: 16.0\n",
      "episode: 36 reward: 330.0\n",
      "episode: 37 reward: 207.0\n",
      "episode: 38 reward: 179.0\n",
      "episode: 39 reward: 175.0\n",
      "episode: 40 reward: 500.0\n",
      "episode: 41 reward: 33.0\n",
      "episode: 42 reward: 16.0\n",
      "episode: 43 reward: 9.0\n",
      "episode: 44 reward: 29.0\n",
      "episode: 45 reward: 15.0\n",
      "episode: 46 reward: 21.0\n",
      "episode: 47 reward: 22.0\n",
      "episode: 48 reward: 36.0\n",
      "episode: 49 reward: 50.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "s = SarsaApproximation(env, \n",
    "                       num_state=env.observation_space.shape[0],\n",
    "                       num_action=env.action_space.n, \n",
    "                       num_episodes=50,\n",
    "                       num_experience=128,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.007, \n",
    "                       gamma=.99)\n",
    "s.train(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

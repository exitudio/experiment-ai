{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning) <br>\n",
    "[another one](https://github.com/vikasjiitk/Deep-RL-Mountain-Car/blob/master/MCqlearn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approx_v_base import ApproxVBase, ExperienceReplay\n",
    "from gridworld_env_2d_state import GridworldEnv2DState\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -1, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            _state, reward, done, _= self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[_state, _action]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Q[state, action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50, \"epsilon_greedy\")\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -1, -1, -2],\n",
       "       [-1, -1, -2, -1],\n",
       "       [-1, -2, -2, -1],\n",
       "       [-2, -1, -1,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, policy, *args, **kwargs):\n",
    "        super().__init__(env, num_episodes, policy, *args, **kwargs)\n",
    "        self.initialize()\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[_state, _action]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 510, \"epsilon_greedy\")\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: 0.0\n",
      "episode: 1 reward: -8.0\n",
      "episode: 2 reward: -3.0\n",
      "episode: 3 reward: -10.0\n",
      "episode: 4 reward: -9.0\n",
      "episode: 5 reward: -11.0\n",
      "episode: 6 reward: -4.0\n",
      "episode: 7 reward: -2.0\n",
      "episode: 8 reward: -5.0\n",
      "episode: 9 reward: -1.0\n",
      "episode: 10 reward: -12.0\n",
      "episode: 11 reward: -3.0\n",
      "episode: 12 reward: -6.0\n",
      "episode: 13 reward: -13.0\n",
      "episode: 14 reward: -5.0\n",
      "episode: 15 reward: 0.0\n",
      "episode: 16 reward: -4.0\n",
      "episode: 17 reward: -2.0\n",
      "episode: 18 reward: -23.0\n",
      "episode: 19 reward: -10.0\n",
      "episode: 20 reward: -2.0\n",
      "episode: 21 reward: 0.0\n",
      "episode: 22 reward: -10.0\n",
      "episode: 23 reward: -3.0\n",
      "episode: 24 reward: -6.0\n",
      "episode: 25 reward: -16.0\n",
      "episode: 26 reward: -26.0\n",
      "episode: 27 reward: -2.0\n",
      "episode: 28 reward: -9.0\n",
      "episode: 29 reward: -17.0\n",
      "episode: 30 reward: -17.0\n",
      "episode: 31 reward: -9.0\n",
      "episode: 32 reward: -2.0\n",
      "episode: 33 reward: -20.0\n",
      "episode: 34 reward: -15.0\n",
      "episode: 35 reward: -23.0\n",
      "episode: 36 reward: -7.0\n",
      "episode: 37 reward: -19.0\n",
      "episode: 38 reward: -3.0\n",
      "episode: 39 reward: -21.0\n",
      "episode: 40 reward: 0.0\n",
      "episode: 41 reward: 0.0\n",
      "episode: 42 reward: -1.0\n",
      "episode: 43 reward: -17.0\n",
      "episode: 44 reward: -11.0\n",
      "episode: 45 reward: -10.0\n",
      "episode: 46 reward: -1.0\n",
      "episode: 47 reward: -2.0\n",
      "episode: 48 reward: -3.0\n",
      "episode: 49 reward: -6.0\n",
      "[0 0] [-0.8034409  -1.2975942  -1.2448027  -0.68539697]\n",
      "[0 1] [-1.1326454 -1.0031531 -1.1907488 -1.1086234]\n",
      "[0 2] [-1.3711474 -1.1346642 -1.3827724 -1.2758855]\n",
      "[0 3] [-1.6581422 -1.2862777 -1.5874413 -1.4355247]\n",
      "[1 0] [-0.89961016 -0.8904731  -0.952909   -0.7966276 ]\n",
      "[1 1] [-1.124142  -1.0121145 -1.1422875 -1.0525802]\n",
      "[1 2] [-1.3537604 -1.1681015 -1.3395491 -1.2202883]\n",
      "[1 3] [-1.5976837 -1.3251737 -1.5438195 -1.3936906]\n",
      "[2 0] [-0.93046    -0.8883728  -0.98303914 -0.89171666]\n",
      "[2 1] [-1.1514677 -1.0437065 -1.1760818 -1.0559969]\n",
      "[2 2] [-1.3953909 -1.2007788 -1.3803523 -1.2293992]\n",
      "[2 3] [-1.6471775 -1.36256   -1.5861666 -1.409463 ]\n",
      "[3 0] [-0.95660794 -0.9251632  -1.0151097  -0.89547336]\n",
      "[3 1] [-1.1930982 -1.0763838 -1.216885  -1.065108 ]\n",
      "[3 2] [-1.4370214 -1.2334561 -1.4211557 -1.2385104]\n",
      "[3 3] [-1.7002276 -1.402076  -1.629212  -1.428248 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.68539697, -1.00315309, -1.13466418, -1.28627765],\n",
       "       [-0.79662758, -1.01211452, -1.16810155, -1.32517374],\n",
       "       [-0.88837278, -1.04370654, -1.20077884, -1.36256003],\n",
       "       [-0.89547336, -1.06510794, -1.23345613, -1.40207601]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaApproximation(ApproxVBase):\n",
    "    def __init__(self, num_experience=100, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "        if num_experience==1:\n",
    "            self.update_experience = self.update_step_by_step_experience\n",
    "        else:\n",
    "            self.experience_replay = ExperienceReplay(num_experience) \n",
    "            self.update_experience = self.update_experience_replay\n",
    "    \n",
    "    def update_step_by_step_experience(self, state, action, reward, _state, _action, done):\n",
    "        if done:\n",
    "            td_target = torch.Tensor(np.array(reward))\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.get_q(_state)[_action]\n",
    "        current_q = self.get_q(state)[action]\n",
    "        self.update_q(td_target, current_q)\n",
    "    \n",
    "    def update_experience_replay(self, state, action, reward, _state, _action, done):\n",
    "        def get_target(state, action, reward, _state, _action, done):\n",
    "            if done:\n",
    "                td_target = torch.Tensor(np.array(reward))\n",
    "            else:\n",
    "                td_target = reward + self.gamma * self.get_q(_state)[_action]\n",
    "            predict_q = self.get_q(state)[action]\n",
    "            return td_target, predict_q\n",
    "\n",
    "        self.experience_replay.remember(state, action, reward, _state, _action, done)\n",
    "        targets, predict_qs = self.experience_replay.get_batch(get_target)\n",
    "        current_q = self.get_q(state)[action]\n",
    "        self.update_q(targets, current_q)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        while not done:\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            self.update_experience(state, action, reward, _state, _action, done)\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = env.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.get_q(convert_state).detach().numpy())\n",
    "            V[state] = self.get_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "s.train(True)\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nn by Q from Table base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.29720393 -0.77073574 -0.39772606 -0.09194392]\n",
      "[0 1] [-1.4442132 -1.6489762 -1.2947102 -1.1942033]\n",
      "[0 2] [-2.331369  -2.3084543 -1.9601423 -1.9670415]\n",
      "[0 3] [-2.953517  -2.7731466 -2.2698061 -2.405674 ]\n",
      "[1 0] [-1.2534157 -1.2084376 -1.3155695 -1.4328315]\n",
      "[1 1] [-1.5475113 -1.566447  -1.4979403 -1.675886 ]\n",
      "[1 2] [-1.6540701 -1.7119943 -1.4306933 -1.5571195]\n",
      "[1 3] [-2.0827663 -2.0529833 -1.5269271 -1.7968255]\n",
      "[2 0] [-1.9395704 -1.6109526 -1.6911267 -2.23518  ]\n",
      "[2 1] [-1.7863352 -1.5094359 -1.5015067 -2.0322442]\n",
      "[2 2] [-1.0499052 -1.1368555 -0.8278215 -1.2377869]\n",
      "[2 3] [-1.0864139 -1.2765877 -0.7017511 -1.1208764]\n",
      "[3 0] [-2.4744623 -1.9700685 -2.0178757 -2.951979 ]\n",
      "[3 1] [-2.1332045 -1.7127857 -1.6662935 -2.6075447]\n",
      "[3 2] [-1.2380323  -1.0950063  -0.79769313 -1.6168311 ]\n",
      "[3 3] [-0.38406634 -0.59264046  0.01429594 -0.6642961 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.09194392, -1.19420326, -1.96014225, -2.26980615],\n",
       "       [-1.20843756, -1.4979403 , -1.43069327, -1.52692711],\n",
       "       [-1.61095262, -1.50150669, -0.82782149, -0.70175111],\n",
       "       [-1.97006845, -1.6662935 , -0.79769313,  0.01429594]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_v(td_target, s.get_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       num_experience=512,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.007, \n",
    "                       gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

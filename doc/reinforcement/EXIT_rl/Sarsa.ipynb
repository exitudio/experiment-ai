{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning) <br>\n",
    "[another one](https://github.com/vikasjiitk/Deep-RL-Mountain-Car/blob/master/MCqlearn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approx_v_base import ApproxVBase, ExperienceReplay\n",
    "from gridworld_env_2d_state import GridworldEnv2DState\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\tAverage Score: -69.00 \tother{}\r",
      "Episode 2\tAverage Score: -35.50 \tother{}\r",
      "Episode 3\tAverage Score: -24.67 \tother{}\r",
      "Episode 4\tAverage Score: -19.25 \tother{}\r",
      "Episode 5\tAverage Score: -20.80 \tother{}\r",
      "Episode 6\tAverage Score: -17.67 \tother{}\r",
      "Episode 7\tAverage Score: -15.43 \tother{}\r",
      "Episode 8\tAverage Score: -17.38 \tother{}\r",
      "Episode 9\tAverage Score: -15.44 \tother{}\r",
      "Episode 10\tAverage Score: -14.00 \tother{}\r",
      "Episode 10\tAverage Score: -14.00 \tother{}\n",
      "\r",
      "Episode 11\tAverage Score: -7.10 \tother{}\r",
      "Episode 12\tAverage Score: -8.20 \tother{}\r",
      "Episode 13\tAverage Score: -9.00 \tother{}\r",
      "Episode 14\tAverage Score: -9.20 \tother{}\r",
      "Episode 15\tAverage Score: -6.50 \tother{}\r",
      "Episode 16\tAverage Score: -6.90 \tother{}\r",
      "Episode 17\tAverage Score: -7.20 \tother{}\r",
      "Episode 18\tAverage Score: -5.00 \tother{}\r",
      "Episode 19\tAverage Score: -5.40 \tother{}\r",
      "Episode 20\tAverage Score: -5.60 \tother{}\r",
      "Episode 20\tAverage Score: -5.60 \tother{}\n",
      "\r",
      "Episode 21\tAverage Score: -5.70 \tother{}\r",
      "Episode 22\tAverage Score: -4.50 \tother{}\r",
      "Episode 23\tAverage Score: -3.50 \tother{}\r",
      "Episode 24\tAverage Score: -3.50 \tother{}\r",
      "Episode 25\tAverage Score: -3.70 \tother{}\r",
      "Episode 26\tAverage Score: -3.60 \tother{}\r",
      "Episode 27\tAverage Score: -3.30 \tother{}\r",
      "Episode 28\tAverage Score: -3.40 \tother{}\r",
      "Episode 29\tAverage Score: -3.20 \tother{}\r",
      "Episode 30\tAverage Score: -3.10 \tother{}\r",
      "Episode 30\tAverage Score: -3.10 \tother{}\n",
      "\r",
      "Episode 31\tAverage Score: -3.40 \tother{}\r",
      "Episode 32\tAverage Score: -5.10 \tother{}\r",
      "Episode 33\tAverage Score: -5.00 \tother{}\r",
      "Episode 34\tAverage Score: -4.70 \tother{}\r",
      "Episode 35\tAverage Score: -4.60 \tother{}\r",
      "Episode 36\tAverage Score: -4.30 \tother{}\r",
      "Episode 37\tAverage Score: -4.30 \tother{}\r",
      "Episode 38\tAverage Score: -3.30 \tother{}\r",
      "Episode 39\tAverage Score: -3.10 \tother{}\r",
      "Episode 40\tAverage Score: -3.20 \tother{}\r",
      "Episode 40\tAverage Score: -3.20 \tother{}\n",
      "\r",
      "Episode 41\tAverage Score: -2.90 \tother{}\r",
      "Episode 42\tAverage Score: -1.40 \tother{}\r",
      "Episode 43\tAverage Score: -2.00 \tother{}\r",
      "Episode 44\tAverage Score: -2.00 \tother{}\r",
      "Episode 45\tAverage Score: -1.90 \tother{}\r",
      "Episode 46\tAverage Score: -1.90 \tother{}\r",
      "Episode 47\tAverage Score: -1.70 \tother{}\r",
      "Episode 48\tAverage Score: -2.10 \tother{}\r",
      "Episode 49\tAverage Score: -2.10 \tother{}\r",
      "Episode 50\tAverage Score: -2.00 \tother{}\r",
      "Episode 50\tAverage Score: -2.00 \tother{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -1],\n",
       "       [ 0, -1, -1, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            _state, reward, done, _= self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[_state, _action]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Q[state, action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50, policy=\"epsilon_greedy\")\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\tAverage Score: -35.00 \tother{}\r",
      "Episode 2\tAverage Score: -18.00 \tother{}\r",
      "Episode 3\tAverage Score: -24.00 \tother{}\r",
      "Episode 4\tAverage Score: -19.25 \tother{}\r",
      "Episode 5\tAverage Score: -17.60 \tother{}\r",
      "Episode 6\tAverage Score: -17.67 \tother{}\r",
      "Episode 7\tAverage Score: -17.43 \tother{}\r",
      "Episode 8\tAverage Score: -17.75 \tother{}\r",
      "Episode 9\tAverage Score: -17.00 \tother{}\r",
      "Episode 10\tAverage Score: -15.90 \tother{}\r",
      "Episode 10\tAverage Score: -15.90 \tother{}\n",
      "\r",
      "Episode 11\tAverage Score: -12.50 \tother{}\r",
      "Episode 12\tAverage Score: -12.50 \tother{}\r",
      "Episode 13\tAverage Score: -9.00 \tother{}\r",
      "Episode 14\tAverage Score: -9.00 \tother{}\r",
      "Episode 15\tAverage Score: -8.80 \tother{}\r",
      "Episode 16\tAverage Score: -7.60 \tother{}\r",
      "Episode 17\tAverage Score: -7.40 \tother{}\r",
      "Episode 18\tAverage Score: -6.90 \tother{}\r",
      "Episode 19\tAverage Score: -6.10 \tother{}\r",
      "Episode 20\tAverage Score: -6.10 \tother{}\r",
      "Episode 20\tAverage Score: -6.10 \tother{}\n",
      "\r",
      "Episode 21\tAverage Score: -6.40 \tother{}\r",
      "Episode 22\tAverage Score: -6.90 \tother{}\r",
      "Episode 23\tAverage Score: -7.40 \tother{}\r",
      "Episode 24\tAverage Score: -7.10 \tother{}\r",
      "Episode 25\tAverage Score: -6.60 \tother{}\r",
      "Episode 26\tAverage Score: -6.60 \tother{}\r",
      "Episode 27\tAverage Score: -5.50 \tother{}\r",
      "Episode 28\tAverage Score: -4.30 \tother{}\r",
      "Episode 29\tAverage Score: -4.40 \tother{}\r",
      "Episode 30\tAverage Score: -3.90 \tother{}\r",
      "Episode 30\tAverage Score: -3.90 \tother{}\n",
      "\r",
      "Episode 31\tAverage Score: -3.80 \tother{}\r",
      "Episode 32\tAverage Score: -3.20 \tother{}\r",
      "Episode 33\tAverage Score: -3.20 \tother{}\r",
      "Episode 34\tAverage Score: -3.10 \tother{}\r",
      "Episode 35\tAverage Score: -3.20 \tother{}\r",
      "Episode 36\tAverage Score: -2.60 \tother{}\r",
      "Episode 37\tAverage Score: -2.50 \tother{}\r",
      "Episode 38\tAverage Score: -2.40 \tother{}\r",
      "Episode 39\tAverage Score: -2.50 \tother{}\r",
      "Episode 40\tAverage Score: -2.70 \tother{}\r",
      "Episode 40\tAverage Score: -2.70 \tother{}\n",
      "\r",
      "Episode 41\tAverage Score: -2.70 \tother{}\r",
      "Episode 42\tAverage Score: -2.80 \tother{}\r",
      "Episode 43\tAverage Score: -2.40 \tother{}\r",
      "Episode 44\tAverage Score: -2.50 \tother{}\r",
      "Episode 45\tAverage Score: -2.10 \tother{}\r",
      "Episode 46\tAverage Score: -2.40 \tother{}\r",
      "Episode 47\tAverage Score: -2.90 \tother{}\r",
      "Episode 48\tAverage Score: -2.90 \tother{}\r",
      "Episode 49\tAverage Score: -2.70 \tother{}\r",
      "Episode 50\tAverage Score: -2.60 \tother{}\r",
      "Episode 50\tAverage Score: -2.60 \tother{}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -1, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            ########## CORE Algorithm #########\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.Q[_state, _action]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 50, policy=\"epsilon_greedy\")\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: -4.30 \tother{}\n",
      "Episode 20\tAverage Score: -7.30 \tother{}\n",
      "Episode 30\tAverage Score: -5.20 \tother{}\n",
      "Episode 40\tAverage Score: -9.40 \tother{}}\n",
      "Episode 50\tAverage Score: -5.20 \tother{}\n",
      "[0 0] [-0.9077287 -1.282341  -1.1293762 -1.2470013]\n",
      "[0 1] [-0.87199724 -1.0577717  -0.7746947  -1.0634011 ]\n",
      "[0 2] [-1.0023547  -1.1457618  -0.78058726 -1.1445779 ]\n",
      "[0 3] [-1.2064143 -1.3191538 -0.8626278 -1.2914224]\n",
      "[1 0] [-0.8681651 -1.2761105 -0.7396302 -1.2011229]\n",
      "[1 1] [-0.88112247 -1.1920553  -0.6213519  -1.1986345 ]\n",
      "[1 2] [-1.0490115  -1.3578742  -0.68235993 -1.339334  ]\n",
      "[1 3] [-1.2284728  -1.5378819  -0.75267804 -1.4917085 ]\n",
      "[2 0] [-0.99110204 -1.52512    -0.75940585 -1.4113271 ]\n",
      "[2 1] [-1.0444002 -1.4676784 -0.7043794 -1.4636323]\n",
      "[2 2] [-1.1804383  -1.5996135  -0.75728023 -1.6135907 ]\n",
      "[2 3] [-1.3209777  -1.7362726  -0.81212986 -1.7646189 ]\n",
      "[3 0] [-1.1745627  -1.8637522  -0.83010244 -1.6675584 ]\n",
      "[3 1] [-1.2222526 -1.7895045 -0.79371   -1.7461829]\n",
      "[3 2] [-1.3582909 -1.9214396 -0.8466109 -1.8961412]\n",
      "[3 3] [-1.494329  -2.0533748 -0.8995117 -2.0460997]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.90772867, -0.77469468, -0.78058726, -0.8626278 ],\n",
       "       [-0.73963022, -0.6213519 , -0.68235993, -0.75267804],\n",
       "       [-0.75940585, -0.70437938, -0.75728023, -0.81212986],\n",
       "       [-0.83010244, -0.79370999, -0.8466109 , -0.89951169]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaApproximation(ApproxVBase):\n",
    "    def __init__(self, num_experience=100, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "        if num_experience==1:\n",
    "            self.update_experience = self.update_step_by_step_experience\n",
    "        else:\n",
    "            self.experience_replay = ExperienceReplay(num_experience) \n",
    "            self.update_experience = self.update_experience_replay\n",
    "    \n",
    "    def update_step_by_step_experience(self, state, action, reward, _state, _action, done):\n",
    "        if done:\n",
    "            td_target = torch.Tensor(np.array(reward))\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.get_q(_state)[_action]\n",
    "        current_q = self.get_q(state)[action]\n",
    "        self.update_q(td_target, current_q)\n",
    "    \n",
    "    def update_experience_replay(self, state, action, reward, _state, _action, done):\n",
    "        def get_target(state, action, reward, _state, _action, done):\n",
    "            if done:\n",
    "                td_target = torch.Tensor(np.array(reward))\n",
    "            else:\n",
    "                td_target = reward + self.gamma * self.get_q(_state)[_action]\n",
    "            predict_q = self.get_q(state)[action]\n",
    "            return td_target, predict_q\n",
    "\n",
    "        self.experience_replay.remember(state, action, reward, _state, _action, done)\n",
    "        targets, predict_qs = self.experience_replay.get_batch(get_target)\n",
    "        current_q = self.get_q(state)[action]\n",
    "        self.update_q(targets, current_q)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = self.policy(state)\n",
    "        while not done:\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _action = self.policy(_state)\n",
    "            self.update_experience(state, action, reward, _state, _action, done)\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = env.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.get_q(convert_state).detach().numpy())\n",
    "            V[state] = self.get_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "s.train(True)\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test nn by Q from Table base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.08950523 -0.04526846 -0.4771667  -0.518084  ]\n",
      "[0 1] [-1.3291311 -1.3113158 -1.2687043 -1.242636 ]\n",
      "[0 2] [-2.2121005 -2.2506235 -1.8334401 -2.0281816]\n",
      "[0 3] [-2.7804286 -2.812126  -2.0403173 -2.5864334]\n",
      "[1 0] [-1.0722723 -1.1309662 -1.5587296 -1.9006   ]\n",
      "[1 1] [-1.1853498 -1.3835914 -1.5559304 -1.6240482]\n",
      "[1 2] [-1.4170573 -1.5560282 -1.491466  -1.6300268]\n",
      "[1 3] [-1.8548989 -1.9498202 -1.5356302 -2.021651 ]\n",
      "[2 0] [-2.1810641 -2.0493307 -2.4047909 -2.9850712]\n",
      "[2 1] [-1.4733295 -1.5274479 -1.7964101 -2.1572762]\n",
      "[2 2] [-0.71565205 -0.88860154 -1.0541437  -1.2763841 ]\n",
      "[2 3] [-0.92003447 -1.0146617  -0.85674316 -1.2646188 ]\n",
      "[3 0] [-2.8465388 -2.507756  -2.814615  -3.7934325]\n",
      "[3 1] [-1.9259622 -1.7914754 -2.0552983 -2.8513947]\n",
      "[3 2] [-0.96153903 -0.9717023  -1.1913431  -1.7806051 ]\n",
      "[3 3] [-0.14823942 -0.25928116 -0.3466466  -0.76996356]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04526846, -1.24263597, -1.83344007, -2.0403173 ],\n",
       "       [-1.0722723 , -1.18534982, -1.41705728, -1.53563023],\n",
       "       [-2.04933071, -1.47332954, -0.71565205, -0.85674316],\n",
       "       [-2.50775599, -1.79147542, -0.96153903, -0.14823942]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "env = GridworldEnv2DState()\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.008, \n",
    "                       gamma=.9)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_v(td_target, s.get_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\tAverage Score: 9.70 \tother{}\n",
      "Episode 20\tAverage Score: 9.10 \tother{}\n",
      "Episode 30\tAverage Score: 9.50 \tother{}\n",
      "Episode 40\tAverage Score: 9.20 \tother{}\n",
      "Episode 50\tAverage Score: 9.10 \tother{}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "s = SarsaApproximation(env=env, \n",
    "                       num_episodes=50,\n",
    "                       policy=\"epsilon_greedy\",\n",
    "                       num_experience=512,\n",
    "                       epsilon=0.01, \n",
    "                       alpha=0.007, \n",
    "                       gamma=.99)\n",
    "s.train(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

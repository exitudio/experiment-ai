{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [here](https://github.com/moskomule/pytorch.rl.learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approximation_base import ApproximationBase\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -2, -1],\n",
       "       [-1, -2, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sarsa(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _= self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.Q[state_][action_]\n",
    "            td_error = td_target - self.Q[state][action]\n",
    "            self.Q[state][action] += self.alpha * td_error\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = Sarsa(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa lambda (Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, -1, -2],\n",
       "       [ 0, -1, -1, -1],\n",
       "       [-1, -2, -1,  0],\n",
       "       [-2, -1,  0,  0]])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SarsaLambda(TableBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.5, gamma=.9, lambd=0.1):\n",
    "        super().__init__(env, num_episodes, epsilon, alpha, gamma, lambd)\n",
    "        self.Z = self.Q.clone()\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        self.Z.zero_()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.Q[state_, action_]\n",
    "            td_error = td_target - self.Q[state, action]\n",
    "            self.Z[state, action] += 1\n",
    "            self.Q += self.alpha * td_error * self.Z\n",
    "            self.Z = self.gamma * self.lambd * self.Z\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaLambda(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Approximtion (Grid World)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [-0.14976396 -1.3569927  -0.06195487 -0.75239843]\n",
      "[0 1] [-1.25385   -2.7299783 -1.5095493 -1.0155364]\n",
      "[0 2] [-2.3397498 -4.6651506 -2.782865  -1.8875178]\n",
      "[0 3] [-3.4441392 -6.66638   -4.079488  -2.7538733]\n",
      "[1 0] [-1.0204146 -2.569368  -1.5627735 -2.4981616]\n",
      "[1 1] [-1.835434  -3.7967498 -2.4308038 -2.7876065]\n",
      "[1 2] [-2.5968835 -5.521755  -3.4551175 -3.39391  ]\n",
      "[1 3] [-3.4486918 -7.3800316 -4.618518  -4.201066 ]\n",
      "[2 0] [-1.9644141 -4.1391964 -3.1750574 -4.685544 ]\n",
      "[2 1] [-2.575557 -5.295725 -3.727499 -4.803001]\n",
      "[2 2] [-3.2865708 -6.710038  -4.471779  -5.1598663]\n",
      "[2 3] [-4.0161624 -8.435356  -5.5039196 -5.780836 ]\n",
      "[3 0] [-2.8253977 -5.827261  -4.6414165 -6.9541206]\n",
      "[3 1] [-3.3631766 -6.9390755 -5.1094303 -7.009023 ]\n",
      "[3 2] [-4.025936 -8.207821 -5.72496  -7.142169]\n",
      "[3 3] [-4.726742 -9.67179  -6.561225 -7.557959]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06195487, -1.01553643, -1.88751781, -2.75387335],\n",
       "       [-1.02041459, -1.83543396, -2.59688354, -3.44869184],\n",
       "       [-1.96441412, -2.57555699, -3.28657079, -4.0161624 ],\n",
       "       [-2.82539773, -3.36317658, -4.02593613, -4.72674179]])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "env = GridworldEnv()\n",
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.01, alpha=0.01, gamma=.9):\n",
    "        num_state = 2 # width * height\n",
    "        super().__init__(env, \n",
    "                         num_state, \n",
    "                         env.action_space.n, \n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "        self.state_rage = env.observation_space.n ** (1/num_state)\n",
    "        self.experience_replay = new \n",
    "        \n",
    "    def convert_to_2_dimension_state(self, state):\n",
    "        return np.array([math.floor(state/self.state_rage), state%self.state_rage], dtype=int)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.convert_to_2_dimension_state(self.env.reset())\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            state_ = self.convert_to_2_dimension_state(state_)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_)[action_]\n",
    "            qs = self.approximate_q(state)\n",
    "            target_qs = qs.clone().detach()\n",
    "            target_qs[action] = td_target\n",
    "            self.update_weight(target_qs, qs) # ???? detach???\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "#         print(episode, s.convert_Q_to_V())\n",
    "        return total_reward\n",
    "    \n",
    "    def convert_Q_to_V(self):\n",
    "        V = np.array([0.]*self.env.observation_space.n)\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            convert_state = self.convert_to_2_dimension_state(state)\n",
    "            print(convert_state, self.approximate_q(convert_state).detach().numpy())\n",
    "            V[state] = self.approximate_q(convert_state).max().item()\n",
    "        return V.reshape(self.env.shape)\n",
    "        \n",
    "s = SarsaApproximation(env, 50)\n",
    "s.train()\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.07427087  0.07534689 -0.24210957  0.0764288 ]\n",
      "[0 1] [-1.4274087 -1.5826619 -1.6245518 -1.0674704]\n",
      "[0 2] [-2.0183673 -2.4098647 -2.3626473 -1.8950099]\n",
      "[0 3] [-2.2041118 -2.88327   -2.7740805 -2.3001413]\n",
      "[1 0] [-0.85711956 -0.9855417  -1.4769936  -1.1950191 ]\n",
      "[1 1] [-1.5759901 -1.7121133 -1.8472254 -1.4143223]\n",
      "[1 2] [-1.4742002 -1.7901657 -1.8355591 -1.3156751]\n",
      "[1 3] [-1.5201169 -2.13996   -2.0964162 -1.6064342]\n",
      "[2 0] [-1.880239  -1.9784862 -2.69813   -2.435831 ]\n",
      "[2 1] [-1.7874537 -1.7661083 -2.2819705 -1.8166974]\n",
      "[2 2] [-0.97662973 -1.1425152  -1.3656131  -0.8282993 ]\n",
      "[2 3] [-0.71616554 -1.2160507  -1.2582939  -0.735638  ]\n",
      "[3 0] [-2.455274  -2.4101024 -3.2449052 -3.1650648]\n",
      "[3 1] [-1.9490496 -1.9849489 -2.72313   -2.3830712]\n",
      "[3 2] [-1.054707  -1.2291914 -1.7129636 -1.2794075]\n",
      "[3 3] [-0.14162564 -0.51212746 -0.7025315  -0.14466754]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0764288 , -1.06747043, -1.89500988, -2.20411181],\n",
       "       [-0.85711956, -1.41432226, -1.31567514, -1.52011693],\n",
       "       [-1.88023901, -1.76610827, -0.82829928, -0.71616554],\n",
       "       [-2.41010237, -1.94904959, -1.05470705, -0.14162564]])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
    "            [-1.6439, -1.4790, -1.3537, -0.9999],\n",
    "            [-2.1910, -2.2997, -2.0220, -1.9134],\n",
    "            [-2.8211, -2.6443, -2.3472, -2.4686],\n",
    "            [-0.9999, -1.0780, -1.4980, -1.6079],\n",
    "            [-1.8345, -1.8612, -1.7473, -1.6657],\n",
    "            [-1.8404, -2.1300, -2.0000, -2.1527],\n",
    "            [-2.0695, -2.3770, -1.8677, -2.0256],\n",
    "            [-1.8623, -2.0250, -2.4303, -2.0676],\n",
    "            [-2.1694, -1.8296, -2.3622, -1.9963],\n",
    "            [-1.9552, -1.6668, -1.4604, -1.7984],\n",
    "            [-1.0469, -1.5610, -0.9980, -1.0685],\n",
    "            [-2.4361, -2.4313, -2.4637, -2.7323],\n",
    "            [-1.9719, -1.8673, -2.0908, -2.6633],\n",
    "            [-1.4525, -0.9980, -1.5573, -1.9203],\n",
    "            [ 0.0000,  0.0000,  0.0000,  0.0000]])\n",
    "# for i in range(100):\n",
    "#     np.random.choice(Q)\n",
    "# s.update_weight()\n",
    "s = SarsaApproximation(env, 50)\n",
    "for _ in range(500):\n",
    "    idx = np.random.randint(Q.shape[0])\n",
    "    action = np.random.randint(4)\n",
    "    state = np.array([math.floor(idx/4), idx%4], dtype=int)\n",
    "    td_target = Q[idx, action]\n",
    "    s.update_weight(td_target, s.approximate_q(state)[action])\n",
    "s.convert_Q_to_V()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CartPole wiki](https://github.com/openai/gym/wiki/CartPole-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa Aproximation (CartPole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-f6083d4b6fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSarsaApproximation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m# s.convert_Q_to_V()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-264-f6083d4b6fdd>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/table_base.py\u001b[0m in \u001b[0;36mepsilon_greedy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Not support multiple max a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             action = self.argmax([self.approximate_q(state, a)\n\u001b[0;32m---> 73\u001b[0;31m                                   for a in range(self.num_action)])\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/table_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;31m# Not support multiple max a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             action = self.argmax([self.approximate_q(state, a)\n\u001b[0;32m---> 73\u001b[0;31m                                   for a in range(self.num_action)])\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/table_base.py\u001b[0m in \u001b[0;36mapproximate_q\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapproximate_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \"\"\"\n\u001b[0;32m-> 1136\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# env = gym.make('CartPole-v1')\n",
    "env = gym.make('MountainCar-v0')\n",
    "import random\n",
    "class SarsaApproximation(ApproximationBase):\n",
    "    def __init__(self, env, num_episodes, epsilon=0.1, alpha=0.001, gamma=.9):\n",
    "        super().__init__(env,\n",
    "                         env.observation_space.shape[0],\n",
    "                         env.action_space.n,\n",
    "                         num_episodes, \n",
    "                         epsilon, \n",
    "                         alpha, \n",
    "                         gamma)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        policy = self.epsilon_greedy\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action = policy(state)\n",
    "        while not done:\n",
    "            state_, reward, done, _ = self.env.step(action)\n",
    "            action_ = policy(state_)\n",
    "            ########## CORE Algorithm #########\n",
    "            td_target = reward + self.gamma * self.approximate_q(state_, action_)\n",
    "            self.update_weight(td_target, self.approximate_q(state, action))\n",
    "#             td_error = td_target - self.approximate_q(state, action)\n",
    "#             self.weight -= self.alpha * td_error * self.feature(state, action)\n",
    "            ###################################\n",
    "            total_reward += reward\n",
    "            state = state_\n",
    "            action = action_\n",
    "        return total_reward\n",
    "s = SarsaApproximation(env, 50)\n",
    "s.train(True)\n",
    "# s.convert_Q_to_V()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

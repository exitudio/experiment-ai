{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reference](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb) <br>\n",
    "[pytorch ](https://pytorch.org/docs/stable/distributions.html) <br>\n",
    "[1st rank cartpole](https://github.com/udacity/deep-reinforcement-learning/blob/master/hill-climbing/Hill_Climbing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from gridworld_env_2d_state import GridworldEnv2DState\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 12.0\n",
      "episode: 1 reward: 23.0\n",
      "episode: 2 reward: 26.0\n",
      "episode: 3 reward: 11.0\n",
      "episode: 4 reward: 12.0\n",
      "episode: 5 reward: 11.0\n",
      "episode: 6 reward: 13.0\n",
      "episode: 7 reward: 20.0\n",
      "episode: 8 reward: 15.0\n",
      "episode: 9 reward: 11.0\n",
      "episode: 10 reward: 11.0\n",
      "episode: 11 reward: 25.0\n",
      "episode: 12 reward: 14.0\n",
      "episode: 13 reward: 11.0\n",
      "episode: 14 reward: 22.0\n",
      "episode: 15 reward: 19.0\n",
      "episode: 16 reward: 15.0\n",
      "episode: 17 reward: 32.0\n",
      "episode: 18 reward: 23.0\n",
      "episode: 19 reward: 15.0\n",
      "episode: 20 reward: 17.0\n",
      "episode: 21 reward: 26.0\n",
      "episode: 22 reward: 13.0\n",
      "episode: 23 reward: 33.0\n",
      "episode: 24 reward: 41.0\n",
      "episode: 25 reward: 29.0\n",
      "episode: 26 reward: 23.0\n",
      "episode: 27 reward: 33.0\n",
      "episode: 28 reward: 17.0\n",
      "episode: 29 reward: 16.0\n",
      "episode: 30 reward: 11.0\n",
      "episode: 31 reward: 59.0\n",
      "episode: 32 reward: 37.0\n",
      "episode: 33 reward: 17.0\n",
      "episode: 34 reward: 21.0\n",
      "episode: 35 reward: 21.0\n",
      "episode: 36 reward: 33.0\n",
      "episode: 37 reward: 11.0\n",
      "episode: 38 reward: 13.0\n",
      "episode: 39 reward: 25.0\n",
      "episode: 40 reward: 20.0\n",
      "episode: 41 reward: 54.0\n",
      "episode: 42 reward: 16.0\n",
      "episode: 43 reward: 33.0\n",
      "episode: 44 reward: 18.0\n",
      "episode: 45 reward: 24.0\n",
      "episode: 46 reward: 35.0\n",
      "episode: 47 reward: 23.0\n",
      "episode: 48 reward: 15.0\n",
      "episode: 49 reward: 74.0\n",
      "episode: 50 reward: 25.0\n",
      "episode: 51 reward: 30.0\n",
      "episode: 52 reward: 21.0\n",
      "episode: 53 reward: 18.0\n",
      "episode: 54 reward: 103.0\n",
      "episode: 55 reward: 13.0\n",
      "episode: 56 reward: 23.0\n",
      "episode: 57 reward: 21.0\n",
      "episode: 58 reward: 15.0\n",
      "episode: 59 reward: 32.0\n",
      "episode: 60 reward: 50.0\n",
      "episode: 61 reward: 53.0\n",
      "episode: 62 reward: 31.0\n",
      "episode: 63 reward: 21.0\n",
      "episode: 64 reward: 43.0\n",
      "episode: 65 reward: 22.0\n",
      "episode: 66 reward: 50.0\n",
      "episode: 67 reward: 44.0\n",
      "episode: 68 reward: 16.0\n",
      "episode: 69 reward: 40.0\n",
      "episode: 70 reward: 33.0\n",
      "episode: 71 reward: 21.0\n",
      "episode: 72 reward: 56.0\n",
      "episode: 73 reward: 30.0\n",
      "episode: 74 reward: 44.0\n",
      "episode: 75 reward: 32.0\n",
      "episode: 76 reward: 50.0\n",
      "episode: 77 reward: 86.0\n",
      "episode: 78 reward: 42.0\n",
      "episode: 79 reward: 25.0\n",
      "episode: 80 reward: 44.0\n",
      "episode: 81 reward: 41.0\n",
      "episode: 82 reward: 34.0\n",
      "episode: 83 reward: 14.0\n",
      "episode: 84 reward: 44.0\n",
      "episode: 85 reward: 10.0\n",
      "episode: 86 reward: 33.0\n",
      "episode: 87 reward: 38.0\n",
      "episode: 88 reward: 26.0\n",
      "episode: 89 reward: 50.0\n",
      "episode: 90 reward: 40.0\n",
      "episode: 91 reward: 27.0\n",
      "episode: 92 reward: 125.0\n",
      "episode: 93 reward: 22.0\n",
      "episode: 94 reward: 104.0\n",
      "episode: 95 reward: 11.0\n",
      "episode: 96 reward: 26.0\n",
      "episode: 97 reward: 36.0\n",
      "episode: 98 reward: 35.0\n",
      "episode: 99 reward: 25.0\n",
      "episode: 100 reward: 16.0\n",
      "episode: 101 reward: 48.0\n",
      "episode: 102 reward: 50.0\n",
      "episode: 103 reward: 30.0\n",
      "episode: 104 reward: 29.0\n",
      "episode: 105 reward: 47.0\n",
      "episode: 106 reward: 45.0\n",
      "episode: 107 reward: 70.0\n",
      "episode: 108 reward: 32.0\n",
      "episode: 109 reward: 50.0\n",
      "episode: 110 reward: 23.0\n",
      "episode: 111 reward: 76.0\n",
      "episode: 112 reward: 29.0\n",
      "episode: 113 reward: 39.0\n",
      "episode: 114 reward: 41.0\n",
      "episode: 115 reward: 110.0\n",
      "episode: 116 reward: 38.0\n",
      "episode: 117 reward: 43.0\n",
      "episode: 118 reward: 67.0\n",
      "episode: 119 reward: 66.0\n",
      "episode: 120 reward: 20.0\n",
      "episode: 121 reward: 65.0\n",
      "episode: 122 reward: 48.0\n",
      "episode: 123 reward: 76.0\n",
      "episode: 124 reward: 48.0\n",
      "episode: 125 reward: 48.0\n",
      "episode: 126 reward: 102.0\n",
      "episode: 127 reward: 34.0\n",
      "episode: 128 reward: 70.0\n",
      "episode: 129 reward: 56.0\n",
      "episode: 130 reward: 72.0\n",
      "episode: 131 reward: 95.0\n",
      "episode: 132 reward: 83.0\n",
      "episode: 133 reward: 50.0\n",
      "episode: 134 reward: 58.0\n",
      "episode: 135 reward: 16.0\n",
      "episode: 136 reward: 67.0\n",
      "episode: 137 reward: 57.0\n",
      "episode: 138 reward: 56.0\n",
      "episode: 139 reward: 39.0\n",
      "episode: 140 reward: 57.0\n",
      "episode: 141 reward: 136.0\n",
      "episode: 142 reward: 32.0\n",
      "episode: 143 reward: 98.0\n",
      "episode: 144 reward: 62.0\n",
      "episode: 145 reward: 59.0\n",
      "episode: 146 reward: 102.0\n",
      "episode: 147 reward: 221.0\n",
      "episode: 148 reward: 46.0\n",
      "episode: 149 reward: 146.0\n",
      "episode: 150 reward: 83.0\n",
      "episode: 151 reward: 123.0\n",
      "episode: 152 reward: 24.0\n",
      "episode: 153 reward: 75.0\n",
      "episode: 154 reward: 100.0\n",
      "episode: 155 reward: 84.0\n",
      "episode: 156 reward: 93.0\n",
      "episode: 157 reward: 98.0\n",
      "episode: 158 reward: 53.0\n",
      "episode: 159 reward: 97.0\n",
      "episode: 160 reward: 169.0\n",
      "episode: 161 reward: 123.0\n",
      "episode: 162 reward: 162.0\n",
      "episode: 163 reward: 60.0\n",
      "episode: 164 reward: 103.0\n",
      "episode: 165 reward: 102.0\n",
      "episode: 166 reward: 105.0\n",
      "episode: 167 reward: 140.0\n",
      "episode: 168 reward: 101.0\n",
      "episode: 169 reward: 38.0\n",
      "episode: 170 reward: 83.0\n",
      "episode: 171 reward: 95.0\n",
      "episode: 172 reward: 113.0\n",
      "episode: 173 reward: 24.0\n",
      "episode: 174 reward: 48.0\n",
      "episode: 175 reward: 74.0\n",
      "episode: 176 reward: 75.0\n",
      "episode: 177 reward: 126.0\n",
      "episode: 178 reward: 188.0\n",
      "episode: 179 reward: 73.0\n",
      "episode: 180 reward: 107.0\n",
      "episode: 181 reward: 69.0\n",
      "episode: 182 reward: 16.0\n",
      "episode: 183 reward: 62.0\n",
      "episode: 184 reward: 101.0\n",
      "episode: 185 reward: 237.0\n",
      "episode: 186 reward: 39.0\n",
      "episode: 187 reward: 134.0\n",
      "episode: 188 reward: 175.0\n",
      "episode: 189 reward: 139.0\n",
      "episode: 190 reward: 134.0\n",
      "episode: 191 reward: 182.0\n",
      "episode: 192 reward: 194.0\n",
      "episode: 193 reward: 97.0\n",
      "episode: 194 reward: 301.0\n",
      "episode: 195 reward: 47.0\n",
      "episode: 196 reward: 226.0\n",
      "episode: 197 reward: 181.0\n",
      "episode: 198 reward: 190.0\n",
      "episode: 199 reward: 106.0\n",
      "episode: 200 reward: 150.0\n",
      "episode: 201 reward: 237.0\n",
      "episode: 202 reward: 147.0\n",
      "episode: 203 reward: 120.0\n",
      "episode: 204 reward: 165.0\n",
      "episode: 205 reward: 104.0\n",
      "episode: 206 reward: 146.0\n",
      "episode: 207 reward: 47.0\n",
      "episode: 208 reward: 131.0\n",
      "episode: 209 reward: 161.0\n",
      "episode: 210 reward: 59.0\n",
      "episode: 211 reward: 168.0\n",
      "episode: 212 reward: 122.0\n",
      "episode: 213 reward: 29.0\n",
      "episode: 214 reward: 121.0\n",
      "episode: 215 reward: 162.0\n",
      "episode: 216 reward: 143.0\n",
      "episode: 217 reward: 141.0\n",
      "episode: 218 reward: 156.0\n",
      "episode: 219 reward: 185.0\n",
      "episode: 220 reward: 155.0\n",
      "episode: 221 reward: 150.0\n",
      "episode: 222 reward: 140.0\n",
      "episode: 223 reward: 207.0\n",
      "episode: 224 reward: 119.0\n",
      "episode: 225 reward: 227.0\n",
      "episode: 226 reward: 315.0\n",
      "episode: 227 reward: 195.0\n",
      "episode: 228 reward: 185.0\n",
      "episode: 229 reward: 288.0\n",
      "episode: 230 reward: 192.0\n",
      "episode: 231 reward: 233.0\n",
      "episode: 232 reward: 195.0\n",
      "episode: 233 reward: 178.0\n",
      "episode: 234 reward: 199.0\n",
      "episode: 235 reward: 379.0\n",
      "episode: 236 reward: 235.0\n",
      "episode: 237 reward: 124.0\n",
      "episode: 238 reward: 125.0\n",
      "episode: 239 reward: 174.0\n",
      "episode: 240 reward: 200.0\n",
      "episode: 241 reward: 85.0\n",
      "episode: 242 reward: 175.0\n",
      "episode: 243 reward: 170.0\n",
      "episode: 244 reward: 146.0\n",
      "episode: 245 reward: 408.0\n",
      "episode: 246 reward: 94.0\n",
      "episode: 247 reward: 96.0\n",
      "episode: 248 reward: 194.0\n",
      "episode: 249 reward: 200.0\n",
      "episode: 250 reward: 157.0\n",
      "episode: 251 reward: 266.0\n",
      "episode: 252 reward: 500.0\n",
      "episode: 253 reward: 363.0\n",
      "episode: 254 reward: 144.0\n",
      "episode: 255 reward: 315.0\n",
      "episode: 256 reward: 196.0\n",
      "episode: 257 reward: 165.0\n",
      "episode: 258 reward: 280.0\n",
      "episode: 259 reward: 375.0\n",
      "episode: 260 reward: 232.0\n",
      "episode: 261 reward: 203.0\n",
      "episode: 262 reward: 168.0\n",
      "episode: 263 reward: 227.0\n",
      "episode: 264 reward: 280.0\n",
      "episode: 265 reward: 149.0\n",
      "episode: 266 reward: 169.0\n",
      "episode: 267 reward: 198.0\n",
      "episode: 268 reward: 175.0\n",
      "episode: 269 reward: 180.0\n",
      "episode: 270 reward: 173.0\n",
      "episode: 271 reward: 187.0\n",
      "episode: 272 reward: 182.0\n",
      "episode: 273 reward: 269.0\n",
      "episode: 274 reward: 164.0\n",
      "episode: 275 reward: 217.0\n",
      "episode: 276 reward: 171.0\n",
      "episode: 277 reward: 161.0\n",
      "episode: 278 reward: 174.0\n",
      "episode: 279 reward: 207.0\n",
      "episode: 280 reward: 211.0\n",
      "episode: 281 reward: 169.0\n",
      "episode: 282 reward: 175.0\n",
      "episode: 283 reward: 210.0\n",
      "episode: 284 reward: 158.0\n",
      "episode: 285 reward: 192.0\n",
      "episode: 286 reward: 189.0\n",
      "episode: 287 reward: 399.0\n",
      "episode: 288 reward: 214.0\n",
      "episode: 289 reward: 230.0\n",
      "episode: 290 reward: 214.0\n",
      "episode: 291 reward: 240.0\n",
      "episode: 292 reward: 220.0\n",
      "episode: 293 reward: 171.0\n",
      "episode: 294 reward: 242.0\n",
      "episode: 295 reward: 246.0\n",
      "episode: 296 reward: 315.0\n",
      "episode: 297 reward: 268.0\n",
      "episode: 298 reward: 257.0\n",
      "episode: 299 reward: 258.0\n",
      "episode: 300 reward: 309.0\n",
      "episode: 301 reward: 218.0\n",
      "episode: 302 reward: 316.0\n",
      "episode: 303 reward: 339.0\n",
      "episode: 304 reward: 324.0\n",
      "episode: 305 reward: 283.0\n",
      "episode: 306 reward: 372.0\n",
      "episode: 307 reward: 326.0\n",
      "episode: 308 reward: 444.0\n",
      "episode: 309 reward: 323.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 310 reward: 353.0\n",
      "episode: 311 reward: 500.0\n",
      "episode: 312 reward: 388.0\n",
      "episode: 313 reward: 500.0\n",
      "episode: 314 reward: 469.0\n",
      "episode: 315 reward: 353.0\n",
      "episode: 316 reward: 437.0\n",
      "episode: 317 reward: 386.0\n",
      "episode: 318 reward: 333.0\n",
      "episode: 319 reward: 490.0\n",
      "episode: 320 reward: 379.0\n",
      "episode: 321 reward: 453.0\n",
      "episode: 322 reward: 482.0\n",
      "episode: 323 reward: 428.0\n",
      "episode: 324 reward: 323.0\n",
      "episode: 325 reward: 439.0\n",
      "episode: 326 reward: 434.0\n",
      "episode: 327 reward: 338.0\n",
      "episode: 328 reward: 500.0\n",
      "episode: 329 reward: 348.0\n",
      "episode: 330 reward: 347.0\n",
      "episode: 331 reward: 494.0\n",
      "episode: 332 reward: 464.0\n",
      "episode: 333 reward: 62.0\n",
      "episode: 334 reward: 472.0\n",
      "episode: 335 reward: 398.0\n",
      "episode: 336 reward: 424.0\n",
      "episode: 337 reward: 254.0\n",
      "episode: 338 reward: 370.0\n",
      "episode: 339 reward: 328.0\n",
      "episode: 340 reward: 372.0\n",
      "episode: 341 reward: 392.0\n",
      "episode: 342 reward: 397.0\n",
      "episode: 343 reward: 73.0\n",
      "episode: 344 reward: 345.0\n",
      "episode: 345 reward: 317.0\n",
      "episode: 346 reward: 351.0\n",
      "episode: 347 reward: 373.0\n",
      "episode: 348 reward: 352.0\n",
      "episode: 349 reward: 349.0\n",
      "episode: 350 reward: 445.0\n",
      "episode: 351 reward: 347.0\n",
      "episode: 352 reward: 274.0\n",
      "episode: 353 reward: 407.0\n",
      "episode: 354 reward: 432.0\n",
      "episode: 355 reward: 317.0\n",
      "episode: 356 reward: 441.0\n",
      "episode: 357 reward: 452.0\n",
      "episode: 358 reward: 391.0\n",
      "episode: 359 reward: 451.0\n",
      "episode: 360 reward: 485.0\n",
      "episode: 361 reward: 473.0\n",
      "episode: 362 reward: 500.0\n",
      "episode: 363 reward: 500.0\n",
      "episode: 364 reward: 455.0\n",
      "episode: 365 reward: 500.0\n",
      "episode: 366 reward: 444.0\n",
      "episode: 367 reward: 500.0\n",
      "episode: 368 reward: 500.0\n",
      "episode: 369 reward: 231.0\n",
      "episode: 370 reward: 496.0\n",
      "episode: 371 reward: 388.0\n",
      "episode: 372 reward: 69.0\n",
      "episode: 373 reward: 73.0\n",
      "episode: 374 reward: 163.0\n",
      "episode: 375 reward: 185.0\n",
      "episode: 376 reward: 500.0\n",
      "episode: 377 reward: 332.0\n",
      "episode: 378 reward: 500.0\n",
      "episode: 379 reward: 500.0\n",
      "episode: 380 reward: 114.0\n",
      "episode: 381 reward: 98.0\n",
      "episode: 382 reward: 105.0\n",
      "episode: 383 reward: 500.0\n",
      "episode: 384 reward: 500.0\n",
      "episode: 385 reward: 500.0\n",
      "episode: 386 reward: 147.0\n",
      "episode: 387 reward: 500.0\n",
      "episode: 388 reward: 500.0\n",
      "episode: 389 reward: 500.0\n",
      "episode: 390 reward: 500.0\n",
      "episode: 391 reward: 306.0\n",
      "episode: 392 reward: 500.0\n",
      "episode: 393 reward: 500.0\n",
      "episode: 394 reward: 500.0\n",
      "episode: 395 reward: 500.0\n",
      "episode: 396 reward: 207.0\n",
      "episode: 397 reward: 19.0\n",
      "episode: 398 reward: 401.0\n",
      "episode: 399 reward: 62.0\n",
      "episode: 400 reward: 158.0\n",
      "episode: 401 reward: 49.0\n",
      "episode: 402 reward: 187.0\n",
      "episode: 403 reward: 25.0\n",
      "episode: 404 reward: 426.0\n",
      "episode: 405 reward: 467.0\n",
      "episode: 406 reward: 500.0\n",
      "episode: 407 reward: 179.0\n",
      "episode: 408 reward: 351.0\n",
      "episode: 409 reward: 188.0\n",
      "episode: 410 reward: 129.0\n",
      "episode: 411 reward: 500.0\n",
      "episode: 412 reward: 95.0\n",
      "episode: 413 reward: 124.0\n",
      "episode: 414 reward: 500.0\n",
      "episode: 415 reward: 500.0\n",
      "episode: 416 reward: 82.0\n",
      "episode: 417 reward: 58.0\n",
      "episode: 418 reward: 270.0\n",
      "episode: 419 reward: 346.0\n",
      "episode: 420 reward: 500.0\n",
      "episode: 421 reward: 341.0\n",
      "episode: 422 reward: 331.0\n",
      "episode: 423 reward: 500.0\n",
      "episode: 424 reward: 146.0\n",
      "episode: 425 reward: 417.0\n",
      "episode: 426 reward: 500.0\n",
      "episode: 427 reward: 500.0\n",
      "episode: 428 reward: 500.0\n",
      "episode: 429 reward: 339.0\n",
      "episode: 430 reward: 236.0\n",
      "episode: 431 reward: 500.0\n",
      "episode: 432 reward: 114.0\n",
      "episode: 433 reward: 500.0\n",
      "episode: 434 reward: 99.0\n",
      "episode: 435 reward: 131.0\n",
      "episode: 436 reward: 415.0\n",
      "episode: 437 reward: 312.0\n",
      "episode: 438 reward: 241.0\n",
      "episode: 439 reward: 65.0\n",
      "episode: 440 reward: 500.0\n",
      "episode: 441 reward: 276.0\n",
      "episode: 442 reward: 500.0\n",
      "episode: 443 reward: 99.0\n",
      "episode: 444 reward: 500.0\n",
      "episode: 445 reward: 257.0\n",
      "episode: 446 reward: 129.0\n",
      "episode: 447 reward: 500.0\n",
      "episode: 448 reward: 211.0\n",
      "episode: 449 reward: 180.0\n",
      "episode: 450 reward: 53.0\n",
      "episode: 451 reward: 500.0\n",
      "episode: 452 reward: 299.0\n",
      "episode: 453 reward: 360.0\n",
      "episode: 454 reward: 500.0\n",
      "episode: 455 reward: 500.0\n",
      "episode: 456 reward: 500.0\n",
      "episode: 457 reward: 500.0\n",
      "episode: 458 reward: 500.0\n",
      "episode: 459 reward: 75.0\n",
      "episode: 460 reward: 255.0\n",
      "episode: 461 reward: 31.0\n",
      "episode: 462 reward: 500.0\n",
      "episode: 463 reward: 500.0\n",
      "episode: 464 reward: 500.0\n",
      "episode: 465 reward: 500.0\n",
      "episode: 466 reward: 500.0\n",
      "episode: 467 reward: 500.0\n",
      "episode: 468 reward: 500.0\n",
      "episode: 469 reward: 500.0\n",
      "episode: 470 reward: 500.0\n",
      "episode: 471 reward: 500.0\n",
      "episode: 472 reward: 251.0\n",
      "episode: 473 reward: 500.0\n",
      "episode: 474 reward: 250.0\n",
      "episode: 475 reward: 500.0\n",
      "episode: 476 reward: 420.0\n",
      "episode: 477 reward: 500.0\n",
      "episode: 478 reward: 500.0\n",
      "episode: 479 reward: 306.0\n",
      "episode: 480 reward: 29.0\n",
      "episode: 481 reward: 277.0\n",
      "episode: 482 reward: 134.0\n",
      "episode: 483 reward: 500.0\n",
      "episode: 484 reward: 298.0\n",
      "episode: 485 reward: 500.0\n",
      "episode: 486 reward: 306.0\n",
      "episode: 487 reward: 425.0\n",
      "episode: 488 reward: 500.0\n",
      "episode: 489 reward: 411.0\n",
      "episode: 490 reward: 500.0\n",
      "episode: 491 reward: 106.0\n",
      "episode: 492 reward: 500.0\n",
      "episode: 493 reward: 346.0\n",
      "episode: 494 reward: 500.0\n",
      "episode: 495 reward: 157.0\n",
      "episode: 496 reward: 500.0\n",
      "episode: 497 reward: 500.0\n",
      "episode: 498 reward: 169.0\n",
      "episode: 499 reward: 500.0\n",
      "episode: 500 reward: 225.0\n",
      "episode: 501 reward: 500.0\n",
      "episode: 502 reward: 45.0\n",
      "episode: 503 reward: 500.0\n",
      "episode: 504 reward: 496.0\n",
      "episode: 505 reward: 500.0\n",
      "episode: 506 reward: 40.0\n",
      "episode: 507 reward: 52.0\n",
      "episode: 508 reward: 500.0\n",
      "episode: 509 reward: 189.0\n",
      "episode: 510 reward: 500.0\n",
      "episode: 511 reward: 210.0\n",
      "episode: 512 reward: 500.0\n",
      "episode: 513 reward: 54.0\n",
      "episode: 514 reward: 500.0\n",
      "episode: 515 reward: 500.0\n",
      "episode: 516 reward: 500.0\n",
      "episode: 517 reward: 500.0\n",
      "episode: 518 reward: 500.0\n",
      "episode: 519 reward: 500.0\n",
      "episode: 520 reward: 500.0\n",
      "episode: 521 reward: 500.0\n",
      "episode: 522 reward: 358.0\n",
      "episode: 523 reward: 473.0\n",
      "episode: 524 reward: 500.0\n",
      "episode: 525 reward: 118.0\n",
      "episode: 526 reward: 498.0\n",
      "episode: 527 reward: 317.0\n",
      "episode: 528 reward: 500.0\n",
      "episode: 529 reward: 235.0\n",
      "episode: 530 reward: 494.0\n",
      "episode: 531 reward: 307.0\n",
      "episode: 532 reward: 66.0\n",
      "episode: 533 reward: 246.0\n",
      "episode: 534 reward: 496.0\n",
      "episode: 535 reward: 56.0\n",
      "episode: 536 reward: 131.0\n",
      "episode: 537 reward: 362.0\n",
      "episode: 538 reward: 43.0\n",
      "episode: 539 reward: 158.0\n",
      "episode: 540 reward: 227.0\n",
      "episode: 541 reward: 260.0\n",
      "episode: 542 reward: 48.0\n",
      "episode: 543 reward: 290.0\n",
      "episode: 544 reward: 500.0\n",
      "episode: 545 reward: 383.0\n",
      "episode: 546 reward: 500.0\n",
      "episode: 547 reward: 500.0\n",
      "episode: 548 reward: 500.0\n",
      "episode: 549 reward: 500.0\n",
      "episode: 550 reward: 500.0\n",
      "episode: 551 reward: 384.0\n",
      "episode: 552 reward: 500.0\n",
      "episode: 553 reward: 500.0\n",
      "episode: 554 reward: 432.0\n",
      "episode: 555 reward: 500.0\n",
      "episode: 556 reward: 180.0\n",
      "episode: 557 reward: 500.0\n",
      "episode: 558 reward: 422.0\n",
      "episode: 559 reward: 500.0\n",
      "episode: 560 reward: 500.0\n",
      "episode: 561 reward: 354.0\n",
      "episode: 562 reward: 282.0\n",
      "episode: 563 reward: 500.0\n",
      "episode: 564 reward: 445.0\n",
      "episode: 565 reward: 500.0\n",
      "episode: 566 reward: 500.0\n",
      "episode: 567 reward: 500.0\n",
      "episode: 568 reward: 500.0\n",
      "episode: 569 reward: 495.0\n",
      "episode: 570 reward: 500.0\n",
      "episode: 571 reward: 322.0\n",
      "episode: 572 reward: 404.0\n",
      "episode: 573 reward: 500.0\n",
      "episode: 574 reward: 500.0\n",
      "episode: 575 reward: 135.0\n",
      "episode: 576 reward: 428.0\n",
      "episode: 577 reward: 500.0\n",
      "episode: 578 reward: 500.0\n",
      "episode: 579 reward: 500.0\n",
      "episode: 580 reward: 357.0\n",
      "episode: 581 reward: 336.0\n",
      "episode: 582 reward: 449.0\n",
      "episode: 583 reward: 395.0\n",
      "episode: 584 reward: 356.0\n",
      "episode: 585 reward: 383.0\n",
      "episode: 586 reward: 209.0\n",
      "episode: 587 reward: 288.0\n",
      "episode: 588 reward: 393.0\n",
      "episode: 589 reward: 409.0\n",
      "episode: 590 reward: 494.0\n",
      "episode: 591 reward: 455.0\n",
      "episode: 592 reward: 150.0\n",
      "episode: 593 reward: 322.0\n",
      "episode: 594 reward: 338.0\n",
      "episode: 595 reward: 500.0\n",
      "episode: 596 reward: 18.0\n",
      "episode: 597 reward: 479.0\n",
      "episode: 598 reward: 224.0\n",
      "episode: 599 reward: 397.0\n",
      "episode: 600 reward: 245.0\n",
      "episode: 601 reward: 292.0\n",
      "episode: 602 reward: 184.0\n",
      "episode: 603 reward: 500.0\n",
      "episode: 604 reward: 500.0\n",
      "episode: 605 reward: 332.0\n",
      "episode: 606 reward: 500.0\n",
      "episode: 607 reward: 379.0\n",
      "episode: 608 reward: 500.0\n",
      "episode: 609 reward: 500.0\n",
      "episode: 610 reward: 477.0\n",
      "episode: 611 reward: 500.0\n",
      "episode: 612 reward: 500.0\n",
      "episode: 613 reward: 500.0\n",
      "episode: 614 reward: 405.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 615 reward: 500.0\n",
      "episode: 616 reward: 493.0\n",
      "episode: 617 reward: 370.0\n",
      "episode: 618 reward: 500.0\n",
      "episode: 619 reward: 500.0\n",
      "episode: 620 reward: 500.0\n",
      "episode: 621 reward: 500.0\n",
      "episode: 622 reward: 500.0\n",
      "episode: 623 reward: 500.0\n",
      "episode: 624 reward: 500.0\n",
      "episode: 625 reward: 500.0\n",
      "episode: 626 reward: 500.0\n",
      "episode: 627 reward: 227.0\n",
      "episode: 628 reward: 500.0\n",
      "episode: 629 reward: 500.0\n",
      "episode: 630 reward: 91.0\n",
      "episode: 631 reward: 500.0\n",
      "episode: 632 reward: 294.0\n",
      "episode: 633 reward: 413.0\n",
      "episode: 634 reward: 474.0\n",
      "episode: 635 reward: 265.0\n",
      "episode: 636 reward: 384.0\n",
      "episode: 637 reward: 302.0\n",
      "episode: 638 reward: 282.0\n",
      "episode: 639 reward: 312.0\n",
      "episode: 640 reward: 297.0\n",
      "episode: 641 reward: 276.0\n",
      "episode: 642 reward: 243.0\n",
      "episode: 643 reward: 308.0\n",
      "episode: 644 reward: 190.0\n",
      "episode: 645 reward: 234.0\n",
      "episode: 646 reward: 223.0\n",
      "episode: 647 reward: 201.0\n",
      "episode: 648 reward: 219.0\n",
      "episode: 649 reward: 212.0\n",
      "episode: 650 reward: 253.0\n",
      "episode: 651 reward: 214.0\n",
      "episode: 652 reward: 241.0\n",
      "episode: 653 reward: 278.0\n",
      "episode: 654 reward: 211.0\n",
      "episode: 655 reward: 186.0\n",
      "episode: 656 reward: 231.0\n",
      "episode: 657 reward: 167.0\n",
      "episode: 658 reward: 155.0\n",
      "episode: 659 reward: 264.0\n",
      "episode: 660 reward: 104.0\n",
      "episode: 661 reward: 170.0\n",
      "episode: 662 reward: 155.0\n",
      "episode: 663 reward: 209.0\n",
      "episode: 664 reward: 247.0\n",
      "episode: 665 reward: 182.0\n",
      "episode: 666 reward: 161.0\n",
      "episode: 667 reward: 213.0\n",
      "episode: 668 reward: 125.0\n",
      "episode: 669 reward: 210.0\n",
      "episode: 670 reward: 138.0\n",
      "episode: 671 reward: 239.0\n",
      "episode: 672 reward: 19.0\n",
      "episode: 673 reward: 213.0\n",
      "episode: 674 reward: 163.0\n",
      "episode: 675 reward: 212.0\n",
      "episode: 676 reward: 150.0\n",
      "episode: 677 reward: 160.0\n",
      "episode: 678 reward: 194.0\n",
      "episode: 679 reward: 179.0\n",
      "episode: 680 reward: 187.0\n",
      "episode: 681 reward: 226.0\n",
      "episode: 682 reward: 201.0\n",
      "episode: 683 reward: 166.0\n",
      "episode: 684 reward: 214.0\n",
      "episode: 685 reward: 186.0\n",
      "episode: 686 reward: 185.0\n",
      "episode: 687 reward: 118.0\n",
      "episode: 688 reward: 167.0\n",
      "episode: 689 reward: 218.0\n",
      "episode: 690 reward: 214.0\n",
      "episode: 691 reward: 170.0\n",
      "episode: 692 reward: 237.0\n",
      "episode: 693 reward: 172.0\n",
      "episode: 694 reward: 27.0\n",
      "episode: 695 reward: 18.0\n",
      "episode: 696 reward: 211.0\n",
      "episode: 697 reward: 194.0\n",
      "episode: 698 reward: 232.0\n",
      "episode: 699 reward: 145.0\n",
      "episode: 700 reward: 198.0\n",
      "episode: 701 reward: 198.0\n",
      "episode: 702 reward: 205.0\n",
      "episode: 703 reward: 164.0\n",
      "episode: 704 reward: 173.0\n",
      "episode: 705 reward: 134.0\n",
      "episode: 706 reward: 196.0\n",
      "episode: 707 reward: 204.0\n",
      "episode: 708 reward: 212.0\n",
      "episode: 709 reward: 185.0\n",
      "episode: 710 reward: 156.0\n",
      "episode: 711 reward: 153.0\n",
      "episode: 712 reward: 219.0\n",
      "episode: 713 reward: 17.0\n",
      "episode: 714 reward: 179.0\n",
      "episode: 715 reward: 210.0\n",
      "episode: 716 reward: 253.0\n",
      "episode: 717 reward: 178.0\n",
      "episode: 718 reward: 249.0\n",
      "episode: 719 reward: 158.0\n",
      "episode: 720 reward: 179.0\n",
      "episode: 721 reward: 196.0\n",
      "episode: 722 reward: 201.0\n",
      "episode: 723 reward: 237.0\n",
      "episode: 724 reward: 172.0\n",
      "episode: 725 reward: 209.0\n",
      "episode: 726 reward: 169.0\n",
      "episode: 727 reward: 186.0\n",
      "episode: 728 reward: 170.0\n",
      "episode: 729 reward: 212.0\n",
      "episode: 730 reward: 33.0\n",
      "episode: 731 reward: 236.0\n",
      "episode: 732 reward: 188.0\n",
      "episode: 733 reward: 205.0\n",
      "episode: 734 reward: 134.0\n",
      "episode: 735 reward: 239.0\n",
      "episode: 736 reward: 201.0\n",
      "episode: 737 reward: 209.0\n",
      "episode: 738 reward: 21.0\n",
      "episode: 739 reward: 146.0\n",
      "episode: 740 reward: 203.0\n",
      "episode: 741 reward: 127.0\n",
      "episode: 742 reward: 205.0\n",
      "episode: 743 reward: 166.0\n",
      "episode: 744 reward: 182.0\n",
      "episode: 745 reward: 171.0\n",
      "episode: 746 reward: 264.0\n",
      "episode: 747 reward: 150.0\n",
      "episode: 748 reward: 135.0\n",
      "episode: 749 reward: 190.0\n",
      "episode: 750 reward: 224.0\n",
      "episode: 751 reward: 122.0\n",
      "episode: 752 reward: 11.0\n",
      "episode: 753 reward: 206.0\n",
      "episode: 754 reward: 13.0\n",
      "episode: 755 reward: 106.0\n",
      "episode: 756 reward: 16.0\n",
      "episode: 757 reward: 149.0\n",
      "episode: 758 reward: 178.0\n",
      "episode: 759 reward: 25.0\n",
      "episode: 760 reward: 201.0\n",
      "episode: 761 reward: 18.0\n",
      "episode: 762 reward: 158.0\n",
      "episode: 763 reward: 18.0\n",
      "episode: 764 reward: 178.0\n",
      "episode: 765 reward: 184.0\n",
      "episode: 766 reward: 171.0\n",
      "episode: 767 reward: 198.0\n",
      "episode: 768 reward: 164.0\n",
      "episode: 769 reward: 222.0\n",
      "episode: 770 reward: 19.0\n",
      "episode: 771 reward: 18.0\n",
      "episode: 772 reward: 22.0\n",
      "episode: 773 reward: 96.0\n",
      "episode: 774 reward: 149.0\n",
      "episode: 775 reward: 167.0\n",
      "episode: 776 reward: 227.0\n",
      "episode: 777 reward: 179.0\n",
      "episode: 778 reward: 240.0\n",
      "episode: 779 reward: 135.0\n",
      "episode: 780 reward: 197.0\n",
      "episode: 781 reward: 227.0\n",
      "episode: 782 reward: 141.0\n",
      "episode: 783 reward: 223.0\n",
      "episode: 784 reward: 255.0\n",
      "episode: 785 reward: 200.0\n",
      "episode: 786 reward: 17.0\n",
      "episode: 787 reward: 233.0\n",
      "episode: 788 reward: 214.0\n",
      "episode: 789 reward: 233.0\n",
      "episode: 790 reward: 166.0\n",
      "episode: 791 reward: 25.0\n",
      "episode: 792 reward: 271.0\n",
      "episode: 793 reward: 157.0\n",
      "episode: 794 reward: 276.0\n",
      "episode: 795 reward: 265.0\n",
      "episode: 796 reward: 282.0\n",
      "episode: 797 reward: 327.0\n",
      "episode: 798 reward: 181.0\n",
      "episode: 799 reward: 435.0\n",
      "episode: 800 reward: 331.0\n",
      "episode: 801 reward: 32.0\n",
      "episode: 802 reward: 431.0\n",
      "episode: 803 reward: 293.0\n",
      "episode: 804 reward: 240.0\n",
      "episode: 805 reward: 280.0\n",
      "episode: 806 reward: 267.0\n",
      "episode: 807 reward: 121.0\n",
      "episode: 808 reward: 393.0\n",
      "episode: 809 reward: 299.0\n",
      "episode: 810 reward: 257.0\n",
      "episode: 811 reward: 320.0\n",
      "episode: 812 reward: 228.0\n",
      "episode: 813 reward: 277.0\n",
      "episode: 814 reward: 180.0\n",
      "episode: 815 reward: 296.0\n",
      "episode: 816 reward: 305.0\n",
      "episode: 817 reward: 351.0\n",
      "episode: 818 reward: 308.0\n",
      "episode: 819 reward: 316.0\n",
      "episode: 820 reward: 387.0\n",
      "episode: 821 reward: 389.0\n",
      "episode: 822 reward: 340.0\n",
      "episode: 823 reward: 500.0\n",
      "episode: 824 reward: 500.0\n",
      "episode: 825 reward: 22.0\n",
      "episode: 826 reward: 500.0\n",
      "episode: 827 reward: 500.0\n",
      "episode: 828 reward: 500.0\n",
      "episode: 829 reward: 500.0\n",
      "episode: 830 reward: 500.0\n",
      "episode: 831 reward: 500.0\n",
      "episode: 832 reward: 500.0\n",
      "episode: 833 reward: 500.0\n",
      "episode: 834 reward: 500.0\n",
      "episode: 835 reward: 500.0\n",
      "episode: 836 reward: 500.0\n",
      "episode: 837 reward: 500.0\n",
      "episode: 838 reward: 500.0\n",
      "episode: 839 reward: 500.0\n",
      "episode: 840 reward: 500.0\n",
      "episode: 841 reward: 500.0\n",
      "episode: 842 reward: 500.0\n",
      "episode: 843 reward: 32.0\n",
      "episode: 844 reward: 500.0\n",
      "episode: 845 reward: 500.0\n",
      "episode: 846 reward: 500.0\n",
      "episode: 847 reward: 500.0\n",
      "episode: 848 reward: 500.0\n",
      "episode: 849 reward: 500.0\n",
      "episode: 850 reward: 500.0\n",
      "episode: 851 reward: 500.0\n",
      "episode: 852 reward: 500.0\n",
      "episode: 853 reward: 500.0\n",
      "episode: 854 reward: 110.0\n",
      "episode: 855 reward: 130.0\n",
      "episode: 856 reward: 500.0\n",
      "episode: 857 reward: 111.0\n",
      "episode: 858 reward: 335.0\n",
      "episode: 859 reward: 336.0\n",
      "episode: 860 reward: 270.0\n",
      "episode: 861 reward: 138.0\n",
      "episode: 862 reward: 377.0\n",
      "episode: 863 reward: 400.0\n",
      "episode: 864 reward: 118.0\n",
      "episode: 865 reward: 458.0\n",
      "episode: 866 reward: 281.0\n",
      "episode: 867 reward: 500.0\n",
      "episode: 868 reward: 500.0\n",
      "episode: 869 reward: 500.0\n",
      "episode: 870 reward: 53.0\n",
      "episode: 871 reward: 500.0\n",
      "episode: 872 reward: 500.0\n",
      "episode: 873 reward: 500.0\n",
      "episode: 874 reward: 142.0\n",
      "episode: 875 reward: 500.0\n",
      "episode: 876 reward: 500.0\n",
      "episode: 877 reward: 500.0\n",
      "episode: 878 reward: 500.0\n",
      "episode: 879 reward: 500.0\n",
      "episode: 880 reward: 500.0\n",
      "episode: 881 reward: 500.0\n",
      "episode: 882 reward: 500.0\n",
      "episode: 883 reward: 497.0\n",
      "episode: 884 reward: 500.0\n",
      "episode: 885 reward: 355.0\n",
      "episode: 886 reward: 372.0\n",
      "episode: 887 reward: 500.0\n",
      "episode: 888 reward: 234.0\n",
      "episode: 889 reward: 500.0\n",
      "episode: 890 reward: 500.0\n",
      "episode: 891 reward: 362.0\n",
      "episode: 892 reward: 297.0\n",
      "episode: 893 reward: 249.0\n",
      "episode: 894 reward: 266.0\n",
      "episode: 895 reward: 261.0\n",
      "episode: 896 reward: 500.0\n",
      "episode: 897 reward: 500.0\n",
      "episode: 898 reward: 500.0\n",
      "episode: 899 reward: 145.0\n",
      "episode: 900 reward: 500.0\n",
      "episode: 901 reward: 500.0\n",
      "episode: 902 reward: 500.0\n",
      "episode: 903 reward: 290.0\n",
      "episode: 904 reward: 500.0\n",
      "episode: 905 reward: 500.0\n",
      "episode: 906 reward: 307.0\n",
      "episode: 907 reward: 500.0\n",
      "episode: 908 reward: 500.0\n",
      "episode: 909 reward: 500.0\n",
      "episode: 910 reward: 500.0\n",
      "episode: 911 reward: 500.0\n",
      "episode: 912 reward: 500.0\n",
      "episode: 913 reward: 500.0\n",
      "episode: 914 reward: 500.0\n",
      "episode: 915 reward: 500.0\n",
      "episode: 916 reward: 500.0\n",
      "episode: 917 reward: 500.0\n",
      "episode: 918 reward: 500.0\n",
      "episode: 919 reward: 500.0\n",
      "episode: 920 reward: 500.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 921 reward: 500.0\n",
      "episode: 922 reward: 102.0\n",
      "episode: 923 reward: 500.0\n",
      "episode: 924 reward: 500.0\n",
      "episode: 925 reward: 500.0\n",
      "episode: 926 reward: 500.0\n",
      "episode: 927 reward: 500.0\n",
      "episode: 928 reward: 500.0\n",
      "episode: 929 reward: 111.0\n",
      "episode: 930 reward: 500.0\n",
      "episode: 931 reward: 500.0\n",
      "episode: 932 reward: 500.0\n",
      "episode: 933 reward: 500.0\n",
      "episode: 934 reward: 104.0\n",
      "episode: 935 reward: 253.0\n",
      "episode: 936 reward: 304.0\n",
      "episode: 937 reward: 224.0\n",
      "episode: 938 reward: 301.0\n",
      "episode: 939 reward: 127.0\n",
      "episode: 940 reward: 29.0\n",
      "episode: 941 reward: 17.0\n",
      "episode: 942 reward: 246.0\n",
      "episode: 943 reward: 126.0\n",
      "episode: 944 reward: 122.0\n",
      "episode: 945 reward: 305.0\n",
      "episode: 946 reward: 139.0\n",
      "episode: 947 reward: 236.0\n",
      "episode: 948 reward: 100.0\n",
      "episode: 949 reward: 211.0\n",
      "episode: 950 reward: 436.0\n",
      "episode: 951 reward: 272.0\n",
      "episode: 952 reward: 430.0\n",
      "episode: 953 reward: 287.0\n",
      "episode: 954 reward: 186.0\n",
      "episode: 955 reward: 443.0\n",
      "episode: 956 reward: 213.0\n",
      "episode: 957 reward: 453.0\n",
      "episode: 958 reward: 500.0\n",
      "episode: 959 reward: 500.0\n",
      "episode: 960 reward: 127.0\n",
      "episode: 961 reward: 500.0\n",
      "episode: 962 reward: 500.0\n",
      "episode: 963 reward: 500.0\n",
      "episode: 964 reward: 500.0\n",
      "episode: 965 reward: 500.0\n",
      "episode: 966 reward: 500.0\n",
      "episode: 967 reward: 500.0\n",
      "episode: 968 reward: 500.0\n",
      "episode: 969 reward: 500.0\n",
      "episode: 970 reward: 500.0\n",
      "episode: 971 reward: 500.0\n",
      "episode: 972 reward: 500.0\n",
      "episode: 973 reward: 500.0\n",
      "episode: 974 reward: 500.0\n",
      "episode: 975 reward: 500.0\n",
      "episode: 976 reward: 500.0\n",
      "episode: 977 reward: 500.0\n",
      "episode: 978 reward: 392.0\n",
      "episode: 979 reward: 500.0\n",
      "episode: 980 reward: 379.0\n",
      "episode: 981 reward: 451.0\n",
      "episode: 982 reward: 500.0\n",
      "episode: 983 reward: 358.0\n",
      "episode: 984 reward: 377.0\n",
      "episode: 985 reward: 430.0\n",
      "episode: 986 reward: 406.0\n",
      "episode: 987 reward: 500.0\n",
      "episode: 988 reward: 500.0\n",
      "episode: 989 reward: 433.0\n",
      "episode: 990 reward: 500.0\n",
      "episode: 991 reward: 500.0\n",
      "episode: 992 reward: 500.0\n",
      "episode: 993 reward: 500.0\n",
      "episode: 994 reward: 461.0\n",
      "episode: 995 reward: 500.0\n",
      "episode: 996 reward: 500.0\n",
      "episode: 997 reward: 500.0\n",
      "episode: 998 reward: 500.0\n",
      "episode: 999 reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class REINFORCE(ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize()\n",
    "    \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        for i in range(1000):\n",
    "            action, log_prob = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            trajectory.append((state, reward, done, log_prob))\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: break\n",
    "                \n",
    "        loss = []\n",
    "        for t in range(len(trajectory)):\n",
    "            G = sum([self.gamma**k * reward for k, (state, reward, done, _) in enumerate(trajectory[t:])])\n",
    "            _, _, _, log_prob = trajectory[t]\n",
    "            # from Sutton's book said multiply by gamma**t \n",
    "            # but from David course no need too \n",
    "            # I tried both work\n",
    "            loss.append(self.gamma**t * G * (-log_prob))\n",
    "        self.update_policy(torch.stack(loss).sum())\n",
    "        return total_reward\n",
    "    \n",
    "s = REINFORCE(env, \n",
    "               num_episodes=1000,\n",
    "               policy=\"softmax_policy\",\n",
    "               alpha=0.007, \n",
    "               gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 10.0\n",
      "episode: 1 reward: 21.0\n",
      "episode: 2 reward: 15.0\n",
      "episode: 3 reward: 11.0\n",
      "episode: 4 reward: 14.0\n",
      "episode: 5 reward: 11.0\n",
      "episode: 6 reward: 58.0\n",
      "episode: 7 reward: 15.0\n",
      "episode: 8 reward: 15.0\n",
      "episode: 9 reward: 11.0\n",
      "episode: 10 reward: 9.0\n",
      "episode: 11 reward: 21.0\n",
      "episode: 12 reward: 9.0\n",
      "episode: 13 reward: 13.0\n",
      "episode: 14 reward: 13.0\n",
      "episode: 15 reward: 12.0\n",
      "episode: 16 reward: 19.0\n",
      "episode: 17 reward: 27.0\n",
      "episode: 18 reward: 62.0\n",
      "episode: 19 reward: 54.0\n",
      "episode: 20 reward: 29.0\n",
      "episode: 21 reward: 22.0\n",
      "episode: 22 reward: 18.0\n",
      "episode: 23 reward: 15.0\n",
      "episode: 24 reward: 18.0\n",
      "episode: 25 reward: 18.0\n",
      "episode: 26 reward: 23.0\n",
      "episode: 27 reward: 22.0\n",
      "episode: 28 reward: 35.0\n",
      "episode: 29 reward: 13.0\n",
      "episode: 30 reward: 12.0\n",
      "episode: 31 reward: 16.0\n",
      "episode: 32 reward: 22.0\n",
      "episode: 33 reward: 23.0\n",
      "episode: 34 reward: 51.0\n",
      "episode: 35 reward: 14.0\n",
      "episode: 36 reward: 49.0\n",
      "episode: 37 reward: 47.0\n",
      "episode: 38 reward: 67.0\n",
      "episode: 39 reward: 25.0\n",
      "episode: 40 reward: 61.0\n",
      "episode: 41 reward: 57.0\n",
      "episode: 42 reward: 21.0\n",
      "episode: 43 reward: 25.0\n",
      "episode: 44 reward: 96.0\n",
      "episode: 45 reward: 77.0\n",
      "episode: 46 reward: 64.0\n",
      "episode: 47 reward: 23.0\n",
      "episode: 48 reward: 28.0\n",
      "episode: 49 reward: 90.0\n",
      "episode: 50 reward: 28.0\n",
      "episode: 51 reward: 61.0\n",
      "episode: 52 reward: 45.0\n",
      "episode: 53 reward: 114.0\n",
      "episode: 54 reward: 24.0\n",
      "episode: 55 reward: 59.0\n",
      "episode: 56 reward: 58.0\n",
      "episode: 57 reward: 38.0\n",
      "episode: 58 reward: 120.0\n",
      "episode: 59 reward: 43.0\n",
      "episode: 60 reward: 46.0\n",
      "episode: 61 reward: 38.0\n",
      "episode: 62 reward: 43.0\n",
      "episode: 63 reward: 33.0\n",
      "episode: 64 reward: 29.0\n",
      "episode: 65 reward: 20.0\n",
      "episode: 66 reward: 34.0\n",
      "episode: 67 reward: 29.0\n",
      "episode: 68 reward: 34.0\n",
      "episode: 69 reward: 29.0\n",
      "episode: 70 reward: 33.0\n",
      "episode: 71 reward: 31.0\n",
      "episode: 72 reward: 45.0\n",
      "episode: 73 reward: 44.0\n",
      "episode: 74 reward: 36.0\n",
      "episode: 75 reward: 31.0\n",
      "episode: 76 reward: 57.0\n",
      "episode: 77 reward: 51.0\n",
      "episode: 78 reward: 33.0\n",
      "episode: 79 reward: 43.0\n",
      "episode: 80 reward: 61.0\n",
      "episode: 81 reward: 59.0\n",
      "episode: 82 reward: 114.0\n",
      "episode: 83 reward: 77.0\n",
      "episode: 84 reward: 137.0\n",
      "episode: 85 reward: 74.0\n",
      "episode: 86 reward: 54.0\n",
      "episode: 87 reward: 61.0\n",
      "episode: 88 reward: 43.0\n",
      "episode: 89 reward: 35.0\n",
      "episode: 90 reward: 41.0\n",
      "episode: 91 reward: 23.0\n",
      "episode: 92 reward: 42.0\n",
      "episode: 93 reward: 31.0\n",
      "episode: 94 reward: 29.0\n",
      "episode: 95 reward: 22.0\n",
      "episode: 96 reward: 30.0\n",
      "episode: 97 reward: 24.0\n",
      "episode: 98 reward: 29.0\n",
      "episode: 99 reward: 32.0\n",
      "episode: 100 reward: 22.0\n",
      "episode: 101 reward: 22.0\n",
      "episode: 102 reward: 20.0\n",
      "episode: 103 reward: 21.0\n",
      "episode: 104 reward: 21.0\n",
      "episode: 105 reward: 19.0\n",
      "episode: 106 reward: 21.0\n",
      "episode: 107 reward: 19.0\n",
      "episode: 108 reward: 23.0\n",
      "episode: 109 reward: 15.0\n",
      "episode: 110 reward: 22.0\n",
      "episode: 111 reward: 26.0\n",
      "episode: 112 reward: 20.0\n",
      "episode: 113 reward: 20.0\n",
      "episode: 114 reward: 22.0\n",
      "episode: 115 reward: 14.0\n",
      "episode: 116 reward: 15.0\n",
      "episode: 117 reward: 16.0\n",
      "episode: 118 reward: 21.0\n",
      "episode: 119 reward: 14.0\n",
      "episode: 120 reward: 19.0\n",
      "episode: 121 reward: 21.0\n",
      "episode: 122 reward: 20.0\n",
      "episode: 123 reward: 16.0\n",
      "episode: 124 reward: 17.0\n",
      "episode: 125 reward: 19.0\n",
      "episode: 126 reward: 20.0\n",
      "episode: 127 reward: 21.0\n",
      "episode: 128 reward: 16.0\n",
      "episode: 129 reward: 22.0\n",
      "episode: 130 reward: 14.0\n",
      "episode: 131 reward: 18.0\n",
      "episode: 132 reward: 20.0\n",
      "episode: 133 reward: 13.0\n",
      "episode: 134 reward: 13.0\n",
      "episode: 135 reward: 18.0\n",
      "episode: 136 reward: 16.0\n",
      "episode: 137 reward: 15.0\n",
      "episode: 138 reward: 21.0\n",
      "episode: 139 reward: 17.0\n",
      "episode: 140 reward: 19.0\n",
      "episode: 141 reward: 20.0\n",
      "episode: 142 reward: 20.0\n",
      "episode: 143 reward: 20.0\n",
      "episode: 144 reward: 22.0\n",
      "episode: 145 reward: 24.0\n",
      "episode: 146 reward: 24.0\n",
      "episode: 147 reward: 31.0\n",
      "episode: 148 reward: 36.0\n",
      "episode: 149 reward: 34.0\n",
      "episode: 150 reward: 37.0\n",
      "episode: 151 reward: 34.0\n",
      "episode: 152 reward: 54.0\n",
      "episode: 153 reward: 59.0\n",
      "episode: 154 reward: 94.0\n",
      "episode: 155 reward: 115.0\n",
      "episode: 156 reward: 113.0\n",
      "episode: 157 reward: 129.0\n",
      "episode: 158 reward: 150.0\n",
      "episode: 159 reward: 171.0\n",
      "episode: 160 reward: 196.0\n",
      "episode: 161 reward: 259.0\n",
      "episode: 162 reward: 500.0\n",
      "episode: 163 reward: 500.0\n",
      "episode: 164 reward: 500.0\n",
      "episode: 165 reward: 500.0\n",
      "episode: 166 reward: 500.0\n",
      "episode: 167 reward: 500.0\n",
      "episode: 168 reward: 500.0\n",
      "episode: 169 reward: 500.0\n",
      "episode: 170 reward: 500.0\n",
      "episode: 171 reward: 500.0\n",
      "episode: 172 reward: 500.0\n",
      "episode: 173 reward: 500.0\n",
      "episode: 174 reward: 500.0\n",
      "episode: 175 reward: 500.0\n",
      "episode: 176 reward: 500.0\n",
      "episode: 177 reward: 500.0\n",
      "episode: 178 reward: 500.0\n",
      "episode: 179 reward: 500.0\n",
      "episode: 180 reward: 287.0\n",
      "episode: 181 reward: 143.0\n",
      "episode: 182 reward: 126.0\n",
      "episode: 183 reward: 112.0\n",
      "episode: 184 reward: 103.0\n",
      "episode: 185 reward: 103.0\n",
      "episode: 186 reward: 112.0\n",
      "episode: 187 reward: 125.0\n",
      "episode: 188 reward: 108.0\n",
      "episode: 189 reward: 132.0\n",
      "episode: 190 reward: 174.0\n",
      "episode: 191 reward: 186.0\n",
      "episode: 192 reward: 208.0\n",
      "episode: 193 reward: 191.0\n",
      "episode: 194 reward: 328.0\n",
      "episode: 195 reward: 500.0\n",
      "episode: 196 reward: 500.0\n",
      "episode: 197 reward: 500.0\n",
      "episode: 198 reward: 500.0\n",
      "episode: 199 reward: 500.0\n",
      "episode: 200 reward: 500.0\n",
      "episode: 201 reward: 500.0\n",
      "episode: 202 reward: 500.0\n",
      "episode: 203 reward: 500.0\n",
      "episode: 204 reward: 500.0\n",
      "episode: 205 reward: 500.0\n",
      "episode: 206 reward: 500.0\n",
      "episode: 207 reward: 500.0\n",
      "episode: 208 reward: 500.0\n",
      "episode: 209 reward: 500.0\n",
      "episode: 210 reward: 464.0\n",
      "episode: 211 reward: 500.0\n",
      "episode: 212 reward: 500.0\n",
      "episode: 213 reward: 500.0\n",
      "episode: 214 reward: 500.0\n",
      "episode: 215 reward: 500.0\n",
      "episode: 216 reward: 280.0\n",
      "episode: 217 reward: 500.0\n",
      "episode: 218 reward: 500.0\n",
      "episode: 219 reward: 274.0\n",
      "episode: 220 reward: 500.0\n",
      "episode: 221 reward: 500.0\n",
      "episode: 222 reward: 500.0\n",
      "episode: 223 reward: 500.0\n",
      "episode: 224 reward: 500.0\n",
      "episode: 225 reward: 500.0\n",
      "episode: 226 reward: 500.0\n",
      "episode: 227 reward: 500.0\n",
      "episode: 228 reward: 500.0\n",
      "episode: 229 reward: 500.0\n",
      "episode: 230 reward: 385.0\n",
      "episode: 231 reward: 290.0\n",
      "episode: 232 reward: 237.0\n",
      "episode: 233 reward: 165.0\n",
      "episode: 234 reward: 107.0\n",
      "episode: 235 reward: 10.0\n",
      "episode: 236 reward: 158.0\n",
      "episode: 237 reward: 347.0\n",
      "episode: 238 reward: 500.0\n",
      "episode: 239 reward: 500.0\n",
      "episode: 240 reward: 500.0\n",
      "episode: 241 reward: 500.0\n",
      "episode: 242 reward: 500.0\n",
      "episode: 243 reward: 500.0\n",
      "episode: 244 reward: 500.0\n",
      "episode: 245 reward: 500.0\n",
      "episode: 246 reward: 500.0\n",
      "episode: 247 reward: 500.0\n",
      "episode: 248 reward: 500.0\n",
      "episode: 249 reward: 500.0\n",
      "episode: 250 reward: 500.0\n",
      "episode: 251 reward: 500.0\n",
      "episode: 252 reward: 500.0\n",
      "episode: 253 reward: 500.0\n",
      "episode: 254 reward: 500.0\n",
      "episode: 255 reward: 500.0\n",
      "episode: 256 reward: 500.0\n",
      "episode: 257 reward: 500.0\n",
      "episode: 258 reward: 500.0\n",
      "episode: 259 reward: 500.0\n",
      "episode: 260 reward: 500.0\n",
      "episode: 261 reward: 500.0\n",
      "episode: 262 reward: 500.0\n",
      "episode: 263 reward: 500.0\n",
      "episode: 264 reward: 500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-70834d9f232b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m                \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                gamma=.99)\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-70834d9f232b>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtd_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mloss_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtd_error\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/approx_v_base.py\u001b[0m in \u001b[0;36mget_q\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapprox_v\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "class REINFORCE_with_baseline(ApproxVBase, ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hidden = 64\n",
    "        features = torch.nn.Linear(self.num_state, hidden)\n",
    "        model_q = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        model_policy = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        ApproxVBase.initialize(self, learning_rate_name=\"beta\", model=model_q)\n",
    "        ApproxPolicyBase.initialize(self, learning_rate_name=\"alpha\", model=model_policy)\n",
    "    \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        for i in range(1000):\n",
    "            action, log_prob = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            trajectory.append((state, action, reward, done, log_prob))\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: break\n",
    "                \n",
    "        td_targets = []\n",
    "        qs = []\n",
    "        loss_policy = []\n",
    "        for t in range(len(trajectory)):\n",
    "            G = sum([self.gamma**k * reward for k, (state, action, reward, done, log_prob) in enumerate(trajectory[t:])])\n",
    "            state, action, reward, done, log_prob = trajectory[t]\n",
    "            td_error = G - self.get_q(state)[action]\n",
    "            td_targets.append(G)\n",
    "            qs.append(self.get_q(state)[action])\n",
    "            loss_policy.append(self.gamma**t * td_error * (-log_prob))\n",
    "        self.update_q(torch.Tensor(td_targets), torch.stack(qs))\n",
    "        self.update_policy(torch.stack(loss_policy).sum())\n",
    "        return total_reward\n",
    "    \n",
    "s = REINFORCE_with_baseline(env, \n",
    "               num_episodes=1000,\n",
    "               policy=\"softmax_policy\",\n",
    "               alpha=0.007, \n",
    "               beta=0.001,\n",
    "               gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's really bad for table base problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -6.0\n",
      "episode: 1 reward: -90.0\n",
      "episode: 2 reward: -28.0\n",
      "episode: 3 reward: -9.0\n",
      "episode: 4 reward: -12.0\n",
      "episode: 5 reward: 0.0\n",
      "episode: 6 reward: -23.0\n",
      "episode: 7 reward: -9.0\n",
      "episode: 8 reward: -29.0\n",
      "episode: 9 reward: -10.0\n",
      "episode: 10 reward: -80.0\n",
      "episode: 11 reward: -26.0\n",
      "episode: 12 reward: -23.0\n",
      "episode: 13 reward: -8.0\n",
      "episode: 14 reward: 0.0\n",
      "episode: 15 reward: -3.0\n",
      "episode: 16 reward: -26.0\n",
      "episode: 17 reward: -6.0\n",
      "episode: 18 reward: -5.0\n",
      "episode: 19 reward: 0.0\n",
      "episode: 20 reward: -6.0\n",
      "episode: 21 reward: 0.0\n",
      "episode: 22 reward: -12.0\n",
      "episode: 23 reward: -18.0\n",
      "episode: 24 reward: -22.0\n",
      "episode: 25 reward: 0.0\n",
      "episode: 26 reward: -17.0\n",
      "episode: 27 reward: -12.0\n",
      "episode: 28 reward: 0.0\n",
      "episode: 29 reward: -13.0\n",
      "episode: 30 reward: -6.0\n",
      "episode: 31 reward: 0.0\n",
      "episode: 32 reward: 0.0\n",
      "episode: 33 reward: -4.0\n",
      "episode: 34 reward: -5.0\n",
      "episode: 35 reward: -4.0\n",
      "episode: 36 reward: -2.0\n",
      "episode: 37 reward: -2.0\n",
      "episode: 38 reward: -7.0\n",
      "episode: 39 reward: 0.0\n",
      "episode: 40 reward: -5.0\n",
      "episode: 41 reward: -1.0\n",
      "episode: 42 reward: 0.0\n",
      "episode: 43 reward: -5.0\n",
      "episode: 44 reward: -6.0\n",
      "episode: 45 reward: -9.0\n",
      "episode: 46 reward: -8.0\n",
      "episode: 47 reward: -2.0\n",
      "episode: 48 reward: -18.0\n",
      "episode: 49 reward: -6.0\n",
      "episode: 50 reward: -5.0\n",
      "episode: 51 reward: -17.0\n",
      "episode: 52 reward: -33.0\n",
      "episode: 53 reward: -12.0\n",
      "episode: 54 reward: -12.0\n",
      "episode: 55 reward: -6.0\n",
      "episode: 56 reward: -19.0\n",
      "episode: 57 reward: -1.0\n",
      "episode: 58 reward: -12.0\n",
      "episode: 59 reward: -30.0\n",
      "episode: 60 reward: 0.0\n",
      "episode: 61 reward: -1.0\n",
      "episode: 62 reward: -3.0\n",
      "episode: 63 reward: -3.0\n",
      "episode: 64 reward: -14.0\n",
      "episode: 65 reward: -16.0\n",
      "episode: 66 reward: -3.0\n",
      "episode: 67 reward: -3.0\n",
      "episode: 68 reward: -13.0\n",
      "episode: 69 reward: -3.0\n",
      "episode: 70 reward: 0.0\n",
      "episode: 71 reward: -2.0\n",
      "episode: 72 reward: -9.0\n",
      "episode: 73 reward: -2.0\n",
      "episode: 74 reward: 0.0\n",
      "episode: 75 reward: -2.0\n",
      "episode: 76 reward: -10.0\n",
      "episode: 77 reward: 0.0\n",
      "episode: 78 reward: -8.0\n",
      "episode: 79 reward: 0.0\n",
      "episode: 80 reward: -8.0\n",
      "episode: 81 reward: -10.0\n",
      "episode: 82 reward: -9.0\n",
      "episode: 83 reward: -12.0\n",
      "episode: 84 reward: -1.0\n",
      "episode: 85 reward: -5.0\n",
      "episode: 86 reward: -7.0\n",
      "episode: 87 reward: -18.0\n",
      "episode: 88 reward: -9.0\n",
      "episode: 89 reward: -3.0\n",
      "episode: 90 reward: -5.0\n",
      "episode: 91 reward: -4.0\n",
      "episode: 92 reward: -2.0\n",
      "episode: 93 reward: -5.0\n",
      "episode: 94 reward: 0.0\n",
      "episode: 95 reward: -2.0\n",
      "episode: 96 reward: -4.0\n",
      "episode: 97 reward: -15.0\n",
      "episode: 98 reward: -2.0\n",
      "episode: 99 reward: -12.0\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv2DState()\n",
    "s = REINFORCE(env, \n",
    "               num_episodes=100,\n",
    "               policy=\"softmax_policy\",\n",
    "               epsilon=0.01, \n",
    "               alpha=0.008, \n",
    "               gamma=.9)\n",
    "s.train(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class GaussianPolicyModel(nn.Module):\n",
    "    \"\"\"TODO add to device()\"\"\"\n",
    "    def __init__(self, num_input=2, num_output=1):\n",
    "        super().__init__()\n",
    "        hidden=32\n",
    "        # 0. feature\n",
    "        self.model_features = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "        )\n",
    "        # 1. mu\n",
    "        self.model_mu = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, num_output),\n",
    "        )\n",
    "        \n",
    "        # 2. sigma\n",
    "        self.model_sigma = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, num_output),\n",
    "            torch.nn.Softplus() # make it positive, be careful threshold=20???\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model_features(x)\n",
    "        mu = self.model_mu(features)\n",
    "        sigma = self.model_sigma(features)\n",
    "        return mu, sigma\n",
    "policy = GaussianPolicyModel(2)\n",
    "# optimizer_sigma = torch.optim.Adam(policy.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: -100.35842308465391\n",
      "episode: 1 reward: -113.24992277126263\n",
      "episode: 2 reward: -65.70190685751913\n",
      "episode: 3 reward: -103.13668024333319\n",
      "episode: 4 reward: -100.316343425973\n",
      "episode: 5 reward: -99.58418970435174\n",
      "episode: 6 reward: -101.53118235236033\n",
      "episode: 7 reward: -101.27089708498183\n",
      "episode: 8 reward: -117.21142357075524\n",
      "episode: 9 reward: -101.78536410937024\n",
      "episode: 10 reward: -100.59975659703588\n",
      "episode: 11 reward: -63.612743602661524\n",
      "episode: 12 reward: -61.578593641045174\n",
      "episode: 13 reward: -119.64995834097515\n",
      "episode: 14 reward: -100.90255236416868\n",
      "episode: 15 reward: -100.22263137600694\n",
      "episode: 16 reward: -116.77519086597178\n",
      "episode: 17 reward: -60.878205199879304\n",
      "episode: 18 reward: -123.28582637545219\n",
      "episode: 19 reward: -124.1252290374972\n",
      "episode: 20 reward: -101.52589524583891\n",
      "episode: 21 reward: -102.20943833082293\n",
      "episode: 22 reward: -105.87032954433872\n",
      "episode: 23 reward: -103.97933282472938\n",
      "episode: 24 reward: -113.09609705141652\n",
      "episode: 25 reward: -104.28486967464474\n",
      "episode: 26 reward: -105.41883521668365\n",
      "episode: 27 reward: -97.45064145737017\n",
      "episode: 28 reward: -112.4676786621629\n",
      "episode: 29 reward: -113.39715875427177\n",
      "episode: 30 reward: -120.69377505958578\n",
      "episode: 31 reward: -64.07665785133037\n",
      "episode: 32 reward: -61.57335749584632\n",
      "episode: 33 reward: -116.05228168660713\n",
      "episode: 34 reward: -55.96626023999482\n",
      "episode: 35 reward: -67.89371047206643\n",
      "episode: 36 reward: -59.12832085972491\n",
      "episode: 37 reward: -105.26098895454841\n",
      "episode: 38 reward: -102.33466815679023\n",
      "episode: 39 reward: -113.48977749974405\n",
      "episode: 40 reward: -118.90445513170586\n",
      "episode: 41 reward: -103.30446474802122\n",
      "episode: 42 reward: -120.2817410779701\n",
      "episode: 43 reward: -59.71873157918448\n",
      "episode: 44 reward: -99.7128051759048\n",
      "episode: 45 reward: -124.10218877944598\n",
      "episode: 46 reward: -104.20878128867224\n",
      "episode: 47 reward: -99.49217497997296\n",
      "episode: 48 reward: -118.87023416904546\n",
      "episode: 49 reward: -72.86542001730767\n",
      "episode: 50 reward: -119.56721451724071\n",
      "episode: 51 reward: -102.83651831747095\n",
      "episode: 52 reward: -67.74070421873714\n",
      "episode: 53 reward: -61.607229723295816\n",
      "episode: 54 reward: -99.74026700397768\n",
      "episode: 55 reward: -101.11969740576296\n",
      "episode: 56 reward: -100.50141988662133\n",
      "episode: 57 reward: -75.35656910995186\n",
      "episode: 58 reward: -103.51420521088255\n",
      "episode: 59 reward: -73.60905053572418\n",
      "episode: 60 reward: -102.7477241433269\n",
      "episode: 61 reward: -102.12808875199158\n",
      "episode: 62 reward: -102.42944855509387\n",
      "episode: 63 reward: -70.57060142977588\n",
      "episode: 64 reward: -105.69490192250038\n",
      "episode: 65 reward: -73.39623211140994\n",
      "episode: 66 reward: -101.25733329867944\n",
      "episode: 67 reward: -70.91669493298356\n",
      "episode: 68 reward: -104.50231060765249\n",
      "episode: 69 reward: -69.52635673659861\n",
      "episode: 70 reward: -68.28542245847234\n",
      "episode: 71 reward: -73.74683760737179\n",
      "episode: 72 reward: -80.21163633826319\n",
      "episode: 73 reward: -81.59322104398403\n",
      "episode: 74 reward: -77.32346073867313\n",
      "episode: 75 reward: -112.4481826463559\n",
      "episode: 76 reward: -80.20991200207793\n",
      "episode: 77 reward: -109.36411109998015\n",
      "episode: 78 reward: -85.28580385036639\n",
      "episode: 79 reward: -77.3163444809845\n",
      "episode: 80 reward: -123.14622450395426\n",
      "episode: 81 reward: -103.239788676152\n",
      "episode: 82 reward: -109.91031482983195\n",
      "episode: 83 reward: -87.40975667495655\n",
      "episode: 84 reward: -83.58772089588886\n",
      "episode: 85 reward: -118.66199860449011\n",
      "episode: 86 reward: -115.34941248853815\n",
      "episode: 87 reward: -118.86821092038167\n",
      "episode: 88 reward: -94.05628861777734\n",
      "episode: 89 reward: -90.48582768723243\n",
      "episode: 90 reward: -117.14298930572409\n",
      "episode: 91 reward: -95.77746202443525\n",
      "episode: 92 reward: -114.89818955361844\n",
      "episode: 93 reward: -94.71784834000275\n",
      "episode: 94 reward: -111.82721999038942\n",
      "episode: 95 reward: -94.19249793330013\n",
      "episode: 96 reward: -101.18927498621667\n",
      "episode: 97 reward: -118.77567857478809\n",
      "episode: 98 reward: -102.29718024893333\n",
      "episode: 99 reward: -121.209726479937\n",
      "episode: 100 reward: -110.54863019786133\n",
      "episode: 101 reward: -100.27352325461288\n",
      "episode: 102 reward: -119.48314474309981\n",
      "episode: 103 reward: -110.42368554172522\n",
      "episode: 104 reward: -116.7942285504279\n",
      "episode: 105 reward: -129.29390412646956\n",
      "episode: 106 reward: -118.58962745228595\n",
      "episode: 107 reward: -113.5960227389938\n",
      "episode: 108 reward: -118.23489615383744\n",
      "episode: 109 reward: -123.5337909666691\n",
      "episode: 110 reward: -115.12364921031907\n",
      "episode: 111 reward: -122.01842089826924\n",
      "episode: 112 reward: -112.02577760576767\n",
      "episode: 113 reward: -124.8695211413037\n",
      "episode: 114 reward: -128.01084424258954\n",
      "episode: 115 reward: -128.01091861020217\n",
      "episode: 116 reward: -127.46347491508226\n",
      "episode: 117 reward: -128.08137622830336\n",
      "episode: 118 reward: -127.38841424131891\n",
      "episode: 119 reward: -127.90680604059128\n",
      "episode: 120 reward: -115.25698660640543\n",
      "episode: 121 reward: -128.00103118162664\n",
      "episode: 122 reward: -126.07687290151355\n",
      "episode: 123 reward: -124.86711443457692\n",
      "episode: 124 reward: -128.12786857552268\n",
      "episode: 125 reward: -127.5015856719433\n",
      "episode: 126 reward: -128.04994224883183\n",
      "episode: 127 reward: -128.0716584731918\n",
      "episode: 128 reward: -127.66596362250235\n",
      "episode: 129 reward: -127.45011051775764\n",
      "episode: 130 reward: -127.51256214067217\n",
      "episode: 131 reward: -128.0329511248289\n",
      "episode: 132 reward: -127.4621806524278\n",
      "episode: 133 reward: -128.06093736897222\n",
      "episode: 134 reward: -128.0371456155957\n",
      "episode: 135 reward: -127.47908750821091\n",
      "episode: 136 reward: -127.48239914305074\n",
      "episode: 137 reward: -128.02605548016106\n",
      "episode: 138 reward: -128.0334937710253\n",
      "episode: 139 reward: -128.03724169246232\n",
      "episode: 140 reward: -127.46031068522049\n",
      "episode: 141 reward: -127.48746790062202\n",
      "episode: 142 reward: -127.40591036458997\n",
      "episode: 143 reward: -127.51492937390879\n",
      "episode: 144 reward: -127.43855015313127\n",
      "episode: 145 reward: -128.0440051294491\n",
      "episode: 146 reward: -127.43470461255748\n",
      "episode: 147 reward: -128.08940975271167\n",
      "episode: 148 reward: -127.46644387697367\n",
      "episode: 149 reward: -127.54240551397392\n",
      "episode: 150 reward: -127.54354525109628\n",
      "episode: 151 reward: -128.08187263082712\n",
      "episode: 152 reward: -127.49996566920419\n",
      "episode: 153 reward: -128.04115061163282\n",
      "episode: 154 reward: -128.07953892660018\n",
      "episode: 155 reward: -127.41781290683151\n",
      "episode: 156 reward: -127.44033675625798\n",
      "episode: 157 reward: -127.54267398229676\n",
      "episode: 158 reward: -127.4243941341266\n",
      "episode: 159 reward: -127.55282741187948\n",
      "episode: 160 reward: -127.46721041218564\n",
      "episode: 161 reward: -127.97655278036991\n",
      "episode: 162 reward: -127.975483905642\n",
      "episode: 163 reward: -127.9400519318506\n",
      "episode: 164 reward: -128.05189889394057\n",
      "episode: 165 reward: -127.93055658240493\n",
      "episode: 166 reward: -127.50700853982816\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error in potrf : the leading minor of order 1 is not positive definite at /Users/administrator/nightlies/2018_10_14/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly-cpu_1539513053549/work/aten/src/TH/generic/THTensorLapack.cpp:631",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bfb4c5613294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0007\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                gamma=.9)\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-bfb4c5613294>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#             print(\"\\rreward\", reward, end=\"\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_state_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/approx_policy_base.py\u001b[0m in \u001b[0;36mgaussian_policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapprox_policy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         m = torch.distributions.MultivariateNormal(\n\u001b[0;32m---> 48\u001b[0;31m             mu, torch.eye(self.num_action)*sigma)\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         action = torch.clamp(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprecision_matrix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_inverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unbroadcasted_scale_tril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_batch_potrf_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m_batch_potrf_lower\u001b[0;34m(bmat)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/multivariate_normal.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mcholesky\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpotrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lapack Error in potrf : the leading minor of order 1 is not positive definite at /Users/administrator/nightlies/2018_10_14/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch-nightly-cpu_1539513053549/work/aten/src/TH/generic/THTensorLapack.cpp:631"
     ]
    }
   ],
   "source": [
    "class REINFORCE_continuous(ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.initialize(model=GaussianPolicyModel(self.num_state, self.num_action))\n",
    "        \n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        trajectory = []\n",
    "        for i in range(1000):\n",
    "            action, log_prob = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "#             print(\"\\rreward\", reward, end=\"\")\n",
    "#             self.env.render()\n",
    "            trajectory.append((state, reward, done, log_prob))\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: break\n",
    "                \n",
    "        loss = []\n",
    "        for t in range(len(trajectory)):\n",
    "            G = sum([self.gamma**k * reward for k, (state, reward, done, _) in enumerate(trajectory[t:])])\n",
    "            _, _, _, log_prob = trajectory[t]\n",
    "            loss.append(self.gamma**t * G * (-log_prob))\n",
    "        self.update_policy(torch.stack(loss).sum())\n",
    "        return total_reward\n",
    "\n",
    "try: env.close()\n",
    "except: pass\n",
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "env = gym.make('BipedalWalker-v2')\n",
    "s = REINFORCE_continuous(env, \n",
    "               num_episodes=1000,\n",
    "               policy=\"gaussian_policy\",\n",
    "               alpha=0.0007, \n",
    "               gamma=.9)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

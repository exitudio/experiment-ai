{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4) <br>\n",
    "[ref](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import update_params, ExperienceReplay, WeightDecay\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.local_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.target_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=0 # use manual update\n",
    "        )\n",
    "        self.num_step = 0\n",
    "    \n",
    "    def initialize(self, num_step_to_learn, eps_start, eps_end, eps_decay, num_experience, num_recall):\n",
    "        self.num_step_to_learn= num_step_to_learn\n",
    "        self.epsilon_decay = WeightDecay(eps_start, eps_end, eps_decay)\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        self.experience_replay = ExperienceReplay(num_experience=num_experience, num_recall=num_recall)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        return self.local_q_network.epsilon_greedy(state, self.epsilon)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # detach because we only backprop local network and update target network weight manually\n",
    "        targets_next_Q = self.target_q_network.forward(next_state).detach().max(1)[0]\n",
    "        targets_Q = reward + (self.gamma * targets_next_Q * (1 - done))\n",
    "\n",
    "        local_Q = self.local_q_network.forward(state)\n",
    "        expected_Q = local_Q.gather(1, action.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "        loss = F.mse_loss(expected_Q, targets_Q)\n",
    "        self.local_q_network.backprop(loss)\n",
    "\n",
    "        update_params(self.local_q_network.model, self.target_q_network.model, self.tau)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            action = self.policy(state)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            self.num_step += 1\n",
    "            if self.num_step%self.num_step_to_learn == 0:\n",
    "                experiences = self.experience_replay.recall()\n",
    "                self.learn(*experiences)\n",
    "            state = _state\n",
    "            \n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        return total_reward\n",
    "    \n",
    "    def _save(self, reward):\n",
    "        torch.save(self.local_q_network.model.state_dict(), self.save_name)\n",
    "    def _load(self):\n",
    "        self.epsilon = 0\n",
    "        self.local_q_network.model.load_state_dict(torch.load(self.save_name))\n",
    "        self.local_q_network.model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -154.91\n",
      "Episode 200\tAverage Score: -89.525\n",
      "Episode 300\tAverage Score: -29.54\n",
      "Episode 400\tAverage Score: 40.573\n",
      "Episode 500\tAverage Score: 177.55\n",
      "Episode 522\tAverage Score: 200.56--- early stop ----\n",
      " current_mean_reward: 200.56308424718296 num_mean_episode: 100\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/LunarLander-v2-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2024, \n",
    "               num_recall=512)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 53\tAverage Score: 1.19"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('Breakout-ram-v0')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/Breakout-ram-v0-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2024, \n",
    "               num_recall=512)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

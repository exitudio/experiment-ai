{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4) <br>\n",
    "[ref](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import print_weight_size, copy_params, update_params, ExperienceReplay, convert_to_tensor\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: -175.6719309152444\n",
      "episode: 1 reward: -274.2036314363211\n",
      "episode: 2 reward: -722.2442052580515\n",
      "episode: 3 reward: -604.2080585438855\n",
      "episode: 4 reward: -609.0064950254724\n",
      "episode: 5 reward: -746.7980859504426\n",
      "episode: 6 reward: -368.5739171188871\n",
      "episode: 7 reward: -361.50637711332973\n",
      "episode: 8 reward: -300.7242017902653\n",
      "episode: 9 reward: -213.75063227264351\n",
      "episode: 10 reward: -392.0468522259577\n",
      "episode: 11 reward: -423.35488025564035\n",
      "episode: 12 reward: -384.03144561757983\n",
      "episode: 13 reward: -339.97046435141135\n",
      "episode: 14 reward: -257.6834502151773\n",
      "episode: 15 reward: -74.38314981819939\n",
      "episode: 16 reward: -178.16870332948685\n",
      "episode: 17 reward: -160.72072009302877\n",
      "episode: 18 reward: -194.29686943986513\n",
      "episode: 19 reward: -211.77497581571183\n",
      "episode: 20 reward: -182.33292643802736\n",
      "episode: 21 reward: -129.6409757528953\n",
      "episode: 22 reward: -209.4044546894741\n",
      "episode: 23 reward: -210.29530958118448\n",
      "episode: 24 reward: -292.0097264524917\n",
      "episode: 25 reward: -319.8130153817992\n",
      "episode: 26 reward: -321.2197677821723\n",
      "episode: 27 reward: -319.82334563645304\n",
      "episode: 28 reward: -344.0975476476812\n",
      "episode: 29 reward: -351.2280055557684\n",
      "episode: 30 reward: -313.6052656618342\n",
      "episode: 31 reward: -333.57598892846437\n",
      "episode: 32 reward: -377.65045720699135\n",
      "episode: 33 reward: -372.72710015928965\n",
      "episode: 34 reward: -389.8228116523765\n",
      "episode: 35 reward: -476.72428974489594\n",
      "episode: 36 reward: -587.6532375032104\n",
      "episode: 37 reward: -751.7166372381859\n",
      "episode: 38 reward: -556.8957173852058\n",
      "episode: 39 reward: -807.0619494227736\n",
      "episode: 40 reward: -689.594701087781\n",
      "episode: 41 reward: -586.5296086933529\n",
      "episode: 42 reward: -453.86142457780295\n",
      "episode: 43 reward: -559.9577667597545\n",
      "episode: 44 reward: -583.7658492929122\n",
      "episode: 45 reward: -819.5303924981148\n",
      "episode: 46 reward: -830.2649645331758\n",
      "episode: 47 reward: -864.0461243685291\n",
      "episode: 48 reward: -704.6799542235548\n",
      "episode: 49 reward: -630.2923461788304\n",
      "episode: 50 reward: -570.9592353770645\n",
      "episode: 51 reward: -806.0726642539902\n",
      "episode: 52 reward: -622.3052193015387\n",
      "episode: 53 reward: -490.3520489039068\n",
      "episode: 54 reward: -505.73507467150205\n",
      "episode: 55 reward: -849.8557194925104\n",
      "episode: 56 reward: -676.260267547913\n",
      "episode: 57 reward: -527.0192088984666\n",
      "episode: 58 reward: -384.3565787141354\n",
      "episode: 59 reward: -483.4556666504723\n",
      "episode: 60 reward: -584.8472116095279\n",
      "episode: 61 reward: -583.3242137096215\n",
      "episode: 62 reward: -945.4882120247642\n",
      "episode: 63 reward: -481.2296154661714\n",
      "episode: 64 reward: -697.3844440969398\n",
      "episode: 65 reward: -556.7084161725311\n",
      "episode: 66 reward: -522.8085219896082\n",
      "episode: 67 reward: -909.9337996091763\n",
      "episode: 68 reward: -542.9841598770585\n",
      "episode: 69 reward: -724.1881780823678\n",
      "episode: 70 reward: -542.9518917807757\n",
      "episode: 71 reward: -420.72562011407143\n",
      "episode: 72 reward: -792.4257281450864\n",
      "episode: 73 reward: -545.6175759570282\n",
      "episode: 74 reward: -596.7344058253409\n",
      "episode: 75 reward: -567.317885520114\n",
      "episode: 76 reward: -544.4308546681814\n",
      "episode: 77 reward: -502.0377831277137\n",
      "episode: 78 reward: -594.8474971025307\n",
      "episode: 79 reward: -559.6152753187201\n",
      "episode: 80 reward: -381.39873047672444\n",
      "episode: 81 reward: -493.6967699870573\n",
      "episode: 82 reward: -445.88983875629725\n",
      "episode: 83 reward: -522.0555593482399\n",
      "episode: 84 reward: -479.31333265025086\n",
      "episode: 85 reward: -428.15331149564486\n",
      "episode: 86 reward: -588.9890279031354\n",
      "episode: 87 reward: -396.9753313132667\n",
      "episode: 88 reward: -568.9930836409001\n",
      "episode: 89 reward: -486.45253025634435\n",
      "episode: 90 reward: -378.9340617037203\n",
      "episode: 91 reward: -377.5903795231819\n",
      "episode: 92 reward: -358.429189725591\n",
      "episode: 93 reward: -453.16260743551277\n",
      "episode: 94 reward: -385.38532004251687\n",
      "episode: 95 reward: -264.5175423284917\n",
      "episode: 96 reward: -313.0107047747577\n",
      "episode: 97 reward: -352.71156265577633\n",
      "episode: 98 reward: -199.91298722350626\n",
      "episode: 99 reward: -253.18452794896118\n",
      "episode: 100 reward: -182.68393898741994\n",
      "episode: 101 reward: -391.600480512736\n",
      "episode: 102 reward: -368.8505949333855\n",
      "episode: 103 reward: -310.2160911744337\n",
      "episode: 104 reward: -227.33332071461703\n",
      "episode: 105 reward: -312.856810444383\n",
      "episode: 106 reward: -302.9451647613734\n",
      "episode: 107 reward: -380.9145511679926\n",
      "episode: 108 reward: -342.9454248478685\n",
      "episode: 109 reward: -264.9404731802435\n",
      "episode: 110 reward: -341.54419639831417\n",
      "episode: 111 reward: -361.1906396785002\n",
      "episode: 112 reward: -235.26914144017698\n",
      "episode: 113 reward: -418.8382605222428\n",
      "episode: 114 reward: -357.6283825118455\n",
      "episode: 115 reward: -409.653609861027\n",
      "episode: 116 reward: -403.02028920431565\n",
      "episode: 117 reward: -444.24789741518464\n",
      "episode: 118 reward: -394.21874298745274\n",
      "episode: 119 reward: -332.28229853034895\n",
      "episode: 120 reward: -384.88726699753767\n",
      "episode: 121 reward: -364.26475183768605\n",
      "episode: 122 reward: -371.0707500806189\n",
      "episode: 123 reward: -342.81459431235754\n",
      "episode: 124 reward: -385.66637730207225\n",
      "episode: 125 reward: -374.26837924847365\n",
      "episode: 126 reward: -360.1655518782637\n",
      "episode: 127 reward: -328.87568008066285\n",
      "episode: 128 reward: -398.1629208901027\n",
      "episode: 129 reward: -379.97661253047977\n",
      "episode: 130 reward: -389.3775470561238\n",
      "episode: 131 reward: -404.18937051571953\n",
      "episode: 132 reward: -421.9339739953245\n",
      "episode: 133 reward: -411.9945518084723\n",
      "episode: 134 reward: -396.5718979973485\n",
      "episode: 135 reward: -397.3864422324362\n",
      "episode: 136 reward: -376.5675985476064\n",
      "episode: 137 reward: -370.9961766047384\n",
      "episode: 138 reward: -419.9592096877002\n",
      "episode: 139 reward: -378.40395637734633\n",
      "episode: 140 reward: -412.91544380356385\n",
      "episode: 141 reward: -406.9735954768945\n",
      "episode: 142 reward: -405.99083518646154\n",
      "episode: 143 reward: -355.05048069698256\n",
      "episode: 144 reward: -405.38509437977484\n",
      "episode: 145 reward: -417.2598276072241\n",
      "episode: 146 reward: -429.47547188163406\n",
      "episode: 147 reward: -420.9105329120126\n",
      "episode: 148 reward: -410.80662351435296\n",
      "episode: 149 reward: -428.8009221756158\n",
      "episode: 150 reward: -420.00516747855113\n",
      "episode: 151 reward: -433.78732462324166\n",
      "episode: 152 reward: -395.3372293936646\n",
      "episode: 153 reward: -413.90333535992653\n",
      "episode: 154 reward: -426.31736624400764\n",
      "episode: 155 reward: -404.6181083846335\n",
      "episode: 156 reward: -394.98687340068096\n",
      "episode: 157 reward: -439.41481502820625\n",
      "episode: 158 reward: -480.4200703553529\n",
      "episode: 159 reward: -416.9545085692985\n",
      "episode: 160 reward: -453.1740562726722\n",
      "episode: 161 reward: -423.0629089014697\n",
      "episode: 162 reward: -410.60461604868976\n",
      "episode: 163 reward: -358.8718053473127\n",
      "episode: 164 reward: -388.85137449533994\n",
      "episode: 165 reward: -388.8982775627875\n",
      "episode: 166 reward: -367.7473945089031\n",
      "episode: 167 reward: -355.79639177456136\n",
      "episode: 168 reward: -340.4028953402792\n",
      "episode: 169 reward: -347.2249170237742\n",
      "episode: 170 reward: -359.87832443889056\n",
      "episode: 171 reward: -364.5739509857203\n",
      "episode: 172 reward: -337.8262403228175\n",
      "episode: 173 reward: -304.06554767665915\n",
      "episode: 174 reward: -279.2840702085158\n",
      "episode: 175 reward: -255.22807445954842\n",
      "episode: 176 reward: -295.86188611928696\n",
      "episode: 177 reward: -420.91931877723044\n",
      "episode: 178 reward: -192.28014930212166\n",
      "episode: 179 reward: -470.7634436955429\n",
      "episode: 180 reward: -212.73499938890785\n",
      "episode: 181 reward: -197.18347108517196\n",
      "episode: 182 reward: -194.6496049401021\n",
      "episode: 183 reward: -211.14189194123662\n",
      "episode: 184 reward: -197.99602015393458\n",
      "episode: 185 reward: -79.50176713937111\n",
      "episode: 186 reward: -173.68503712349496\n",
      "episode: 187 reward: -234.01451498226464\n",
      "episode: 188 reward: -203.16342663851918\n",
      "episode: 189 reward: -177.77093059317335\n",
      "episode: 190 reward: -195.6167962928773\n",
      "episode: 191 reward: -161.70232248945103\n",
      "episode: 192 reward: -195.79735849859554\n",
      "episode: 193 reward: -68.0377347428634\n",
      "episode: 194 reward: -209.10434490486995\n",
      "episode: 195 reward: -185.7243077342531\n",
      "episode: 196 reward: -222.8154759346588\n",
      "episode: 197 reward: -119.26522580619447\n",
      "episode: 198 reward: -228.5209787685164\n",
      "episode: 199 reward: -183.78749934491861\n",
      "episode: 200 reward: -224.9822928472804\n",
      "episode: 201 reward: -178.10063180282307\n",
      "episode: 202 reward: -216.79104465277754\n",
      "episode: 203 reward: -216.5434823768095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 204 reward: -208.45953305789487\n",
      "episode: 205 reward: -186.02351037636566\n",
      "episode: 206 reward: -102.49398031014175\n",
      "episode: 207 reward: -204.97813435975007\n",
      "episode: 208 reward: -144.50872762729617\n",
      "episode: 209 reward: -204.13124876808678\n",
      "episode: 210 reward: -203.1463065344698\n",
      "episode: 211 reward: -166.60191439819528\n",
      "episode: 212 reward: -144.18963980850455\n",
      "episode: 213 reward: -198.37004013383108\n",
      "episode: 214 reward: -229.82572419729865\n",
      "episode: 215 reward: -177.82667980689962\n",
      "episode: 216 reward: -176.72956455991329\n",
      "episode: 217 reward: -210.52785160041282\n",
      "episode: 218 reward: -163.90683215380886\n",
      "episode: 219 reward: -177.40116407154167\n",
      "episode: 220 reward: -179.72572463067138\n",
      "episode: 221 reward: -173.0306820535867\n",
      "episode: 222 reward: -195.00359872345305\n",
      "episode: 223 reward: -154.26997335706784\n",
      "episode: 224 reward: -127.65625215225828\n",
      "episode: 225 reward: -154.02885857354045\n",
      "episode: 226 reward: -169.6159373520187\n",
      "episode: 227 reward: -153.6750219204473\n",
      "episode: 228 reward: -166.85671098151897\n",
      "episode: 229 reward: -134.7244979543466\n",
      "episode: 230 reward: -172.43082649915334\n",
      "episode: 231 reward: -169.54205536705334\n",
      "episode: 232 reward: -148.8876724629086\n",
      "episode: 233 reward: -142.98317423320165\n",
      "episode: 234 reward: -189.96008872177623\n",
      "episode: 235 reward: -156.7035430630943\n",
      "episode: 236 reward: -166.40919018463637\n",
      "episode: 237 reward: -188.84059509832286\n",
      "episode: 238 reward: -171.20556892459254\n",
      "episode: 239 reward: -161.3817889751747\n",
      "episode: 240 reward: -188.96603391232327\n",
      "episode: 241 reward: -167.6940568828325\n",
      "episode: 242 reward: -195.9300437451065\n",
      "episode: 243 reward: -193.05153981678447\n",
      "episode: 244 reward: -192.1939301705874\n",
      "episode: 245 reward: -199.85534516650648\n",
      "episode: 246 reward: -192.564410186742\n",
      "episode: 247 reward: -189.00959098504168\n",
      "episode: 248 reward: -185.52443324469766\n",
      "episode: 249 reward: -169.65644787259373\n",
      "episode: 250 reward: -193.825368625478\n",
      "episode: 251 reward: -142.74870486591755\n",
      "episode: 252 reward: -161.45172311785956\n",
      "episode: 253 reward: -192.20645411181417\n",
      "episode: 254 reward: -183.23684872742376\n",
      "episode: 255 reward: -174.52074661927475\n",
      "episode: 256 reward: -194.32509728248425\n",
      "episode: 257 reward: -191.92737080256288\n",
      "episode: 258 reward: -167.20413056853252\n",
      "episode: 259 reward: -204.65120283993522\n",
      "episode: 260 reward: -163.95780377426837\n",
      "episode: 261 reward: -224.07671162412777\n",
      "episode: 262 reward: -197.13784148352528\n",
      "episode: 263 reward: -187.4907581939724\n",
      "episode: 264 reward: -114.86634769620295\n",
      "episode: 265 reward: -171.183473914562\n",
      "episode: 266 reward: -166.2370649790061\n",
      "episode: 267 reward: -168.01991063301924\n",
      "episode: 268 reward: -168.65378468455236\n",
      "episode: 269 reward: -185.87693116569983\n",
      "episode: 270 reward: -198.70326222785172\n",
      "episode: 271 reward: -163.52535581776016\n",
      "episode: 272 reward: -174.70504502755563\n",
      "episode: 273 reward: -213.1777496460848\n",
      "episode: 274 reward: -168.47059998163456\n",
      "episode: 275 reward: -215.44061854682388\n",
      "episode: 276 reward: -234.16128689308263\n",
      "episode: 277 reward: -137.44537889812182\n",
      "episode: 278 reward: -172.13076252416238\n",
      "episode: 279 reward: -196.44435651885146\n",
      "episode: 280 reward: -208.84737744313836\n",
      "episode: 281 reward: -173.67419960362716\n",
      "episode: 282 reward: -156.50414879136525\n",
      "episode: 283 reward: -194.693634930145\n",
      "episode: 284 reward: -182.7536993005495\n",
      "episode: 285 reward: -398.1457032403869\n",
      "episode: 286 reward: -197.02915092701375\n",
      "episode: 287 reward: -450.9141129654674\n",
      "episode: 288 reward: -180.35672982100266\n",
      "episode: 289 reward: -189.2922722999689\n",
      "episode: 290 reward: -215.64437772884986\n",
      "episode: 291 reward: -178.24417789477818\n",
      "episode: 292 reward: -198.48074101875346\n",
      "episode: 293 reward: -185.40464692836684\n",
      "episode: 294 reward: -162.53013034534882\n",
      "episode: 295 reward: -240.801392216077\n",
      "episode: 296 reward: -208.2240829751137\n",
      "episode: 297 reward: -92.74631633577827\n",
      "episode: 298 reward: -177.63456163498182\n",
      "episode: 299 reward: -212.32482677132387\n",
      "episode: 300 reward: -230.41908289548755\n",
      "episode: 301 reward: -210.91447524478963\n",
      "episode: 302 reward: -243.3752148229393\n",
      "episode: 303 reward: -226.2227746654504\n",
      "episode: 304 reward: -172.00938831231886\n",
      "episode: 305 reward: -183.03555066927603\n",
      "episode: 306 reward: -210.00937846215913\n",
      "episode: 307 reward: -167.91776110617016\n",
      "episode: 308 reward: -190.48078522734005\n",
      "episode: 309 reward: -203.95714440907489\n",
      "episode: 310 reward: -158.01840413047137\n",
      "episode: 311 reward: -171.57950353041196\n",
      "episode: 312 reward: -163.04529712276184\n",
      "episode: 313 reward: -192.6632904004353\n",
      "episode: 314 reward: -208.17354913529584\n",
      "episode: 315 reward: -162.9027373760697\n",
      "episode: 316 reward: -170.26929682521194\n",
      "episode: 317 reward: -253.8174129512291\n",
      "episode: 318 reward: -150.93175133243517\n",
      "episode: 319 reward: -166.50195161511766\n",
      "episode: 320 reward: -176.76872984715635\n",
      "episode: 321 reward: -153.9521218008012\n",
      "episode: 322 reward: -165.8082234134817\n",
      "episode: 323 reward: -162.75778998056825\n",
      "episode: 324 reward: -206.88596947119072\n",
      "episode: 325 reward: -174.78295669558446\n",
      "episode: 326 reward: -210.78105892145928\n",
      "episode: 327 reward: -174.9522219844215\n",
      "episode: 328 reward: -191.51413189572986\n",
      "episode: 329 reward: -166.59653080893042\n",
      "episode: 330 reward: -169.8785921915987\n",
      "episode: 331 reward: -164.23636345288838\n",
      "episode: 332 reward: -164.4997635868304\n",
      "episode: 333 reward: -161.040419086375\n",
      "episode: 334 reward: -166.6263728935265\n",
      "episode: 335 reward: -175.65358648878527\n",
      "episode: 336 reward: -200.68606063174786\n",
      "episode: 337 reward: -169.63954323428376\n",
      "episode: 338 reward: -163.5737212921603\n",
      "episode: 339 reward: -180.11688967270686\n",
      "episode: 340 reward: -167.59918504670634\n",
      "episode: 341 reward: -159.58971679705365\n",
      "episode: 342 reward: -184.2200987708273\n",
      "episode: 343 reward: -166.8240541666242\n",
      "episode: 344 reward: -171.86960167580077\n",
      "episode: 345 reward: -168.60242178289656\n",
      "episode: 346 reward: -185.9711613777344\n",
      "episode: 347 reward: -157.46665428925678\n",
      "episode: 348 reward: -171.0409013850686\n",
      "episode: 349 reward: -170.32091352401275\n",
      "episode: 350 reward: -163.33083079968873\n",
      "episode: 351 reward: -162.65134491800555\n",
      "episode: 352 reward: -151.9627670962921\n",
      "episode: 353 reward: -185.7732479716678\n",
      "episode: 354 reward: -147.59163365488368\n",
      "episode: 355 reward: -204.68132715618952\n",
      "episode: 356 reward: -157.73969854377088\n",
      "episode: 357 reward: -203.5854917301706\n",
      "episode: 358 reward: -166.66142412760775\n",
      "episode: 359 reward: -142.1375819027176\n",
      "episode: 360 reward: -141.11942546527752\n",
      "episode: 361 reward: -145.2174643835136\n",
      "episode: 362 reward: -152.83445836410672\n",
      "episode: 363 reward: -166.166385334606\n",
      "episode: 364 reward: -138.70896977640447\n",
      "episode: 365 reward: -174.11957006479244\n",
      "episode: 366 reward: -172.62675514707456\n",
      "episode: 367 reward: -182.75112701272153\n",
      "episode: 368 reward: -175.7655286549317\n",
      "episode: 369 reward: -163.43306177613738\n",
      "episode: 370 reward: -155.97529575720188\n",
      "episode: 371 reward: -164.37338886014186\n",
      "episode: 372 reward: -182.0529853019267\n",
      "episode: 373 reward: -144.77408080146358\n",
      "episode: 374 reward: -152.31302435897788\n",
      "episode: 375 reward: -153.44680343546526\n",
      "episode: 376 reward: -179.19503816197573\n",
      "episode: 377 reward: -179.88354126928343\n",
      "episode: 378 reward: -186.4964696385078\n",
      "episode: 379 reward: -173.1812021125156\n",
      "episode: 380 reward: -130.81485585745483\n",
      "episode: 381 reward: -173.16310327123904\n",
      "episode: 382 reward: -143.91577117868667\n",
      "episode: 383 reward: -138.82037559391787\n",
      "episode: 384 reward: -234.5629906278224\n",
      "episode: 385 reward: -295.4775416877315\n",
      "episode: 386 reward: -278.1144980485375\n",
      "episode: 387 reward: -176.02724767203404\n",
      "episode: 388 reward: -159.43946048229833\n",
      "episode: 389 reward: -167.39498742926213\n",
      "episode: 390 reward: -104.2296857424706\n",
      "episode: 391 reward: -225.07268981241413\n",
      "episode: 392 reward: -150.6552029956418\n",
      "episode: 393 reward: -162.35724548530888\n",
      "episode: 394 reward: -161.00354387557826\n",
      "episode: 395 reward: -184.63073094311014\n",
      "episode: 396 reward: -270.77345810833566\n",
      "episode: 397 reward: -174.28565341681397\n",
      "episode: 398 reward: -169.21584385778993\n",
      "episode: 399 reward: -152.56847006721244\n",
      "episode: 400 reward: -429.331163844765\n",
      "episode: 401 reward: -159.5951381241557\n",
      "episode: 402 reward: -248.65287124296196\n",
      "episode: 403 reward: -145.1520878877859\n",
      "episode: 404 reward: -181.2607601131715\n",
      "episode: 405 reward: -224.5699479895233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 406 reward: -381.30473572750765\n",
      "episode: 407 reward: -223.08556373090966\n",
      "episode: 408 reward: -75.13335974147395\n",
      "episode: 409 reward: -239.70892811441024\n",
      "episode: 410 reward: -229.15229287615662\n",
      "episode: 411 reward: -322.4740800261673\n",
      "episode: 412 reward: -211.50681750886818\n",
      "episode: 413 reward: -164.86452139128153\n",
      "episode: 414 reward: -217.08654131743566\n",
      "episode: 415 reward: -130.7270882541523\n",
      "episode: 416 reward: -270.59072901730104\n",
      "episode: 417 reward: -186.82526939782724\n",
      "episode: 418 reward: -263.0587601512783\n",
      "episode: 419 reward: -214.13598709533426\n",
      "episode: 420 reward: -157.01027244109272\n",
      "episode: 421 reward: -156.8989601187941\n",
      "episode: 422 reward: -157.3788848955874\n",
      "episode: 423 reward: -302.33723044978854\n",
      "episode: 424 reward: -113.43178860591335\n",
      "episode: 425 reward: -237.23038689470863\n",
      "episode: 426 reward: -205.50622845546638\n",
      "episode: 427 reward: -86.90556628710611\n",
      "episode: 428 reward: -144.68937343312257\n",
      "episode: 429 reward: -126.6551214526395\n",
      "episode: 430 reward: -159.07982757268138\n",
      "episode: 431 reward: -113.38596964712391\n",
      "episode: 432 reward: -231.4620888411439\n",
      "episode: 433 reward: -257.46980759123585\n",
      "episode: 434 reward: -191.71098104829855\n",
      "episode: 435 reward: -151.35987359650724\n",
      "episode: 436 reward: -168.58254784635866\n",
      "episode: 437 reward: -147.82622154986043\n",
      "episode: 438 reward: -324.8330813518271\n",
      "episode: 439 reward: -210.4051583726243\n",
      "episode: 440 reward: -261.08933236987883\n",
      "episode: 441 reward: -172.6932719699201\n",
      "episode: 442 reward: -198.46689859021427\n",
      "episode: 443 reward: -200.14970957804016\n",
      "episode: 444 reward: -144.15348377616053\n",
      "episode: 445 reward: -169.78908622272294\n",
      "episode: 446 reward: -207.26689740617064\n",
      "episode: 447 reward: -153.59401707048795\n",
      "episode: 448 reward: -130.74452986027543\n",
      "episode: 449 reward: -193.7826142389285\n"
     ]
    }
   ],
   "source": [
    "class DQN(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.local_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.target_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.beta\n",
    "        )\n",
    "        self.experience_replay = ExperienceReplay(num_experience=2048)\n",
    "        \n",
    "        self.update_count = 0\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            self.epsilon *= 0.99\n",
    "            if self.epsilon<0.01:\n",
    "                self.epsilon= 0.01\n",
    "            \n",
    "            action = self.local_q_network.epsilon_greedy(state, self.epsilon)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            if self.update_count%4 == 0:\n",
    "                batch_state, \\\n",
    "                batch_action, \\\n",
    "                batch_reward, \\\n",
    "                batch_next_state, \\\n",
    "                batch_done = self.experience_replay.recall(batch_size=64)\n",
    "\n",
    "                # detach because we only backprop local network and update target network weight manually\n",
    "                targets_next_Q = self.target_q_network.forward(batch_next_state).detach().max(1)[0]\n",
    "                targets_Q = batch_reward + (self.gamma * targets_next_Q * (1 - batch_done))\n",
    "\n",
    "                local_Q = self.local_q_network.forward(batch_state)\n",
    "                expected_Q = local_Q.gather(1, batch_action.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "                loss = F.mse_loss(expected_Q, targets_Q)\n",
    "                self.local_q_network.backprop(loss)\n",
    "            \n",
    "                update_params(self.local_q_network.model, self.target_q_network.model, self.tau)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            self.update_count += 1\n",
    "            if done: return total_reward\n",
    "        \n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_episodes=1000,\n",
    "      alpha=0.0001, \n",
    "      beta=0.0001,\n",
    "      gamma=.99)\n",
    "dqn.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([1,0], dtype=torch.long)\n",
    "a[b]\n",
    "\n",
    "a.gather(1, b.unsqueeze(1).long()).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "State shape:  (8,)\n",
      "Number of actions:  4\n",
      "Episode 100\tAverage Score: -225.00\n",
      "Episode 200\tAverage Score: -172.18\n",
      "Episode 272\tAverage Score: -107.51"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-f019b2d7bc5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-f019b2d7bc5e>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mBeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mcontactListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mBeginContact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixtureB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

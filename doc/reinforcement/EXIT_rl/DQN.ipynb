{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4) <br>\n",
    "[ref](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import print_weight_size, copy_params, update_params, ExperienceReplay, convert_to_tensor\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 reward: -226.44267540066366\n",
      "episode: 1 reward: -326.62694074050955\n",
      "episode: 2 reward: -351.1618916365005\n",
      "episode: 3 reward: -238.70116875024559\n",
      "episode: 4 reward: -358.52970085684\n",
      "episode: 5 reward: -705.1513261532389\n",
      "episode: 6 reward: -902.6289750560985\n",
      "episode: 7 reward: -542.8400062226499\n",
      "episode: 8 reward: -325.94967965055076\n",
      "episode: 9 reward: -195.68620219895047\n",
      "episode: 10 reward: -157.03485895256097\n",
      "episode: 11 reward: -155.07345102613698\n",
      "episode: 12 reward: -335.19486260536985\n",
      "episode: 13 reward: -107.28955070040168\n",
      "episode: 14 reward: -134.5242711741377\n",
      "episode: 15 reward: -115.10715352842595\n",
      "episode: 16 reward: -120.41377269354793\n",
      "episode: 17 reward: -117.69433396457848\n",
      "episode: 18 reward: -121.25979196350139\n",
      "episode: 19 reward: -121.03164120309083\n",
      "episode: 20 reward: -127.62192166571037\n",
      "episode: 21 reward: -124.48268513541106\n",
      "episode: 22 reward: -133.4827280362311\n",
      "episode: 23 reward: -150.30187188751324\n",
      "episode: 24 reward: -218.83925985542567\n",
      "episode: 25 reward: -135.55337975710918\n",
      "episode: 26 reward: -153.74871435938377\n",
      "episode: 27 reward: -167.42613458879208\n",
      "episode: 28 reward: -148.5305688298817\n",
      "episode: 29 reward: -170.77371515213207\n",
      "episode: 30 reward: -149.6562612660535\n",
      "episode: 31 reward: -17.136903742905176\n",
      "episode: 32 reward: -152.74125922319328\n",
      "episode: 33 reward: -154.13918465002448\n",
      "episode: 34 reward: -159.42753942074694\n",
      "episode: 35 reward: -15.665781832155346\n",
      "episode: 36 reward: -121.08371173253181\n",
      "episode: 37 reward: -168.11561237279057\n",
      "episode: 38 reward: -121.76263131350126\n",
      "episode: 39 reward: -141.32150626936073\n",
      "episode: 40 reward: -139.72309980504951\n",
      "episode: 41 reward: -111.92090598717026\n",
      "episode: 42 reward: -151.3255535101727\n",
      "episode: 43 reward: -256.05607880522314\n",
      "episode: 44 reward: -134.4727629362034\n",
      "episode: 45 reward: -129.82486603865732\n",
      "episode: 46 reward: -112.60888495175688\n",
      "episode: 47 reward: -127.00213432261287\n",
      "episode: 48 reward: -92.2125531571734\n",
      "episode: 49 reward: -136.7411923930668\n",
      "episode: 50 reward: -130.35116358347668\n",
      "episode: 51 reward: -138.47705895913245\n",
      "episode: 52 reward: -94.64250256563274\n",
      "episode: 53 reward: -122.19236579513175\n",
      "episode: 54 reward: -115.98797593067425\n",
      "episode: 55 reward: -110.01497379409855\n",
      "episode: 56 reward: -130.53551357522622\n",
      "episode: 57 reward: -128.47699842364926\n",
      "episode: 58 reward: -232.93605234220007\n",
      "episode: 59 reward: -134.8779894465419\n",
      "episode: 60 reward: -208.27555288843433\n",
      "episode: 61 reward: -5.9931816973575\n",
      "episode: 62 reward: -192.02101608870467\n",
      "episode: 63 reward: -214.4475117295291\n",
      "episode: 64 reward: -265.77943677686125\n",
      "episode: 65 reward: -233.9341083219622\n",
      "episode: 66 reward: -248.83612027105414\n",
      "episode: 67 reward: -307.18494143701844\n",
      "episode: 68 reward: -332.4801335200219\n",
      "episode: 69 reward: -276.0453186034214\n",
      "episode: 70 reward: -325.3312464810782\n",
      "episode: 71 reward: -305.07233817624933\n",
      "episode: 72 reward: -194.07549694251838\n",
      "episode: 73 reward: -227.60007042055804\n",
      "episode: 74 reward: -185.39568421255066\n",
      "episode: 75 reward: -204.7028819084885\n",
      "episode: 76 reward: -147.72540522333946\n",
      "episode: 77 reward: -355.0264631031264\n",
      "episode: 78 reward: -274.7119345261518\n",
      "episode: 79 reward: -198.17842978764958\n",
      "episode: 80 reward: -431.28228237283435\n",
      "episode: 81 reward: -425.17436946414693\n",
      "episode: 82 reward: -432.9990522797574\n",
      "episode: 83 reward: -291.1577748887913\n",
      "episode: 84 reward: -368.83160604196195\n",
      "episode: 85 reward: -363.1696184848257\n",
      "episode: 86 reward: -231.12141559593942\n",
      "episode: 87 reward: -485.305718882111\n",
      "episode: 88 reward: -253.1112923544458\n",
      "episode: 89 reward: -294.7115609787439\n",
      "episode: 90 reward: -218.31916781227736\n",
      "episode: 91 reward: -200.21840447243318\n",
      "episode: 92 reward: -212.94749500125334\n",
      "episode: 93 reward: -270.6187803575192\n",
      "episode: 94 reward: -306.613646246488\n",
      "episode: 95 reward: -221.55929962498072\n",
      "episode: 96 reward: -293.0211919216407\n",
      "episode: 97 reward: -249.6945845931822\n",
      "episode: 98 reward: -277.04204135335067\n",
      "episode: 99 reward: -322.57291663233116\n",
      "episode: 100 reward: -280.96833676677954\n",
      "episode: 101 reward: -265.15217711405614\n",
      "episode: 102 reward: -317.631888205322\n",
      "episode: 103 reward: -270.08371022951206\n",
      "episode: 104 reward: -236.02893459903476\n",
      "episode: 105 reward: -353.18661781848664\n",
      "episode: 106 reward: -240.07364710701927\n",
      "episode: 107 reward: -457.2916958385324\n",
      "episode: 108 reward: -381.8668040663044\n",
      "episode: 109 reward: -515.2157969268486\n",
      "episode: 110 reward: -673.1669337933761\n",
      "episode: 111 reward: -387.5429829916263\n",
      "episode: 112 reward: -353.5594247658131\n",
      "episode: 113 reward: -345.75507055335834\n",
      "episode: 114 reward: -295.12385993509906\n",
      "episode: 115 reward: -274.40320236610376\n",
      "episode: 116 reward: -322.3084528600225\n",
      "episode: 117 reward: -218.18381173960103\n",
      "episode: 118 reward: -358.17593788760956\n",
      "episode: 119 reward: -238.7419903836468\n",
      "episode: 120 reward: -328.64204885484367\n",
      "episode: 121 reward: -248.1286275049664\n",
      "episode: 122 reward: -307.7422741470058\n",
      "episode: 123 reward: -417.1821375582276\n",
      "episode: 124 reward: -188.97123098269404\n",
      "episode: 125 reward: -142.64593309395445\n",
      "episode: 126 reward: -169.51250138241545\n",
      "episode: 127 reward: -186.5121362468666\n",
      "episode: 128 reward: -206.59896360164726\n",
      "episode: 129 reward: -127.26463401541399\n",
      "episode: 130 reward: -140.67091538026722\n",
      "episode: 131 reward: -230.31494878332472\n",
      "episode: 132 reward: -151.85198366764894\n",
      "episode: 133 reward: -160.04447607871887\n",
      "episode: 134 reward: -306.1694219323458\n",
      "episode: 135 reward: -266.4538515851803\n",
      "episode: 136 reward: -205.9492712714554\n",
      "episode: 137 reward: -388.6231797993756\n",
      "episode: 138 reward: -25.76052538425192\n",
      "episode: 139 reward: -216.25318694141754\n",
      "episode: 140 reward: -150.39886441557482\n",
      "episode: 141 reward: -222.53937175695654\n",
      "episode: 142 reward: -142.61459436549094\n",
      "episode: 143 reward: -315.7738745747348\n",
      "episode: 144 reward: -141.24968565428117\n",
      "episode: 145 reward: -159.07323845667423\n",
      "episode: 146 reward: -189.9263224976491\n",
      "episode: 147 reward: -218.74148220502798\n",
      "episode: 148 reward: -181.66380756124926\n",
      "episode: 149 reward: -297.9381009999905\n",
      "episode: 150 reward: -350.3819407648628\n",
      "episode: 151 reward: -207.64092770042697\n",
      "episode: 152 reward: -375.57654686098545\n",
      "episode: 153 reward: -159.83852886867476\n",
      "episode: 154 reward: -170.6988059600355\n",
      "episode: 155 reward: 32.09455795547794\n",
      "episode: 156 reward: -149.10740549856712\n",
      "episode: 157 reward: -178.40328767456884\n",
      "episode: 158 reward: -178.91424458499273\n",
      "episode: 159 reward: -165.76342594569772\n",
      "episode: 160 reward: -154.980349700126\n",
      "episode: 161 reward: -149.908632574043\n",
      "episode: 162 reward: -169.40275789133582\n",
      "episode: 163 reward: -197.73416236480813\n",
      "episode: 164 reward: -172.12204144911044\n",
      "episode: 165 reward: -97.79702008960817\n",
      "episode: 166 reward: -159.86604119949357\n",
      "episode: 167 reward: -167.0564043184567\n",
      "episode: 168 reward: -135.55834538360227\n",
      "episode: 169 reward: -184.48310593423895\n",
      "episode: 170 reward: -173.84061701865798\n",
      "episode: 171 reward: -234.10693543375587\n",
      "episode: 172 reward: -181.71190732209567\n",
      "episode: 173 reward: -116.69902223685031\n",
      "episode: 174 reward: -170.55846984213565\n",
      "episode: 175 reward: -183.23690639200538\n",
      "episode: 176 reward: -198.31467958443656\n",
      "episode: 177 reward: -163.1338148814061\n",
      "episode: 178 reward: -158.12863789539944\n",
      "episode: 179 reward: -94.91316377327956\n",
      "episode: 180 reward: -183.0778073043383\n",
      "episode: 181 reward: -134.45268229818845\n",
      "episode: 182 reward: -167.21320179068346\n",
      "episode: 183 reward: -156.7179662602943\n",
      "episode: 184 reward: -140.50211078807502\n",
      "episode: 185 reward: -144.21356794762266\n",
      "episode: 186 reward: -150.2835348496176\n",
      "episode: 187 reward: -166.76708198313122\n",
      "episode: 188 reward: -143.2685511614161\n",
      "episode: 189 reward: -146.41489906897624\n",
      "episode: 190 reward: -75.95314156316334\n",
      "episode: 191 reward: -172.67259961190683\n",
      "episode: 192 reward: -183.43515350322778\n",
      "episode: 193 reward: -117.07807424011872\n",
      "episode: 194 reward: -155.20265436217264\n",
      "episode: 195 reward: -141.54248362040516\n",
      "episode: 196 reward: -121.0858156237845\n",
      "episode: 197 reward: -109.97513751820415\n",
      "episode: 198 reward: -122.61104973920355\n",
      "episode: 199 reward: -125.66117219882483\n",
      "episode: 200 reward: -151.8850058176314\n",
      "episode: 201 reward: -134.40966040827146\n",
      "episode: 202 reward: -129.55892645877645\n",
      "episode: 203 reward: -188.06076927649616\n",
      "episode: 204 reward: -134.27264930802104\n",
      "episode: 205 reward: -125.11070849270044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 206 reward: -137.77467800709184\n",
      "episode: 207 reward: -125.60115862217208\n",
      "episode: 208 reward: -129.32349369158774\n",
      "episode: 209 reward: -178.00068452048774\n",
      "episode: 210 reward: -145.66206134201934\n",
      "episode: 211 reward: -164.87384316988278\n",
      "episode: 212 reward: -170.50260366363625\n",
      "episode: 213 reward: -199.84084315623477\n",
      "episode: 214 reward: -159.50350787057462\n",
      "episode: 215 reward: -232.02855736860747\n",
      "episode: 216 reward: -278.99987668252874\n",
      "episode: 217 reward: -98.60868014719193\n",
      "episode: 218 reward: -365.3060770387167\n",
      "episode: 219 reward: -170.91583462184525\n",
      "episode: 220 reward: -148.82016705236748\n",
      "episode: 221 reward: -168.37681030627596\n",
      "episode: 222 reward: -175.1514081332245\n",
      "episode: 223 reward: -157.879783313023\n",
      "episode: 224 reward: -164.79724712656156\n",
      "episode: 225 reward: -300.6455830229578\n",
      "episode: 226 reward: -126.35905508005511\n",
      "episode: 227 reward: -162.70301297985958\n",
      "episode: 228 reward: -181.11504333439706\n",
      "episode: 229 reward: -149.1370178663248\n",
      "episode: 230 reward: -157.06941751171047\n",
      "episode: 231 reward: -104.22608378839085\n",
      "episode: 232 reward: -170.2423571625925\n",
      "episode: 233 reward: -171.11931766212894\n",
      "episode: 234 reward: -206.06077297598893\n",
      "episode: 235 reward: -157.97696652813545\n",
      "episode: 236 reward: -142.4002311760591\n",
      "episode: 237 reward: -163.90959148874595\n",
      "episode: 238 reward: -221.16599968291825\n",
      "episode: 239 reward: -159.13293975090482\n",
      "episode: 240 reward: -120.43216813135781\n",
      "episode: 241 reward: -165.12128025386707\n",
      "episode: 242 reward: -172.43305241282755\n",
      "episode: 243 reward: -235.61846642829528\n",
      "episode: 244 reward: -164.2388792787934\n",
      "episode: 245 reward: -188.94247035089475\n",
      "episode: 246 reward: -163.67174862537507\n",
      "episode: 247 reward: -25.249970038939665\n",
      "episode: 248 reward: -164.98193760044566\n",
      "episode: 249 reward: -132.55091671548655\n",
      "episode: 250 reward: -171.39453098188739\n",
      "episode: 251 reward: -178.0736306117014\n",
      "episode: 252 reward: -189.1500040042789\n",
      "episode: 253 reward: -128.553139410557\n",
      "episode: 254 reward: -97.99565570137682\n",
      "episode: 255 reward: -168.59191437696848\n",
      "episode: 256 reward: -135.05768707804756\n",
      "episode: 257 reward: -142.29786625381863\n",
      "episode: 258 reward: -130.08159474820883\n",
      "episode: 259 reward: -147.26718005997188\n",
      "episode: 260 reward: -149.15369554918362\n",
      "episode: 261 reward: -127.05881114066482\n",
      "episode: 262 reward: -155.10616508689083\n",
      "episode: 263 reward: -149.82403750981868\n",
      "episode: 264 reward: -176.64135826483886\n",
      "episode: 265 reward: -161.5565766694834\n",
      "episode: 266 reward: -159.25139530815753\n",
      "episode: 267 reward: -153.54753387971874\n",
      "episode: 268 reward: -108.62408211186147\n",
      "episode: 269 reward: -158.12477738014962\n",
      "episode: 270 reward: -150.2576555998898\n",
      "episode: 271 reward: -163.63454929992156\n",
      "episode: 272 reward: -125.23629512982558\n",
      "episode: 273 reward: -186.41249398227507\n",
      "episode: 274 reward: -141.75946639278885\n",
      "episode: 275 reward: -176.15761669408357\n",
      "episode: 276 reward: -127.69900022137422\n",
      "episode: 277 reward: -163.6850849651542\n",
      "episode: 278 reward: -355.3563404801556\n",
      "episode: 279 reward: -126.84571087375645\n",
      "episode: 280 reward: -225.37154416384612\n",
      "episode: 281 reward: -140.64723609197793\n",
      "episode: 282 reward: -150.23416626584066\n",
      "episode: 283 reward: -137.77293449418133\n",
      "episode: 284 reward: -152.17188033162705\n",
      "episode: 285 reward: -150.434213735915\n",
      "episode: 286 reward: -123.38574730898351\n",
      "episode: 287 reward: -173.58179563077164\n",
      "episode: 288 reward: -154.9766830379576\n",
      "episode: 289 reward: -184.06726450633124\n",
      "episode: 290 reward: -150.39934250778802\n",
      "episode: 291 reward: -132.3085765195892\n",
      "episode: 292 reward: -119.4992893369004\n",
      "episode: 293 reward: -168.7661628560698\n",
      "episode: 294 reward: -149.9438178348089\n",
      "episode: 295 reward: -172.704773143211\n",
      "episode: 296 reward: -157.69608085358328\n",
      "episode: 297 reward: -154.01105263807403\n",
      "episode: 298 reward: -15.372903979519904\n",
      "episode: 299 reward: -200.11127394464285\n",
      "episode: 300 reward: -214.56446750178475\n",
      "episode: 301 reward: -166.60720883179823\n",
      "episode: 302 reward: -185.89050927363795\n",
      "episode: 303 reward: -134.76375873117337\n",
      "episode: 304 reward: -160.20281841366193\n",
      "episode: 305 reward: -129.99997892849908\n",
      "episode: 306 reward: -130.0319814841729\n",
      "episode: 307 reward: -187.48120597324106\n",
      "episode: 308 reward: -167.24265885698378\n",
      "episode: 309 reward: -123.28573666490668\n",
      "episode: 310 reward: -133.50190352645313\n",
      "episode: 311 reward: -180.8789738876404\n",
      "episode: 312 reward: -98.54636878343929\n",
      "episode: 313 reward: -172.89343160078747\n",
      "episode: 314 reward: -181.11030543593972\n",
      "episode: 315 reward: -184.05972715717013\n",
      "episode: 316 reward: -167.12302415915983\n",
      "episode: 317 reward: -136.1935045741621\n",
      "episode: 318 reward: -156.41846703602977\n",
      "episode: 319 reward: -155.88603177550652\n",
      "episode: 320 reward: -118.22255187607273\n",
      "episode: 321 reward: -152.05999324447117\n",
      "episode: 322 reward: -161.67069852509024\n",
      "episode: 323 reward: -46.0269735177258\n",
      "episode: 324 reward: -103.13286980220232\n",
      "episode: 325 reward: -181.43807125994886\n",
      "episode: 326 reward: -230.20467329271676\n",
      "episode: 327 reward: -141.7312424287446\n",
      "episode: 328 reward: -151.13847927492048\n",
      "episode: 329 reward: -159.81245136470068\n",
      "episode: 330 reward: -150.89941635492713\n",
      "episode: 331 reward: -146.84930438481044\n",
      "episode: 332 reward: -125.2931351928883\n",
      "episode: 333 reward: -155.18085794869123\n",
      "episode: 334 reward: -164.4821223572075\n",
      "episode: 335 reward: -152.6595804726016\n",
      "episode: 336 reward: -139.03448553852783\n",
      "episode: 337 reward: -147.27341973410253\n",
      "episode: 338 reward: -166.25785354984208\n",
      "episode: 339 reward: -125.44199722270073\n",
      "episode: 340 reward: -148.2319227293264\n",
      "episode: 341 reward: -109.42869079486566\n",
      "episode: 342 reward: -166.66911544485092\n",
      "episode: 343 reward: -165.7388597403624\n",
      "episode: 344 reward: -139.3773420504441\n",
      "episode: 345 reward: -163.98153095440273\n",
      "episode: 346 reward: -127.70462898090898\n",
      "episode: 347 reward: -92.20832404848056\n",
      "episode: 348 reward: -121.58166063020248\n",
      "episode: 349 reward: -109.3950018927703\n",
      "episode: 350 reward: -100.95026833756293\n",
      "episode: 351 reward: -134.50849923180152\n",
      "episode: 352 reward: -122.83344428979134\n",
      "episode: 353 reward: -126.41360066741578\n",
      "episode: 354 reward: -111.83678895580769\n",
      "episode: 355 reward: -137.30984557382703\n",
      "episode: 356 reward: -106.4122648450667\n",
      "episode: 357 reward: -159.43950550813855\n",
      "episode: 358 reward: -144.17469175013468\n",
      "episode: 359 reward: -118.46711743096046\n",
      "episode: 360 reward: -104.98780706857174\n",
      "episode: 361 reward: -129.89862793587218\n",
      "episode: 362 reward: -124.96397783240585\n",
      "episode: 363 reward: -146.47902887932761\n",
      "episode: 364 reward: -136.78788186739143\n",
      "episode: 365 reward: -134.27492932151745\n",
      "episode: 366 reward: -160.02953032574612\n",
      "episode: 367 reward: -116.354710149978\n",
      "episode: 368 reward: -131.51334989129742\n",
      "episode: 369 reward: -122.92932730224767\n",
      "episode: 370 reward: -160.17889791756002\n",
      "episode: 371 reward: -157.3716568656476\n",
      "episode: 372 reward: -177.0136142470819\n",
      "episode: 373 reward: -150.7806241511609\n",
      "episode: 374 reward: -108.0337954470507\n",
      "episode: 375 reward: -131.69507610326218\n",
      "episode: 376 reward: -130.80710009804054\n",
      "episode: 377 reward: -130.4830539806975\n",
      "episode: 378 reward: -170.75653240087846\n",
      "episode: 379 reward: -125.51996235632325\n",
      "episode: 380 reward: -162.3350401512418\n",
      "episode: 381 reward: -126.6115053818179\n",
      "episode: 382 reward: -157.33199942789196\n",
      "episode: 383 reward: -173.74657759377658\n",
      "episode: 384 reward: -137.62169859665153\n",
      "episode: 385 reward: -158.2932205214579\n",
      "episode: 386 reward: -135.18578573603781\n",
      "episode: 387 reward: -148.63607261469153\n",
      "episode: 388 reward: -146.45669076273413\n",
      "episode: 389 reward: -181.79812205285378\n",
      "episode: 390 reward: -138.93527980512735\n",
      "episode: 391 reward: -148.698077118938\n",
      "episode: 392 reward: -161.8677375473538\n",
      "episode: 393 reward: -153.53782867586528\n",
      "episode: 394 reward: -243.01694267831894\n",
      "episode: 395 reward: -147.67010200169648\n",
      "episode: 396 reward: -147.95650088860782\n",
      "episode: 397 reward: -135.4817233056309\n",
      "episode: 398 reward: -136.2148659155289\n",
      "episode: 399 reward: -153.41958887446137\n",
      "episode: 400 reward: -146.100511437124\n",
      "episode: 401 reward: -119.82558796371005\n",
      "episode: 402 reward: -138.41649463013727\n",
      "episode: 403 reward: -164.71143000644759\n",
      "episode: 404 reward: -159.145080608539\n",
      "episode: 405 reward: -146.09077533614652\n",
      "episode: 406 reward: -135.6665713482899\n",
      "episode: 407 reward: -162.47113982696155\n",
      "episode: 408 reward: -162.80897785816558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 409 reward: -148.30256480716793\n",
      "episode: 410 reward: -119.31309187060631\n",
      "episode: 411 reward: -99.78940674001292\n",
      "episode: 412 reward: -132.1106521327839\n",
      "episode: 413 reward: -146.477423133122\n",
      "episode: 414 reward: -127.55454097288319\n",
      "episode: 415 reward: -114.00437696207915\n",
      "episode: 416 reward: -107.94647165490437\n",
      "episode: 417 reward: -128.33730220768373\n",
      "episode: 418 reward: -112.93548788396288\n",
      "episode: 419 reward: 42.76650762490968\n",
      "episode: 420 reward: -137.92407585215415\n",
      "episode: 421 reward: -116.5660139563826\n",
      "episode: 422 reward: -137.03539926835387\n",
      "episode: 423 reward: -143.95185434732588\n",
      "episode: 424 reward: -111.46743941670094\n",
      "episode: 425 reward: -128.33397090945903\n",
      "episode: 426 reward: -118.68188008180712\n",
      "episode: 427 reward: -113.37358116261329\n",
      "episode: 428 reward: -123.70026015352221\n",
      "episode: 429 reward: -116.94949689615444\n",
      "episode: 430 reward: -111.88994466994444\n",
      "episode: 431 reward: -112.05394892223885\n",
      "episode: 432 reward: -134.79157676482887\n",
      "episode: 433 reward: -111.69325499233118\n",
      "episode: 434 reward: -110.99101347315451\n",
      "episode: 435 reward: -121.99407894644966\n",
      "episode: 436 reward: -108.54965266635219\n",
      "episode: 437 reward: -85.42624649157224\n",
      "episode: 438 reward: -151.78074075396174\n",
      "episode: 439 reward: -131.3262769856779\n",
      "episode: 440 reward: -117.46793034150235\n",
      "episode: 441 reward: -110.00454155227297\n",
      "episode: 442 reward: -128.2766768982971\n",
      "episode: 443 reward: -93.46425276974492\n",
      "episode: 444 reward: -91.52479994735967\n",
      "episode: 445 reward: -99.60239942532064\n",
      "episode: 446 reward: -133.4249297862489\n",
      "episode: 447 reward: -136.53593815749412\n",
      "episode: 448 reward: -138.2369544399978\n",
      "episode: 449 reward: -140.65444206345344\n",
      "episode: 450 reward: -127.46773291841905\n",
      "episode: 451 reward: -151.08567514473293\n",
      "episode: 452 reward: -117.74962400244374\n",
      "episode: 453 reward: -80.00698082468259\n",
      "episode: 454 reward: -142.141533433031\n",
      "episode: 455 reward: -132.92650793815628\n",
      "episode: 456 reward: -131.75952599788232\n",
      "episode: 457 reward: -105.84920981042717\n",
      "episode: 458 reward: -104.88583839510933\n",
      "episode: 459 reward: -126.95060278777486\n",
      "episode: 460 reward: -137.5510432864899\n",
      "episode: 461 reward: -137.44553558185785\n",
      "episode: 462 reward: -114.71242831912231\n",
      "episode: 463 reward: -110.5547494467573\n",
      "episode: 464 reward: -158.59520412053877\n",
      "episode: 465 reward: -119.16887246094973\n",
      "episode: 466 reward: -99.3518527322474\n",
      "episode: 467 reward: -140.38697197190118\n",
      "episode: 468 reward: -145.83368800816322\n",
      "episode: 469 reward: -133.57115894578422\n",
      "episode: 470 reward: -160.8879960238583\n",
      "episode: 471 reward: -129.59853001116778\n",
      "episode: 472 reward: -113.13104866278535\n",
      "episode: 473 reward: -124.01197500437553\n",
      "episode: 474 reward: -118.24305946092397\n",
      "episode: 475 reward: -76.20553760694584\n",
      "episode: 476 reward: -129.9951342778892\n",
      "episode: 477 reward: -130.61929064439533\n",
      "episode: 478 reward: -109.1896296598739\n",
      "episode: 479 reward: -94.4094503034651\n",
      "episode: 480 reward: -115.8653129645665\n",
      "episode: 481 reward: -114.30910466149197\n",
      "episode: 482 reward: -120.28065672950098\n",
      "episode: 483 reward: -121.07119652363612\n",
      "episode: 484 reward: -86.5525412514368\n",
      "episode: 485 reward: -137.3071505875688\n",
      "episode: 486 reward: -66.76332545283238\n",
      "episode: 487 reward: -114.81539466319859\n",
      "episode: 488 reward: -107.82702862321614\n",
      "episode: 489 reward: -82.01445682246293\n",
      "episode: 490 reward: -217.09196466713541\n",
      "episode: 491 reward: -87.87730782623227\n",
      "episode: 492 reward: -138.5632386583659\n",
      "episode: 493 reward: -105.52851615275218\n",
      "episode: 494 reward: -122.79688040609496\n",
      "episode: 495 reward: -138.14567338045006\n",
      "episode: 496 reward: -118.87397236623808\n",
      "episode: 497 reward: -90.1774064144426\n",
      "episode: 498 reward: -142.59791599575934\n",
      "episode: 499 reward: -83.83884402033634\n",
      "episode: 500 reward: -83.84554674265979\n",
      "episode: 501 reward: -87.58734801837102\n",
      "episode: 502 reward: -120.06983656958782\n",
      "episode: 503 reward: -109.15424448182189\n",
      "episode: 504 reward: -113.12847471372338\n",
      "episode: 505 reward: -116.6998957076712\n",
      "episode: 506 reward: -131.46276878370702\n",
      "episode: 507 reward: -66.08509336256168\n",
      "episode: 508 reward: -108.74754384895814\n",
      "episode: 509 reward: -103.85798960364345\n",
      "episode: 510 reward: -111.56531263333015\n",
      "episode: 511 reward: -104.63802977822377\n",
      "episode: 512 reward: -109.31647551768322\n",
      "episode: 513 reward: -116.61396090888125\n",
      "episode: 514 reward: -116.58453030637185\n",
      "episode: 515 reward: -98.44641423125773\n",
      "episode: 516 reward: -82.15791794411764\n",
      "episode: 517 reward: -116.40627609283472\n",
      "episode: 518 reward: -123.73029299081685\n",
      "episode: 519 reward: -112.10147440556719\n",
      "episode: 520 reward: -125.23313739381014\n",
      "episode: 521 reward: -130.42540877073174\n",
      "episode: 522 reward: -121.9720205624167\n",
      "episode: 523 reward: -131.2763124285003\n",
      "episode: 524 reward: -117.46313401769206\n",
      "episode: 525 reward: -140.01254777643334\n",
      "episode: 526 reward: -119.23681904881772\n",
      "episode: 527 reward: -125.5959326144499\n",
      "episode: 528 reward: -124.30915000929987\n",
      "episode: 529 reward: -97.2406181191594\n",
      "episode: 530 reward: -114.81413984179804\n",
      "episode: 531 reward: -114.89031953419104\n",
      "episode: 532 reward: -126.64138101281354\n",
      "episode: 533 reward: -125.16283033122954\n",
      "episode: 534 reward: -101.01924340620118\n",
      "episode: 535 reward: -93.82419741294355\n",
      "episode: 536 reward: -129.08376883970635\n",
      "episode: 537 reward: -93.23697188710649\n",
      "episode: 538 reward: -109.87332413466316\n",
      "episode: 539 reward: -142.15606126470968\n",
      "episode: 540 reward: -137.56030515015402\n",
      "episode: 541 reward: -143.37616717941725\n",
      "episode: 542 reward: -97.11203279478248\n",
      "episode: 543 reward: -99.71705507239335\n",
      "episode: 544 reward: -118.43322180596809\n",
      "episode: 545 reward: -117.67071396766654\n",
      "episode: 546 reward: -123.3489459829702\n",
      "episode: 547 reward: -108.29040597027006\n",
      "episode: 548 reward: -134.39309973560972\n",
      "episode: 549 reward: -112.44380666596716\n",
      "episode: 550 reward: -112.00052599641934\n",
      "episode: 551 reward: -114.95087984892034\n",
      "episode: 552 reward: -92.50508759738142\n",
      "episode: 553 reward: -108.76588230599151\n",
      "episode: 554 reward: -92.92414698028489\n",
      "episode: 555 reward: -105.74049235591099\n",
      "episode: 556 reward: -67.55690129271808\n",
      "episode: 557 reward: -73.11219604208566\n",
      "episode: 558 reward: -117.06115124745185\n",
      "episode: 559 reward: -129.71063600369337\n",
      "episode: 560 reward: -125.91611639328445\n",
      "episode: 561 reward: -139.16740844934088\n",
      "episode: 562 reward: -117.14420290235252\n",
      "episode: 563 reward: -76.6830353480183\n",
      "episode: 564 reward: -113.89223366366619\n",
      "episode: 565 reward: -132.90769008437053\n",
      "episode: 566 reward: -127.42795190462283\n",
      "episode: 567 reward: -114.84089419340953\n",
      "episode: 568 reward: -106.77058581245757\n",
      "episode: 569 reward: -119.26604208388841\n",
      "episode: 570 reward: -124.7695468595251\n",
      "episode: 571 reward: -122.72805654662321\n",
      "episode: 572 reward: -100.44569852647574\n",
      "episode: 573 reward: -110.0286158445964\n",
      "episode: 574 reward: -124.8792435715044\n",
      "episode: 575 reward: -97.25582560911542\n",
      "episode: 576 reward: -107.60514825658214\n",
      "episode: 577 reward: -121.89163696063322\n",
      "episode: 578 reward: -133.21725413878997\n",
      "episode: 579 reward: -130.11903637087383\n",
      "episode: 580 reward: -116.62479864744387\n",
      "episode: 581 reward: -123.15052088446235\n",
      "episode: 582 reward: -126.323173146751\n",
      "episode: 583 reward: -132.47072779475153\n",
      "episode: 584 reward: -87.17004831759098\n",
      "episode: 585 reward: -139.61005461841646\n",
      "episode: 586 reward: -122.30563588791122\n",
      "episode: 587 reward: -126.17872689602785\n",
      "episode: 588 reward: -122.46911390789735\n",
      "episode: 589 reward: -94.99872235091934\n",
      "episode: 590 reward: -89.26160671344485\n",
      "episode: 591 reward: -140.90042903895\n",
      "episode: 592 reward: -116.7395005576132\n",
      "episode: 593 reward: -121.20493919510432\n",
      "episode: 594 reward: -121.39281353828812\n",
      "episode: 595 reward: -123.14671794955808\n",
      "episode: 596 reward: -106.02558080979024\n",
      "episode: 597 reward: -104.33431445249352\n",
      "episode: 598 reward: -131.1796067200295\n",
      "episode: 599 reward: -144.9164356050415\n",
      "episode: 600 reward: -143.28314842814737\n",
      "episode: 601 reward: -92.97707535941902\n",
      "episode: 602 reward: -135.81563424766196\n",
      "episode: 603 reward: -136.0936408718441\n",
      "episode: 604 reward: -142.15213474070168\n",
      "episode: 605 reward: -113.18989877860429\n",
      "episode: 606 reward: -111.2413399191605\n",
      "episode: 607 reward: -116.75652014408614\n",
      "episode: 608 reward: -102.11502608084142\n",
      "episode: 609 reward: -137.7290123391448\n",
      "episode: 610 reward: -122.01239432410634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 611 reward: -113.0613085745108\n",
      "episode: 612 reward: -123.22337174122457\n",
      "episode: 613 reward: -108.67452497163957\n",
      "episode: 614 reward: -119.71435542135882\n",
      "episode: 615 reward: -107.14335401180472\n",
      "episode: 616 reward: -129.20344846546325\n",
      "episode: 617 reward: -110.10206539743844\n",
      "episode: 618 reward: -122.5788954022165\n",
      "episode: 619 reward: -88.11924520589321\n",
      "episode: 620 reward: -120.92448315662824\n",
      "episode: 621 reward: -138.2802241229189\n",
      "episode: 622 reward: -99.64729789735574\n",
      "episode: 623 reward: -97.91218362565016\n",
      "episode: 624 reward: -115.89548128831576\n",
      "episode: 625 reward: -119.38063583394128\n",
      "episode: 626 reward: -107.96253319688587\n",
      "episode: 627 reward: -135.0787032307579\n",
      "episode: 628 reward: -130.33369881938503\n",
      "episode: 629 reward: -85.09681796091931\n",
      "episode: 630 reward: -109.60725626735979\n",
      "episode: 631 reward: -125.80957618225685\n",
      "episode: 632 reward: -106.03858697979068\n",
      "episode: 633 reward: -107.45873084621162\n",
      "episode: 634 reward: -86.92326791612825\n",
      "episode: 635 reward: -123.40425089165257\n",
      "episode: 636 reward: -133.49654719602123\n",
      "episode: 637 reward: -138.13118318445206\n",
      "episode: 638 reward: -118.75510805142983\n",
      "episode: 639 reward: -150.73621514559363\n",
      "episode: 640 reward: -128.9181247523269\n",
      "episode: 641 reward: -110.20695698268945\n",
      "episode: 642 reward: -108.72676640116276\n",
      "episode: 643 reward: -117.25347605366407\n",
      "episode: 644 reward: -130.34354126427584\n",
      "episode: 645 reward: -129.39784219507897\n",
      "episode: 646 reward: -118.35417460017788\n",
      "episode: 647 reward: -129.54427942875017\n",
      "episode: 648 reward: -126.15180898018033\n",
      "episode: 649 reward: -118.06720384425691\n",
      "episode: 650 reward: -124.64920781653102\n",
      "episode: 651 reward: -89.32499562021653\n",
      "episode: 652 reward: -138.74838908220602\n",
      "episode: 653 reward: -131.5236435567038\n",
      "episode: 654 reward: -140.6523735732646\n",
      "episode: 655 reward: -91.51804187958338\n",
      "episode: 656 reward: -125.94384923058988\n",
      "episode: 657 reward: -115.23112398158058\n",
      "episode: 658 reward: -112.35542174350927\n",
      "episode: 659 reward: -133.22847902416567\n",
      "episode: 660 reward: -115.2651700617637\n",
      "episode: 661 reward: -123.67532244313647\n",
      "episode: 662 reward: -134.25685170354734\n",
      "episode: 663 reward: -127.61152296247275\n",
      "episode: 664 reward: -123.65101597139557\n",
      "episode: 665 reward: -112.99021992242706\n",
      "episode: 666 reward: -145.40311957651477\n",
      "episode: 667 reward: -118.19076122027846\n",
      "episode: 668 reward: -130.97218663976435\n",
      "episode: 669 reward: -134.77024208007884\n",
      "episode: 670 reward: -122.33494097110507\n",
      "episode: 671 reward: -142.15586404237916\n",
      "episode: 672 reward: -136.35300345167525\n",
      "episode: 673 reward: -105.92474516447382\n",
      "episode: 674 reward: -134.19195332066064\n",
      "episode: 675 reward: -138.99281132727208\n",
      "episode: 676 reward: -136.76295711402094\n",
      "episode: 677 reward: -128.1904085402486\n",
      "episode: 678 reward: -119.32352418191614\n",
      "episode: 679 reward: -118.90693460519437\n",
      "episode: 680 reward: -125.66801986762094\n",
      "episode: 681 reward: -81.99610100513226\n",
      "episode: 682 reward: -98.14726920055908\n",
      "episode: 683 reward: -129.13936986085673\n",
      "episode: 684 reward: -98.37971532830247\n",
      "episode: 685 reward: -118.24114202085929\n",
      "episode: 686 reward: -115.00979849589791\n",
      "episode: 687 reward: -125.240992568598\n",
      "episode: 688 reward: -108.77263072744273\n",
      "episode: 689 reward: -132.19116423762313\n",
      "episode: 690 reward: -130.16461248085312\n",
      "episode: 691 reward: -130.13227033354173\n",
      "episode: 692 reward: -120.99991702165657\n",
      "episode: 693 reward: -111.73758805131511\n",
      "episode: 694 reward: -74.10856800785959\n",
      "episode: 695 reward: -91.56845279893591\n",
      "episode: 696 reward: -124.5240111084546\n",
      "episode: 697 reward: -102.53635916551974\n",
      "episode: 698 reward: -151.57971157339296\n",
      "episode: 699 reward: -98.5524511611992\n",
      "episode: 700 reward: -112.93508698540089\n",
      "episode: 701 reward: -99.37712611100672\n",
      "episode: 702 reward: -140.16208521444543\n",
      "episode: 703 reward: -115.21044569315923\n",
      "episode: 704 reward: -126.95414149885957\n",
      "episode: 705 reward: -116.47566361375786\n",
      "episode: 706 reward: -10.17768090807644\n",
      "episode: 707 reward: -1.7000035639451596\n",
      "episode: 708 reward: -116.11494299793937\n",
      "episode: 709 reward: -96.35545511829316\n",
      "episode: 710 reward: -124.74829456289652\n",
      "episode: 711 reward: -116.81537511112644\n",
      "episode: 712 reward: -95.08230330154157\n",
      "episode: 713 reward: -131.72406858768286\n",
      "episode: 714 reward: -128.18994461931257\n",
      "episode: 715 reward: -89.68857828865751\n",
      "episode: 716 reward: -101.75763655437012\n",
      "episode: 717 reward: -91.84018188180265\n",
      "episode: 718 reward: -98.53095353307705\n",
      "episode: 719 reward: -123.94343505638662\n",
      "episode: 720 reward: -94.9321738779719\n",
      "episode: 721 reward: -111.98922037161006\n",
      "episode: 722 reward: -105.0825122575337\n",
      "episode: 723 reward: -107.5719097111069\n",
      "episode: 724 reward: -132.2552327438175\n",
      "episode: 725 reward: -75.27549593130232\n",
      "episode: 726 reward: -93.5444395276014\n",
      "episode: 727 reward: -108.56990004387211\n",
      "episode: 728 reward: -120.87263060106383\n",
      "episode: 729 reward: -113.21102613942813\n",
      "episode: 730 reward: -119.52182369316236\n",
      "episode: 731 reward: -122.32833120784838\n",
      "episode: 732 reward: -81.75611349966255\n",
      "episode: 733 reward: -102.74973137487181\n",
      "episode: 734 reward: -114.6898440258658\n",
      "episode: 735 reward: -133.8181676860833\n",
      "episode: 736 reward: -95.21534157124847\n",
      "episode: 737 reward: -132.82940329143446\n",
      "episode: 738 reward: -101.44097314610929\n",
      "episode: 739 reward: -26.477920615732046\n",
      "episode: 740 reward: -17.638039647605723\n",
      "episode: 741 reward: -128.00562020287927\n",
      "episode: 742 reward: -98.70508499669103\n",
      "episode: 743 reward: -94.43383998636733\n",
      "episode: 744 reward: -74.34475385421416\n",
      "episode: 745 reward: -47.672818482194884\n",
      "episode: 746 reward: -110.39012322106602\n",
      "episode: 747 reward: -109.90468244546909\n",
      "episode: 748 reward: -118.99616050744903\n",
      "episode: 749 reward: -109.53021242366505\n",
      "episode: 750 reward: -112.85269816992434\n",
      "episode: 751 reward: -106.27040455272353\n",
      "episode: 752 reward: -114.56028788984045\n",
      "episode: 753 reward: -109.04510795622338\n",
      "episode: 754 reward: -108.28195570184872\n",
      "episode: 755 reward: -106.55596709626995\n",
      "episode: 756 reward: -110.8777847325072\n",
      "episode: 757 reward: -92.45604697791681\n",
      "episode: 758 reward: -93.51632812391941\n",
      "episode: 759 reward: -113.84692921180593\n",
      "episode: 760 reward: -92.65951476757547\n",
      "episode: 761 reward: -91.84826065478352\n",
      "episode: 762 reward: -107.1153547228424\n",
      "episode: 763 reward: -110.29674235773503\n",
      "episode: 764 reward: -110.64406490909704\n",
      "episode: 765 reward: -91.26877320277637\n",
      "episode: 766 reward: -104.17327146928014\n",
      "episode: 767 reward: -116.86966549144103\n",
      "episode: 768 reward: -126.81664162268872\n",
      "episode: 769 reward: -107.02374587865162\n",
      "episode: 770 reward: -122.61215290428154\n",
      "episode: 771 reward: -105.19247902086617\n",
      "episode: 772 reward: -90.83999266707687\n",
      "episode: 773 reward: -149.6555553990763\n",
      "episode: 774 reward: -110.14220450913317\n",
      "episode: 775 reward: -94.13924752559268\n",
      "episode: 776 reward: -103.0200079337451\n",
      "episode: 777 reward: -127.24918545876533\n",
      "episode: 778 reward: -89.26783419671756\n",
      "episode: 779 reward: -104.76152321389199\n",
      "episode: 780 reward: -102.7662986534621\n",
      "episode: 781 reward: -122.57011688852778\n",
      "episode: 782 reward: -98.94314353986755\n",
      "episode: 783 reward: -100.78470137961688\n",
      "episode: 784 reward: -319.07220110077026\n",
      "episode: 785 reward: -177.48293640003186\n",
      "episode: 786 reward: -112.314304989927\n",
      "episode: 787 reward: -99.52478990919981\n",
      "episode: 788 reward: -67.79245769746493\n",
      "episode: 789 reward: -95.4547574062628\n",
      "episode: 790 reward: -115.8244926546468\n",
      "episode: 791 reward: -91.00989596407646\n",
      "episode: 792 reward: -105.20594384646417\n",
      "episode: 793 reward: -100.08638771019355\n",
      "episode: 794 reward: -81.98114744652403\n",
      "episode: 795 reward: -73.42061795702321\n",
      "episode: 796 reward: -95.84052889412104\n",
      "episode: 797 reward: -112.92152798039524\n",
      "episode: 798 reward: -112.42636348200601\n",
      "episode: 799 reward: -120.809116547385\n",
      "episode: 800 reward: -308.99755000839116\n",
      "episode: 801 reward: -76.12552695002071\n",
      "episode: 802 reward: -120.69230380038219\n",
      "episode: 803 reward: -93.68883758726207\n",
      "episode: 804 reward: -105.06655399219545\n",
      "episode: 805 reward: -109.21044326801544\n",
      "episode: 806 reward: -98.38451201506824\n",
      "episode: 807 reward: -298.3847785444593\n",
      "episode: 808 reward: -119.05529752670502\n",
      "episode: 809 reward: -97.93899374409617\n",
      "episode: 810 reward: -177.30496470927574\n",
      "episode: 811 reward: -113.2385301081311\n",
      "episode: 812 reward: -115.44300587290222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 813 reward: -103.35232151771717\n",
      "episode: 814 reward: -190.45651245178726\n",
      "episode: 815 reward: -345.6810050257145\n",
      "episode: 816 reward: -311.912687707654\n",
      "episode: 817 reward: -48.33745366682152\n",
      "episode: 818 reward: -109.64648540477249\n",
      "episode: 819 reward: -221.1691327905677\n",
      "episode: 820 reward: -123.99609128879615\n",
      "episode: 821 reward: -112.01311097186367\n",
      "episode: 822 reward: -102.15701000689243\n",
      "episode: 823 reward: -105.57437169437841\n",
      "episode: 824 reward: -80.43610244534717\n",
      "episode: 825 reward: -89.5470332372083\n",
      "episode: 826 reward: -90.219439694396\n",
      "episode: 827 reward: -104.20096492324282\n",
      "episode: 828 reward: -124.51476009650096\n",
      "episode: 829 reward: -95.96700907893475\n",
      "episode: 830 reward: -85.6133824521793\n",
      "episode: 831 reward: -112.32393128726875\n",
      "episode: 832 reward: -94.65260259580646\n",
      "episode: 833 reward: -115.05370575452734\n",
      "episode: 834 reward: -95.05513341765447\n",
      "episode: 835 reward: -84.492504518006\n",
      "episode: 836 reward: -222.5723474359507\n",
      "episode: 837 reward: -95.84150245596577\n",
      "episode: 838 reward: -91.98885546315532\n",
      "episode: 839 reward: -122.88617663961405\n",
      "episode: 840 reward: -95.24688103130156\n",
      "episode: 841 reward: -102.22818530619625\n",
      "episode: 842 reward: -118.17397850862922\n",
      "episode: 843 reward: -43.53561494589101\n",
      "episode: 844 reward: -83.9370815284924\n",
      "episode: 845 reward: -110.4236328949353\n",
      "episode: 846 reward: -108.122988918473\n",
      "episode: 847 reward: -112.70315030240813\n",
      "episode: 848 reward: -51.35543024088635\n",
      "episode: 849 reward: -134.98091580607775\n",
      "episode: 850 reward: -302.00222459118174\n",
      "episode: 851 reward: -116.06865118990467\n",
      "episode: 852 reward: -89.79744231886843\n",
      "episode: 853 reward: -267.40778913668987\n",
      "episode: 854 reward: -99.59577370811697\n",
      "episode: 855 reward: -95.14820868406588\n",
      "episode: 856 reward: -79.78268491122931\n",
      "episode: 857 reward: -110.28614130795081\n",
      "episode: 858 reward: -111.58623824472687\n",
      "episode: 859 reward: -83.41281894537524\n",
      "episode: 860 reward: -100.15787811349527\n",
      "episode: 861 reward: -80.01106717831905\n",
      "episode: 862 reward: -181.94587737511713\n",
      "episode: 863 reward: -71.59278281918002\n",
      "episode: 864 reward: -92.71858835679231\n",
      "episode: 865 reward: -104.38561128911948\n",
      "episode: 866 reward: -102.3737300666056\n",
      "episode: 867 reward: -74.2951974237957\n",
      "episode: 868 reward: -103.65706805507544\n",
      "episode: 869 reward: -93.53782071547471\n",
      "episode: 870 reward: -96.21121661996067\n",
      "episode: 871 reward: -100.66025898006667\n",
      "episode: 872 reward: -109.25352353432909\n",
      "episode: 873 reward: -104.07276529394572\n",
      "episode: 874 reward: -91.11656434489507\n",
      "episode: 875 reward: -112.86182145728799\n",
      "episode: 876 reward: -336.08602978947135\n",
      "episode: 877 reward: -92.03557585412457\n",
      "episode: 878 reward: -49.70757833402524\n",
      "episode: 879 reward: -113.79102054119201\n",
      "episode: 880 reward: -113.96124923997806\n",
      "episode: 881 reward: -89.5247423534463\n",
      "episode: 882 reward: -112.2320816310853\n",
      "episode: 883 reward: -97.78886229033606\n",
      "episode: 884 reward: -99.25993484478848\n",
      "episode: 885 reward: -77.19687362449208\n",
      "episode: 886 reward: -91.89499874861254\n",
      "episode: 887 reward: -101.92760993117807\n",
      "episode: 888 reward: -93.95251371868935\n",
      "episode: 889 reward: -79.57004481787729\n",
      "episode: 890 reward: -110.46232488808619\n",
      "episode: 891 reward: -112.30228460123297\n",
      "episode: 892 reward: -87.08171295349896\n",
      "episode: 893 reward: -104.67971337674678\n",
      "episode: 894 reward: -85.71735142236616\n",
      "episode: 895 reward: -120.9057619406469\n",
      "episode: 896 reward: -104.23211723317519\n",
      "episode: 897 reward: -102.02478678796717\n",
      "episode: 898 reward: -101.30478141850915\n",
      "episode: 899 reward: -89.36655022612457\n",
      "episode: 900 reward: -109.87808626041866\n",
      "episode: 901 reward: -75.48540093441605\n",
      "episode: 902 reward: -101.3646707136589\n",
      "episode: 903 reward: -93.25142014330369\n",
      "episode: 904 reward: -131.18475613686297\n",
      "episode: 905 reward: -106.44203077645474\n",
      "episode: 906 reward: -98.28229505090567\n",
      "episode: 907 reward: -52.11862903478223\n",
      "episode: 908 reward: -110.48091457397447\n",
      "episode: 909 reward: -85.40403162320679\n",
      "episode: 910 reward: -85.38856084981695\n",
      "episode: 911 reward: -76.03833467854268\n",
      "episode: 912 reward: -111.29992405233143\n",
      "episode: 913 reward: -103.92417242076343\n",
      "episode: 914 reward: -98.2866094599765\n",
      "episode: 915 reward: -109.1351281320932\n",
      "episode: 916 reward: -12.04273542729159\n",
      "episode: 917 reward: -74.64106332866616\n",
      "episode: 918 reward: -83.76348899561668\n",
      "episode: 919 reward: -108.57408284775977\n",
      "episode: 920 reward: -112.15015759174379\n",
      "episode: 921 reward: -110.53127624762043\n",
      "episode: 922 reward: -32.39567605862953\n",
      "episode: 923 reward: -96.95212046631475\n",
      "episode: 924 reward: -106.99701151012009\n",
      "episode: 925 reward: -112.63185257080728\n",
      "episode: 926 reward: -105.51435825313689\n",
      "episode: 927 reward: -71.82560867534227\n",
      "episode: 928 reward: -65.7753952176574\n",
      "episode: 929 reward: -70.00013604725675\n",
      "episode: 930 reward: -93.19356505094234\n",
      "episode: 931 reward: -70.22031744835618\n",
      "episode: 932 reward: -99.44899868623519\n",
      "episode: 933 reward: -83.90465797959963\n",
      "episode: 934 reward: -93.42989619259649\n",
      "episode: 935 reward: -97.28544366223083\n",
      "episode: 936 reward: -78.7718193684915\n",
      "episode: 937 reward: -86.16685741955153\n",
      "episode: 938 reward: -113.01014329543783\n",
      "episode: 939 reward: -93.91967886082486\n",
      "episode: 940 reward: -78.90673159723178\n",
      "episode: 941 reward: -71.83932206222877\n",
      "episode: 942 reward: -68.063963858855\n",
      "episode: 943 reward: -91.57780575376353\n",
      "episode: 944 reward: -106.66008905333602\n",
      "episode: 945 reward: -80.62100827331975\n",
      "episode: 946 reward: -91.99915510766496\n",
      "episode: 947 reward: -115.22621920887656\n",
      "episode: 948 reward: -104.71028694616788\n",
      "episode: 949 reward: -102.36407276871671\n",
      "episode: 950 reward: -93.46006519633559\n",
      "episode: 951 reward: -103.43772971668098\n",
      "episode: 952 reward: -106.67135807127656\n",
      "episode: 953 reward: -73.24823719229342\n",
      "episode: 954 reward: -126.2324231941222\n",
      "episode: 955 reward: -88.06214763580707\n",
      "episode: 956 reward: -91.25267491596531\n",
      "episode: 957 reward: -106.77994537491836\n",
      "episode: 958 reward: -82.48478854460706\n",
      "episode: 959 reward: -91.16165971572447\n",
      "episode: 960 reward: -88.26118996413753\n",
      "episode: 961 reward: -70.75142158438433\n",
      "episode: 962 reward: -106.85306730885047\n",
      "episode: 963 reward: -110.2805375411701\n",
      "episode: 964 reward: -95.7584004105223\n",
      "episode: 965 reward: -64.79245014676283\n",
      "episode: 966 reward: -95.71934959983616\n",
      "episode: 967 reward: -115.29930390083507\n",
      "episode: 968 reward: -91.12430633366756\n",
      "episode: 969 reward: -79.1568796563956\n",
      "episode: 970 reward: -90.170596380668\n",
      "episode: 971 reward: -92.104886647448\n",
      "episode: 972 reward: -93.50984513493717\n",
      "episode: 973 reward: -82.07600821346507\n",
      "episode: 974 reward: -91.42781899295261\n",
      "episode: 975 reward: -75.3574452032444\n",
      "episode: 976 reward: -83.0040737987294\n",
      "episode: 977 reward: -73.50597528679911\n",
      "episode: 978 reward: -92.3189784877429\n",
      "episode: 979 reward: 3.6419908977725726\n",
      "episode: 980 reward: -69.14008011816811\n",
      "episode: 981 reward: -75.13363675465177\n",
      "episode: 982 reward: -98.71647893624153\n",
      "episode: 983 reward: -61.50759118642749\n",
      "episode: 984 reward: -426.74539088867317\n",
      "episode: 985 reward: -78.5098164841992\n",
      "episode: 986 reward: -115.08813099698955\n",
      "episode: 987 reward: -96.14083744103323\n",
      "episode: 988 reward: -77.66630475573507\n",
      "episode: 989 reward: -100.36300050545918\n",
      "episode: 990 reward: -72.82888385726892\n",
      "episode: 991 reward: -67.49820426970362\n",
      "episode: 992 reward: -73.65133658073216\n",
      "episode: 993 reward: -88.19895041459674\n",
      "episode: 994 reward: -77.26939062059257\n",
      "episode: 995 reward: -62.17745682106443\n",
      "episode: 996 reward: -68.82765123900437\n",
      "episode: 997 reward: -79.55412806356532\n",
      "episode: 998 reward: -88.66492241535074\n",
      "episode: 999 reward: -58.6824891180832\n"
     ]
    }
   ],
   "source": [
    "class DQN(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.local_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.target_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.beta\n",
    "        )\n",
    "        self.experience_replay = ExperienceReplay(num_experience=2048)\n",
    "        \n",
    "        self.update_count = 0\n",
    "        self.epsilon = 1\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            self.epsilon *= 0.99\n",
    "            if self.epsilon<0.01:\n",
    "                self.epsilon= 0.01\n",
    "            \n",
    "            action = self.local_q_network.epsilon_greedy(state, self.epsilon)\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            if self.update_count%4 == 0:\n",
    "                batch_state, \\\n",
    "                batch_action, \\\n",
    "                batch_reward, \\\n",
    "                batch_next_state, \\\n",
    "                batch_done = self.experience_replay.recall(batch_size=64)\n",
    "\n",
    "                # detach because we only backprop local network and update target network weight manually\n",
    "                targets_next_Q = self.target_q_network.forward(batch_next_state).detach().max(1)[0]\n",
    "                targets_Q = batch_reward + (self.gamma * targets_next_Q * (1 - batch_done))\n",
    "\n",
    "                local_Q = self.local_q_network.forward(batch_state)\n",
    "                expected_Q = local_Q.gather(1, batch_action.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "                loss = F.mse_loss(expected_Q, targets_Q)\n",
    "                self.local_q_network.backprop(loss)\n",
    "            \n",
    "                update_params(self.local_q_network.model, self.target_q_network.model, self.tau)\n",
    "            \n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            self.update_count += 1\n",
    "            if done: return total_reward\n",
    "        \n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_episodes=1000,\n",
    "      alpha=0.0001, \n",
    "      beta=0.0001,\n",
    "      gamma=.99)\n",
    "dqn.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.tensor([1,2], dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:278: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n",
      "Episode 100\tAverage Score: -203.67\n",
      "Episode 200\tAverage Score: -87.983\n",
      "Episode 299\tAverage Score: -77.68"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f019b2d7bc5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-f019b2d7bc5e>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mApplyLinearImpulse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mox\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0moy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mSIDE_ENGINE_POWER\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpulse_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mFPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlander\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

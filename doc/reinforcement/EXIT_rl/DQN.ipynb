{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4) <br>\n",
    "[github](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn) <br>\n",
    "[paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import update_params, ExperienceReplay, WeightDecay\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.local_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.target_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=0 # use manual update\n",
    "        )\n",
    "        update_params(self.local_q_network.model, self.target_q_network.model, tau=0)\n",
    "        self.num_step = 0\n",
    "    \n",
    "    def initialize(self, num_step_to_learn, eps_start, eps_end, eps_decay, num_experience, num_recall):\n",
    "        self.num_step_to_learn= num_step_to_learn\n",
    "        self.epsilon_decay = WeightDecay(eps_start, eps_end, eps_decay)\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        self.experience_replay = ExperienceReplay(num_experience=num_experience, num_recall=num_recall)\n",
    "        \n",
    "    def policy(self, state):\n",
    "        return self.local_q_network.epsilon_greedy(state, self.epsilon)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # detach because we only backprop local network and update target network weight manually\n",
    "        targets_next_Q = self.target_q_network.forward(next_state).detach().max(1)[0]\n",
    "        targets_Q = reward + (self.gamma * targets_next_Q * (1 - done))\n",
    "\n",
    "        local_Q = self.local_q_network.forward(state)\n",
    "        expected_Q = local_Q.gather(1, action.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "        loss = F.mse_loss(expected_Q, targets_Q)\n",
    "        self.local_q_network.backprop(loss)\n",
    "\n",
    "        update_params(self.local_q_network.model, self.target_q_network.model, self.tau)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            action = self.policy(state)\n",
    "            for i in range(4):\n",
    "                _state, reward, done, _ = self.env.step(action)\n",
    "                if done: break\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            self.num_step += 1\n",
    "            if self.num_step%self.num_step_to_learn == 0:\n",
    "                experiences = self.experience_replay.recall()\n",
    "                self.learn(*experiences)\n",
    "            state = _state\n",
    "            \n",
    "            total_reward += reward\n",
    "            if done: break\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        return total_reward\n",
    "    \n",
    "    def _save(self, reward):\n",
    "        torch.save(self.local_q_network.model.state_dict(), self.save_name)\n",
    "    def _load(self):\n",
    "        self.epsilon = 0\n",
    "        self.local_q_network.model.load_state_dict(torch.load(self.save_name, map_location=device))\n",
    "        self.local_q_network.model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -154.91\n",
      "Episode 200\tAverage Score: -89.525\n",
      "Episode 300\tAverage Score: -29.54\n",
      "Episode 400\tAverage Score: 40.573\n",
      "Episode 500\tAverage Score: 177.55\n",
      "Episode 522\tAverage Score: 200.56--- early stop ----\n",
      " current_mean_reward: 200.56308424718296 num_mean_episode: 100\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/LunarLander-v2-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2024, \n",
    "               num_recall=512)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.33\n",
      "Episode 200\tAverage Score: 1.21\n",
      "Episode 300\tAverage Score: 0.91\n",
      "Episode 400\tAverage Score: 1.33\n",
      "Episode 500\tAverage Score: 2.20\n",
      "Episode 600\tAverage Score: 2.40\n",
      "Episode 700\tAverage Score: 3.04\n",
      "Episode 800\tAverage Score: 3.14\n",
      "Episode 900\tAverage Score: 3.54\n",
      "Episode 1000\tAverage Score: 3.84\n",
      "Episode 1100\tAverage Score: 4.02\n",
      "Episode 1200\tAverage Score: 4.38\n",
      "Episode 1300\tAverage Score: 4.48\n",
      "Episode 1400\tAverage Score: 4.56\n",
      "Episode 1500\tAverage Score: 4.31\n",
      "Episode 1519\tAverage Score: 4.43"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-6fe575e93dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[0mnum_experience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                num_recall=512)\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# dqn.play()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged, early_stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mean_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-4821e903e9ac>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/helpers.py\u001b[0m in \u001b[0;36mremember\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 self.memories[i] = torch.cat(\n\u001b[0;32m---> 56\u001b[0;31m                     (self.memories[i], torch.tensor([args[i]], dtype=torch.float, device=device)), dim=0)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('Breakout-ram-v0')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/Breakout-ram-v0-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2024, \n",
    "               num_recall=512)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8\tAverage Score: 0.62"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-4868f63aee83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                \u001b[0mnum_experience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                num_recall=512)\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;31m# dqn.play()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged, early_stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mean_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-a6dca9e0127c>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step_to_learn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/helpers.py\u001b[0m in \u001b[0;36mrecall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mmemory_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_recall\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_recall\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmemory_length\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmemory_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m         \u001b[0;31m# invariant:  non-selected at [0,n-i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# move non-selected item into vacancy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/random.py\u001b[0m in \u001b[0;36m_randbelow\u001b[0;34m(self, n, int, maxsize, type, Method, BuiltinMethod)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     def _randbelow(self, n, int=int, maxsize=1<<BPF, type=type,\n\u001b[0m\u001b[1;32m    224\u001b[0m                    Method=_MethodType, BuiltinMethod=_BuiltinMethodType):\n\u001b[1;32m    225\u001b[0m         \u001b[0;34m\"Return a random int in the range [0,n).  Raises ValueError if n==0.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('Breakout-ram-v0')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=20000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/Breakout-ram-v0-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2024, \n",
    "               num_recall=512)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

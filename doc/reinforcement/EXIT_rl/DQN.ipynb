{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[medium](https://medium.com/@jonathan_hui/rl-dqn-deep-q-network-e207751f7ae4) <br>\n",
    "[github](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn) <br>\n",
    "[paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import update_params, ExperienceReplay, WeightDecay, device\n",
    "from EXITrl.nn_wrapper import NNWrapper\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Base):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.local_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=self.alpha\n",
    "        )\n",
    "        self.target_q_network = NNWrapper(\n",
    "            QNetwork(self.num_state, 64, self.num_action),\n",
    "            lr=0 # use manual update\n",
    "        )\n",
    "        update_params(self.local_q_network.model, self.target_q_network.model, tau=0)\n",
    "        self.num_step = 0\n",
    "    \n",
    "    def initialize(self, num_step_to_learn, eps_start, eps_end, eps_decay, num_experience, num_recall, skip_frame=1):\n",
    "        self.num_step_to_learn= num_step_to_learn\n",
    "        self.epsilon_decay = WeightDecay(eps_start, eps_end, eps_decay)\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        self.experience_replay = ExperienceReplay(num_experience=num_experience, num_recall=num_recall)\n",
    "        self.skip_frame = skip_frame\n",
    "        \n",
    "    def policy(self, state):\n",
    "        return self.local_q_network.epsilon_greedy(state, self.epsilon)\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # detach because we only backprop local network and update target network weight manually\n",
    "        targets_next_Q = self.target_q_network.forward(next_state).detach().max(1)[0]\n",
    "        targets_Q = reward + (self.gamma * targets_next_Q * (1 - done))\n",
    "\n",
    "        local_Q = self.local_q_network.forward(state)\n",
    "        expected_Q = local_Q.gather(1, action.unsqueeze(1).long()).squeeze(1)\n",
    "\n",
    "        loss = F.mse_loss(expected_Q, targets_Q)\n",
    "        self.local_q_network.backprop(loss)\n",
    "\n",
    "        update_params(self.local_q_network.model, self.target_q_network.model, self.tau)\n",
    "\n",
    "    def _loop(self, episode) -> int:\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        for i in range(1000):\n",
    "            action = self.policy(state)\n",
    "            for _ in range(self.skip_frame):\n",
    "                _state, reward, done, _ = self.env.step(action)\n",
    "                if done: break\n",
    "            self.experience_replay.remember(state, action, reward, _state, done)\n",
    "            \n",
    "            self.num_step += 1\n",
    "            if self.num_step%self.num_step_to_learn == 0:\n",
    "                experiences = self.experience_replay.recall()\n",
    "                self.learn(*experiences)\n",
    "            state = _state\n",
    "            \n",
    "            total_reward += reward\n",
    "            self.additional_log['num_step'] = self.num_step\n",
    "            if done: break\n",
    "        self.epsilon = self.epsilon_decay.step()\n",
    "        return total_reward\n",
    "    \n",
    "    def _save(self, reward):\n",
    "        torch.save(self.local_q_network.model.state_dict(), self.save_name)\n",
    "    def _load(self):\n",
    "        self.epsilon = 0\n",
    "        self.local_q_network.model.load_state_dict(torch.load(self.save_name, map_location=device))\n",
    "        self.local_q_network.model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -158.27 \tother{'num_step': 11629}\n",
      "Episode 200\tAverage Score: -111.80 \tother{'num_step': 32919}\n",
      "Episode 300\tAverage Score: -57.77 \tother{'num_step': 83802}}\n",
      "Episode 400\tAverage Score: -34.82 \tother{'num_step': 153762}\n",
      "Episode 500\tAverage Score: 66.82 \tother{'num_step': 211704}}\n",
      "Episode 600\tAverage Score: 153.99 \tother{'num_step': 250725}\n",
      "Episode 700\tAverage Score: 128.93 \tother{'num_step': 292653}\n",
      "Episode 800\tAverage Score: 176.08 \tother{'num_step': 332247}\n",
      "Episode 900\tAverage Score: 190.32 \tother{'num_step': 368841}\n",
      "--- early stop ---- Score: 200.77 \tother{'num_step': 370030}\n",
      " current_mean_reward: 200.77107305470724 episode: 904\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/LunarLander-v2-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2048, \n",
    "               num_recall=512,\n",
    "               skip_frame=1)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -148.99\n",
      "Episode 200\tAverage Score: -142.11\n",
      "Episode 300\tAverage Score: -149.07\n",
      "Episode 400\tAverage Score: -150.33\n",
      "Episode 500\tAverage Score: -166.24\n",
      "Episode 600\tAverage Score: -175.00\n",
      "Episode 700\tAverage Score: -155.82\n",
      "Episode 800\tAverage Score: -142.49\n",
      "Episode 900\tAverage Score: -95.593\n",
      "Episode 1000\tAverage Score: -10.25\n",
      "Episode 1100\tAverage Score: 32.34\n",
      "Episode 1200\tAverage Score: 36.93\n",
      "Episode 1300\tAverage Score: 31.62\n",
      "Episode 1400\tAverage Score: 53.91\n",
      "Episode 1500\tAverage Score: 56.46\n",
      "Episode 1600\tAverage Score: 70.57\n",
      "Episode 1700\tAverage Score: 54.78\n",
      "Episode 1800\tAverage Score: 103.50\n",
      "Episode 1900\tAverage Score: 102.85\n",
      "Episode 2000\tAverage Score: 85.092\n"
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('LunarLander-v2')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=2000,\n",
    "      alpha=5e-4, \n",
    "      gamma=.95,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/LunarLander-v2-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2048, \n",
    "               num_recall=32,\n",
    "               skip_frame=3)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.42\n",
      "Episode 200\tAverage Score: 1.44\n",
      "Episode 300\tAverage Score: 1.37\n",
      "Episode 400\tAverage Score: 1.14\n",
      "Episode 500\tAverage Score: 1.13\n",
      "Episode 600\tAverage Score: 1.43\n",
      "Episode 700\tAverage Score: 1.45\n",
      "Episode 800\tAverage Score: 1.41\n",
      "Episode 900\tAverage Score: 1.77\n",
      "Episode 1000\tAverage Score: 1.87\n",
      "Episode 1100\tAverage Score: 1.69\n",
      "Episode 1200\tAverage Score: 1.84\n",
      "Episode 1300\tAverage Score: 1.78\n",
      "Episode 1400\tAverage Score: 1.92\n",
      "Episode 1500\tAverage Score: 1.76\n",
      "Episode 1600\tAverage Score: 1.93\n",
      "Episode 1700\tAverage Score: 2.06\n",
      "Episode 1800\tAverage Score: 2.27\n",
      "Episode 1900\tAverage Score: 2.04\n",
      "Episode 2000\tAverage Score: 1.83\n",
      "Episode 2100\tAverage Score: 2.35\n",
      "Episode 2200\tAverage Score: 2.24\n",
      "Episode 2300\tAverage Score: 2.30\n",
      "Episode 2400\tAverage Score: 2.50\n",
      "Episode 2500\tAverage Score: 2.84\n",
      "Episode 2600\tAverage Score: 3.05\n",
      "Episode 2700\tAverage Score: 3.50\n",
      "Episode 2800\tAverage Score: 3.26\n",
      "Episode 2900\tAverage Score: 3.66\n",
      "Episode 3000\tAverage Score: 3.46\n",
      "Episode 3100\tAverage Score: 3.60\n",
      "Episode 3200\tAverage Score: 3.91\n",
      "Episode 3300\tAverage Score: 3.51\n",
      "Episode 3400\tAverage Score: 3.71\n",
      "Episode 3500\tAverage Score: 3.61\n",
      "Episode 3600\tAverage Score: 3.60\n",
      "Episode 3700\tAverage Score: 3.83\n",
      "Episode 3800\tAverage Score: 3.84\n",
      "Episode 3900\tAverage Score: 3.87\n",
      "Episode 4000\tAverage Score: 4.00\n",
      "Episode 4100\tAverage Score: 4.16\n",
      "Episode 4200\tAverage Score: 3.86\n",
      "Episode 4300\tAverage Score: 4.03\n",
      "Episode 4400\tAverage Score: 3.90\n",
      "Episode 4500\tAverage Score: 4.15\n",
      "Episode 4600\tAverage Score: 4.32\n",
      "Episode 4700\tAverage Score: 4.36\n",
      "Episode 4800\tAverage Score: 3.63\n",
      "Episode 4900\tAverage Score: 3.07\n",
      "Episode 5000\tAverage Score: 3.19\n",
      "Episode 5100\tAverage Score: 3.98\n",
      "Episode 5200\tAverage Score: 3.82\n",
      "Episode 5300\tAverage Score: 5.25\n",
      "Episode 5400\tAverage Score: 4.57\n",
      "Episode 5500\tAverage Score: 5.78\n",
      "Episode 5600\tAverage Score: 5.25\n",
      "Episode 5700\tAverage Score: 3.95\n",
      "Episode 5800\tAverage Score: 4.32\n",
      "Episode 5900\tAverage Score: 4.03\n",
      "Episode 6000\tAverage Score: 4.49\n",
      "Episode 6100\tAverage Score: 4.84\n",
      "Episode 6200\tAverage Score: 4.52\n",
      "Episode 6300\tAverage Score: 4.05\n",
      "Episode 6400\tAverage Score: 4.76\n",
      "Episode 6500\tAverage Score: 4.14\n",
      "Episode 6518\tAverage Score: 4.31"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bbed5fe09858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                \u001b[0mnum_recall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                skip_frame=1)\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# dqn.play()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged, early_stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0256f0ccf927>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_step_to_learn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0256f0ccf927>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_Q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/nn_wrapper.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('Breakout-ram-v0')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=int(1e6),\n",
    "      alpha=5e-4, \n",
    "      gamma=.95,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/Breakout-ram-v0-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.9999, \n",
    "               num_experience=20480, \n",
    "               num_recall=1024,\n",
    "               skip_frame=1)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -10.37\n",
      "Episode 200\tAverage Score: -10.57\n",
      "Episode 300\tAverage Score: -10.79\n",
      "Episode 366\tAverage Score: -10.72"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-ecc048f8e1fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                \u001b[0mnum_recall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                skip_frame=2)\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# dqn.play()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research-ai/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged, early_stop)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_mean_episode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-0256f0ccf927>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('Pong-ram-v0')\n",
    "dqn = DQN(env, \n",
    "      num_mean_episode=100,\n",
    "      num_episodes=int(1e6),\n",
    "      alpha=5e-4, \n",
    "      gamma=.99,\n",
    "      tau=1e-3,\n",
    "      save_name=\"checkpoint/Pong-ram-v0-DQN.pth\")\n",
    "dqn.initialize(num_step_to_learn=4, \n",
    "               eps_start=1, \n",
    "               eps_end=.01, \n",
    "               eps_decay=.995, \n",
    "               num_experience=2048, \n",
    "               num_recall=64,\n",
    "               skip_frame=2)\n",
    "dqn.train(early_stop=lambda mean_reward: mean_reward>200)\n",
    "# dqn.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

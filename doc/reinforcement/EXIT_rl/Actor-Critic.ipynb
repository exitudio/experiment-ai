{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. dennybritz](https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb) <br>\n",
    "[2. original](https://gist.github.com/Ashioto/10ec680395db48ddac1ad848f5f7382c#file-actorcritic-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.base import Base\n",
    "from EXITrl.helpers import print_weight_size\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C [Continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]\n",
    "state = env.reset()\n",
    "featurize_state(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class GaussianPolicyModel(nn.Module):\n",
    "    \"\"\"TODO add to device()\"\"\"\n",
    "    def __init__(self, num_input=2, num_output=1):\n",
    "        super().__init__()\n",
    "        # 1. mu\n",
    "        self.model_mu = torch.nn.Linear(num_input, num_output)\n",
    "        # 2. sigma\n",
    "        self.model_sigma = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input, num_output),\n",
    "            torch.nn.Softplus() # make it positive, be careful threshold=20???\n",
    "        )\n",
    "        self.sigma = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.model_mu(x)\n",
    "        sigma = self.model_sigma(x)\n",
    "        return mu, self.sigma\n",
    "policy = GaussianPolicyModel(2)\n",
    "optimizer_sigma = torch.optim.Adam(policy.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 10\tAverage Score: 25.02 \tother{'sigma': 0.6973568802000003}\n",
      "Episode 20\tAverage Score: 66.81 \tother{'sigma': 0.24315330918113873}\n",
      "Episode 30\tAverage Score: 89.11 \tother{'sigma': 0.08478231655043249}\n",
      "Episode 40\tAverage Score: 20.82 \tother{'sigma': 0.029561765882869215}\n",
      "Episode 50\tAverage Score: 58.81 \tother{'sigma': 0.01030755041464024}}\n"
     ]
    }
   ],
   "source": [
    "class REINFORCE_continuous(ApproxVBase, ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        num_feature = 400\n",
    "        model_q = torch.nn.Linear(num_feature, 1)\n",
    "        ApproxVBase.initialize(self, learning_rate_name=\"beta\", model=model_q)\n",
    "        self.model_g = GaussianPolicyModel(num_feature, self.num_action)\n",
    "        ApproxPolicyBase.initialize(self, \n",
    "                                    learning_rate_name=\"alpha\", \n",
    "                                    model=self.model_g)\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        state = featurize_state(state)\n",
    "        # decay sigma to have less exploration\n",
    "        self.model_g.sigma *= 0.9\n",
    "        self.additional_log['sigma'] = self.model_g.sigma\n",
    "        for i in range(1000):\n",
    "            action, log_prob, entropy = self.policy(state)\n",
    "#             print('\\raction:',action,'***********' , end=\"\")\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _state = featurize_state(_state)\n",
    "#             self.env.render()\n",
    "            \n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.get_v(_state)\n",
    "            estimate_v = self.get_v(state)\n",
    "            td_error = td_target - estimate_v\n",
    "            \n",
    "            self.update_v(td_target, self.get_v(state))\n",
    "            loss = (-log_prob) * td_error # using the td error as our advantage estimate\n",
    "            self.update_policy(loss)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: return total_reward\n",
    "\n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "s = REINFORCE_continuous(env, \n",
    "               num_episodes=50,\n",
    "               policy=\"gaussian_policy\",\n",
    "               alpha=0.0001, \n",
    "               beta=0.001,\n",
    "               gamma=.99)\n",
    "s.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

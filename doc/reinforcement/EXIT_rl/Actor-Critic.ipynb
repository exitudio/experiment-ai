{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "from gridworld_env_2d_state import GridworldEnv2DState\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.approx_v_base import ApproxVBase\n",
    "from EXITrl.approx_policy_base import ApproxPolicyBase\n",
    "from EXITrl.table_base import TableBase\n",
    "from EXITrl.base import Base\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 9.0\n",
      "episode: 1 reward: 15.0\n",
      "episode: 2 reward: 11.0\n",
      "episode: 3 reward: 14.0\n",
      "episode: 4 reward: 13.0\n",
      "episode: 5 reward: 14.0\n",
      "episode: 6 reward: 8.0\n",
      "episode: 7 reward: 9.0\n",
      "episode: 8 reward: 12.0\n",
      "episode: 9 reward: 12.0\n",
      "episode: 10 reward: 10.0\n",
      "episode: 11 reward: 9.0\n",
      "episode: 12 reward: 12.0\n",
      "episode: 13 reward: 9.0\n",
      "episode: 14 reward: 9.0\n",
      "episode: 15 reward: 10.0\n",
      "episode: 16 reward: 9.0\n",
      "episode: 17 reward: 9.0\n",
      "episode: 18 reward: 9.0\n",
      "episode: 19 reward: 11.0\n",
      "episode: 20 reward: 9.0\n",
      "episode: 21 reward: 12.0\n",
      "episode: 22 reward: 12.0\n",
      "episode: 23 reward: 10.0\n",
      "episode: 24 reward: 9.0\n",
      "episode: 25 reward: 10.0\n",
      "episode: 26 reward: 10.0\n",
      "episode: 27 reward: 10.0\n",
      "episode: 28 reward: 9.0\n",
      "episode: 29 reward: 9.0\n",
      "episode: 30 reward: 12.0\n",
      "episode: 31 reward: 11.0\n",
      "episode: 32 reward: 9.0\n",
      "episode: 33 reward: 9.0\n",
      "episode: 34 reward: 10.0\n",
      "episode: 35 reward: 10.0\n",
      "episode: 36 reward: 9.0\n",
      "episode: 37 reward: 10.0\n",
      "episode: 38 reward: 8.0\n",
      "episode: 39 reward: 9.0\n",
      "episode: 40 reward: 11.0\n",
      "episode: 41 reward: 10.0\n",
      "episode: 42 reward: 10.0\n",
      "episode: 43 reward: 10.0\n",
      "episode: 44 reward: 9.0\n",
      "episode: 45 reward: 10.0\n",
      "episode: 46 reward: 9.0\n",
      "episode: 47 reward: 8.0\n",
      "episode: 48 reward: 9.0\n",
      "episode: 49 reward: 10.0\n",
      "episode: 50 reward: 10.0\n",
      "episode: 51 reward: 9.0\n",
      "episode: 52 reward: 10.0\n",
      "episode: 53 reward: 10.0\n",
      "episode: 54 reward: 9.0\n",
      "episode: 55 reward: 9.0\n",
      "episode: 56 reward: 10.0\n",
      "episode: 57 reward: 10.0\n",
      "episode: 58 reward: 9.0\n",
      "episode: 59 reward: 9.0\n",
      "episode: 60 reward: 10.0\n",
      "episode: 61 reward: 12.0\n",
      "episode: 62 reward: 10.0\n",
      "episode: 63 reward: 13.0\n",
      "episode: 64 reward: 10.0\n",
      "episode: 65 reward: 10.0\n",
      "episode: 66 reward: 10.0\n",
      "episode: 67 reward: 10.0\n",
      "episode: 68 reward: 10.0\n",
      "episode: 69 reward: 8.0\n",
      "episode: 70 reward: 10.0\n",
      "episode: 71 reward: 9.0\n",
      "episode: 72 reward: 10.0\n",
      "episode: 73 reward: 10.0\n",
      "episode: 74 reward: 10.0\n",
      "episode: 75 reward: 9.0\n",
      "episode: 76 reward: 10.0\n",
      "episode: 77 reward: 10.0\n",
      "episode: 78 reward: 9.0\n",
      "episode: 79 reward: 8.0\n",
      "episode: 80 reward: 10.0\n",
      "episode: 81 reward: 8.0\n",
      "episode: 82 reward: 11.0\n",
      "episode: 83 reward: 9.0\n",
      "episode: 84 reward: 9.0\n",
      "episode: 85 reward: 9.0\n",
      "episode: 86 reward: 10.0\n",
      "episode: 87 reward: 9.0\n",
      "episode: 88 reward: 10.0\n",
      "episode: 89 reward: 8.0\n",
      "episode: 90 reward: 10.0\n",
      "episode: 91 reward: 10.0\n",
      "episode: 92 reward: 10.0\n",
      "episode: 93 reward: 9.0\n",
      "episode: 94 reward: 9.0\n",
      "episode: 95 reward: 9.0\n",
      "episode: 96 reward: 10.0\n",
      "episode: 97 reward: 8.0\n",
      "episode: 98 reward: 9.0\n",
      "episode: 99 reward: 10.0\n",
      "episode: 100 reward: 9.0\n",
      "episode: 101 reward: 10.0\n",
      "episode: 102 reward: 10.0\n",
      "episode: 103 reward: 8.0\n",
      "episode: 104 reward: 12.0\n",
      "episode: 105 reward: 9.0\n",
      "episode: 106 reward: 9.0\n",
      "episode: 107 reward: 10.0\n",
      "episode: 108 reward: 10.0\n",
      "episode: 109 reward: 9.0\n",
      "episode: 110 reward: 11.0\n",
      "episode: 111 reward: 10.0\n",
      "episode: 112 reward: 10.0\n",
      "episode: 113 reward: 9.0\n",
      "episode: 114 reward: 11.0\n",
      "episode: 115 reward: 9.0\n",
      "episode: 116 reward: 11.0\n",
      "episode: 117 reward: 12.0\n",
      "episode: 118 reward: 10.0\n",
      "episode: 119 reward: 11.0\n",
      "episode: 120 reward: 10.0\n",
      "episode: 121 reward: 10.0\n",
      "episode: 122 reward: 8.0\n",
      "episode: 123 reward: 9.0\n",
      "episode: 124 reward: 8.0\n",
      "episode: 125 reward: 10.0\n",
      "episode: 126 reward: 10.0\n",
      "episode: 127 reward: 9.0\n",
      "episode: 128 reward: 10.0\n",
      "episode: 129 reward: 9.0\n",
      "episode: 130 reward: 9.0\n",
      "episode: 131 reward: 8.0\n",
      "episode: 132 reward: 11.0\n",
      "episode: 133 reward: 10.0\n",
      "episode: 134 reward: 10.0\n",
      "episode: 135 reward: 9.0\n",
      "episode: 136 reward: 9.0\n",
      "episode: 137 reward: 10.0\n",
      "episode: 138 reward: 10.0\n",
      "episode: 139 reward: 11.0\n",
      "episode: 140 reward: 10.0\n",
      "episode: 141 reward: 10.0\n",
      "episode: 142 reward: 9.0\n",
      "episode: 143 reward: 10.0\n",
      "episode: 144 reward: 9.0\n",
      "episode: 145 reward: 10.0\n",
      "episode: 146 reward: 10.0\n",
      "episode: 147 reward: 10.0\n",
      "episode: 148 reward: 11.0\n",
      "episode: 149 reward: 10.0\n",
      "episode: 150 reward: 8.0\n",
      "episode: 151 reward: 8.0\n",
      "episode: 152 reward: 9.0\n",
      "episode: 153 reward: 11.0\n",
      "episode: 154 reward: 9.0\n",
      "episode: 155 reward: 9.0\n",
      "episode: 156 reward: 10.0\n",
      "episode: 157 reward: 8.0\n",
      "episode: 158 reward: 10.0\n",
      "episode: 159 reward: 9.0\n",
      "episode: 160 reward: 9.0\n",
      "episode: 161 reward: 10.0\n",
      "episode: 162 reward: 8.0\n",
      "episode: 163 reward: 9.0\n",
      "episode: 164 reward: 9.0\n",
      "episode: 165 reward: 11.0\n",
      "episode: 166 reward: 10.0\n",
      "episode: 167 reward: 9.0\n",
      "episode: 168 reward: 9.0\n",
      "episode: 169 reward: 8.0\n",
      "episode: 170 reward: 10.0\n",
      "episode: 171 reward: 10.0\n",
      "episode: 172 reward: 10.0\n",
      "episode: 173 reward: 9.0\n",
      "episode: 174 reward: 9.0\n",
      "episode: 175 reward: 9.0\n",
      "episode: 176 reward: 9.0\n",
      "episode: 177 reward: 10.0\n",
      "episode: 178 reward: 10.0\n",
      "episode: 179 reward: 10.0\n",
      "episode: 180 reward: 9.0\n",
      "episode: 181 reward: 8.0\n",
      "episode: 182 reward: 10.0\n",
      "episode: 183 reward: 10.0\n",
      "episode: 184 reward: 10.0\n",
      "episode: 185 reward: 10.0\n",
      "episode: 186 reward: 9.0\n",
      "episode: 187 reward: 8.0\n",
      "episode: 188 reward: 9.0\n",
      "episode: 189 reward: 10.0\n",
      "episode: 190 reward: 10.0\n",
      "episode: 191 reward: 10.0\n",
      "episode: 192 reward: 9.0\n",
      "episode: 193 reward: 9.0\n",
      "episode: 194 reward: 10.0\n",
      "episode: 195 reward: 9.0\n",
      "episode: 196 reward: 9.0\n",
      "episode: 197 reward: 9.0\n",
      "episode: 198 reward: 10.0\n",
      "episode: 199 reward: 9.0\n",
      "episode: 200 reward: 8.0\n",
      "episode: 201 reward: 8.0\n",
      "episode: 202 reward: 8.0\n",
      "episode: 203 reward: 9.0\n",
      "episode: 204 reward: 9.0\n",
      "episode: 205 reward: 10.0\n",
      "episode: 206 reward: 10.0\n",
      "episode: 207 reward: 8.0\n",
      "episode: 208 reward: 9.0\n",
      "episode: 209 reward: 9.0\n",
      "episode: 210 reward: 10.0\n",
      "episode: 211 reward: 10.0\n",
      "episode: 212 reward: 8.0\n",
      "episode: 213 reward: 10.0\n",
      "episode: 214 reward: 8.0\n",
      "episode: 215 reward: 9.0\n",
      "episode: 216 reward: 9.0\n",
      "episode: 217 reward: 9.0\n",
      "episode: 218 reward: 11.0\n",
      "episode: 219 reward: 8.0\n",
      "episode: 220 reward: 9.0\n",
      "episode: 221 reward: 9.0\n",
      "episode: 222 reward: 10.0\n",
      "episode: 223 reward: 10.0\n",
      "episode: 224 reward: 10.0\n",
      "episode: 225 reward: 9.0\n",
      "episode: 226 reward: 9.0\n",
      "episode: 227 reward: 9.0\n",
      "episode: 228 reward: 10.0\n",
      "episode: 229 reward: 9.0\n",
      "episode: 230 reward: 9.0\n",
      "episode: 231 reward: 8.0\n",
      "episode: 232 reward: 9.0\n",
      "episode: 233 reward: 11.0\n",
      "episode: 234 reward: 8.0\n",
      "episode: 235 reward: 8.0\n",
      "episode: 236 reward: 10.0\n",
      "episode: 237 reward: 9.0\n",
      "episode: 238 reward: 9.0\n",
      "episode: 239 reward: 9.0\n",
      "episode: 240 reward: 8.0\n",
      "episode: 241 reward: 10.0\n",
      "episode: 242 reward: 9.0\n",
      "episode: 243 reward: 11.0\n",
      "episode: 244 reward: 9.0\n",
      "episode: 245 reward: 10.0\n",
      "episode: 246 reward: 8.0\n",
      "episode: 247 reward: 9.0\n",
      "episode: 248 reward: 8.0\n",
      "episode: 249 reward: 10.0\n",
      "episode: 250 reward: 9.0\n",
      "episode: 251 reward: 10.0\n",
      "episode: 252 reward: 10.0\n",
      "episode: 253 reward: 9.0\n",
      "episode: 254 reward: 9.0\n",
      "episode: 255 reward: 9.0\n",
      "episode: 256 reward: 9.0\n",
      "episode: 257 reward: 9.0\n",
      "episode: 258 reward: 9.0\n",
      "episode: 259 reward: 10.0\n",
      "episode: 260 reward: 10.0\n",
      "episode: 261 reward: 9.0\n",
      "episode: 262 reward: 9.0\n",
      "episode: 263 reward: 10.0\n",
      "episode: 264 reward: 10.0\n",
      "episode: 265 reward: 10.0\n",
      "episode: 266 reward: 9.0\n",
      "episode: 267 reward: 9.0\n",
      "episode: 268 reward: 8.0\n",
      "episode: 269 reward: 10.0\n",
      "episode: 270 reward: 10.0\n",
      "episode: 271 reward: 8.0\n",
      "episode: 272 reward: 8.0\n",
      "episode: 273 reward: 10.0\n",
      "episode: 274 reward: 9.0\n",
      "episode: 275 reward: 8.0\n",
      "episode: 276 reward: 9.0\n",
      "episode: 277 reward: 8.0\n",
      "episode: 278 reward: 10.0\n",
      "episode: 279 reward: 10.0\n",
      "episode: 280 reward: 9.0\n",
      "episode: 281 reward: 10.0\n",
      "episode: 282 reward: 8.0\n",
      "episode: 283 reward: 9.0\n",
      "episode: 284 reward: 9.0\n",
      "episode: 285 reward: 10.0\n",
      "episode: 286 reward: 8.0\n",
      "episode: 287 reward: 10.0\n",
      "episode: 288 reward: 10.0\n",
      "episode: 289 reward: 10.0\n",
      "episode: 290 reward: 11.0\n",
      "episode: 291 reward: 10.0\n",
      "episode: 292 reward: 8.0\n",
      "episode: 293 reward: 10.0\n",
      "episode: 294 reward: 8.0\n",
      "episode: 295 reward: 11.0\n",
      "episode: 296 reward: 10.0\n",
      "episode: 297 reward: 9.0\n",
      "episode: 298 reward: 8.0\n",
      "episode: 299 reward: 9.0\n",
      "episode: 300 reward: 10.0\n",
      "episode: 301 reward: 9.0\n",
      "episode: 302 reward: 10.0\n",
      "episode: 303 reward: 10.0\n",
      "episode: 304 reward: 9.0\n",
      "episode: 305 reward: 9.0\n",
      "episode: 306 reward: 10.0\n",
      "episode: 307 reward: 8.0\n",
      "episode: 308 reward: 10.0\n",
      "episode: 309 reward: 9.0\n",
      "episode: 310 reward: 9.0\n",
      "episode: 311 reward: 9.0\n",
      "episode: 312 reward: 9.0\n",
      "episode: 313 reward: 10.0\n",
      "episode: 314 reward: 8.0\n",
      "episode: 315 reward: 10.0\n",
      "episode: 316 reward: 10.0\n",
      "episode: 317 reward: 9.0\n",
      "episode: 318 reward: 9.0\n",
      "episode: 319 reward: 10.0\n",
      "episode: 320 reward: 9.0\n",
      "episode: 321 reward: 9.0\n",
      "episode: 322 reward: 9.0\n",
      "episode: 323 reward: 10.0\n",
      "episode: 324 reward: 9.0\n",
      "episode: 325 reward: 8.0\n",
      "episode: 326 reward: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 327 reward: 10.0\n",
      "episode: 328 reward: 9.0\n",
      "episode: 329 reward: 9.0\n",
      "episode: 330 reward: 9.0\n",
      "episode: 331 reward: 10.0\n",
      "episode: 332 reward: 10.0\n",
      "episode: 333 reward: 10.0\n",
      "episode: 334 reward: 10.0\n",
      "episode: 335 reward: 10.0\n",
      "episode: 336 reward: 10.0\n",
      "episode: 337 reward: 9.0\n",
      "episode: 338 reward: 9.0\n",
      "episode: 339 reward: 10.0\n",
      "episode: 340 reward: 10.0\n",
      "episode: 341 reward: 10.0\n",
      "episode: 342 reward: 9.0\n",
      "episode: 343 reward: 10.0\n",
      "episode: 344 reward: 9.0\n",
      "episode: 345 reward: 9.0\n",
      "episode: 346 reward: 8.0\n",
      "episode: 347 reward: 8.0\n",
      "episode: 348 reward: 9.0\n",
      "episode: 349 reward: 10.0\n",
      "episode: 350 reward: 9.0\n",
      "episode: 351 reward: 10.0\n",
      "episode: 352 reward: 10.0\n",
      "episode: 353 reward: 10.0\n",
      "episode: 354 reward: 9.0\n",
      "episode: 355 reward: 10.0\n",
      "episode: 356 reward: 10.0\n",
      "episode: 357 reward: 10.0\n",
      "episode: 358 reward: 9.0\n",
      "episode: 359 reward: 9.0\n",
      "episode: 360 reward: 10.0\n",
      "episode: 361 reward: 9.0\n",
      "episode: 362 reward: 9.0\n",
      "episode: 363 reward: 10.0\n",
      "episode: 364 reward: 10.0\n",
      "episode: 365 reward: 8.0\n",
      "episode: 366 reward: 10.0\n",
      "episode: 367 reward: 8.0\n",
      "episode: 368 reward: 9.0\n",
      "episode: 369 reward: 10.0\n",
      "episode: 370 reward: 8.0\n",
      "episode: 371 reward: 10.0\n",
      "episode: 372 reward: 10.0\n",
      "episode: 373 reward: 9.0\n",
      "episode: 374 reward: 9.0\n",
      "episode: 375 reward: 9.0\n",
      "episode: 376 reward: 10.0\n",
      "episode: 377 reward: 9.0\n",
      "episode: 378 reward: 10.0\n",
      "episode: 379 reward: 11.0\n",
      "episode: 380 reward: 8.0\n",
      "episode: 381 reward: 10.0\n",
      "episode: 382 reward: 9.0\n",
      "episode: 383 reward: 10.0\n",
      "episode: 384 reward: 9.0\n",
      "episode: 385 reward: 10.0\n",
      "episode: 386 reward: 9.0\n",
      "episode: 387 reward: 9.0\n",
      "episode: 388 reward: 9.0\n",
      "episode: 389 reward: 9.0\n",
      "episode: 390 reward: 9.0\n",
      "episode: 391 reward: 8.0\n",
      "episode: 392 reward: 9.0\n",
      "episode: 393 reward: 10.0\n",
      "episode: 394 reward: 9.0\n",
      "episode: 395 reward: 9.0\n",
      "episode: 396 reward: 10.0\n",
      "episode: 397 reward: 8.0\n",
      "episode: 398 reward: 9.0\n",
      "episode: 399 reward: 10.0\n",
      "episode: 400 reward: 10.0\n",
      "episode: 401 reward: 9.0\n",
      "episode: 402 reward: 8.0\n",
      "episode: 403 reward: 10.0\n",
      "episode: 404 reward: 10.0\n",
      "episode: 405 reward: 10.0\n",
      "episode: 406 reward: 10.0\n",
      "episode: 407 reward: 9.0\n",
      "episode: 408 reward: 9.0\n",
      "episode: 409 reward: 10.0\n",
      "episode: 410 reward: 10.0\n",
      "episode: 411 reward: 9.0\n",
      "episode: 412 reward: 10.0\n",
      "episode: 413 reward: 8.0\n",
      "episode: 414 reward: 10.0\n",
      "episode: 415 reward: 10.0\n",
      "episode: 416 reward: 9.0\n",
      "episode: 417 reward: 10.0\n",
      "episode: 418 reward: 10.0\n",
      "episode: 419 reward: 9.0\n",
      "episode: 420 reward: 11.0\n",
      "episode: 421 reward: 11.0\n",
      "episode: 422 reward: 11.0\n",
      "episode: 423 reward: 9.0\n",
      "episode: 424 reward: 9.0\n",
      "episode: 425 reward: 9.0\n",
      "episode: 426 reward: 9.0\n",
      "episode: 427 reward: 11.0\n",
      "episode: 428 reward: 10.0\n",
      "episode: 429 reward: 8.0\n",
      "episode: 430 reward: 10.0\n",
      "episode: 431 reward: 10.0\n",
      "episode: 432 reward: 10.0\n",
      "episode: 433 reward: 9.0\n",
      "episode: 434 reward: 10.0\n",
      "episode: 435 reward: 9.0\n",
      "episode: 436 reward: 11.0\n",
      "episode: 437 reward: 10.0\n",
      "episode: 438 reward: 9.0\n",
      "episode: 439 reward: 9.0\n",
      "episode: 440 reward: 9.0\n",
      "episode: 441 reward: 9.0\n",
      "episode: 442 reward: 10.0\n",
      "episode: 443 reward: 9.0\n",
      "episode: 444 reward: 10.0\n",
      "episode: 445 reward: 9.0\n",
      "episode: 446 reward: 10.0\n",
      "episode: 447 reward: 9.0\n",
      "episode: 448 reward: 8.0\n",
      "episode: 449 reward: 9.0\n",
      "episode: 450 reward: 10.0\n",
      "episode: 451 reward: 10.0\n",
      "episode: 452 reward: 10.0\n",
      "episode: 453 reward: 9.0\n",
      "episode: 454 reward: 9.0\n",
      "episode: 455 reward: 10.0\n",
      "episode: 456 reward: 10.0\n",
      "episode: 457 reward: 10.0\n",
      "episode: 458 reward: 9.0\n",
      "episode: 459 reward: 9.0\n",
      "episode: 460 reward: 10.0\n",
      "episode: 461 reward: 10.0\n",
      "episode: 462 reward: 10.0\n",
      "episode: 463 reward: 9.0\n",
      "episode: 464 reward: 9.0\n",
      "episode: 465 reward: 10.0\n",
      "episode: 466 reward: 10.0\n",
      "episode: 467 reward: 9.0\n",
      "episode: 468 reward: 9.0\n",
      "episode: 469 reward: 10.0\n",
      "episode: 470 reward: 10.0\n",
      "episode: 471 reward: 10.0\n",
      "episode: 472 reward: 8.0\n",
      "episode: 473 reward: 10.0\n",
      "episode: 474 reward: 10.0\n",
      "episode: 475 reward: 10.0\n",
      "episode: 476 reward: 9.0\n",
      "episode: 477 reward: 10.0\n",
      "episode: 478 reward: 10.0\n",
      "episode: 479 reward: 10.0\n",
      "episode: 480 reward: 9.0\n",
      "episode: 481 reward: 10.0\n",
      "episode: 482 reward: 9.0\n",
      "episode: 483 reward: 9.0\n",
      "episode: 484 reward: 9.0\n",
      "episode: 485 reward: 9.0\n",
      "episode: 486 reward: 10.0\n",
      "episode: 487 reward: 9.0\n",
      "episode: 488 reward: 9.0\n",
      "episode: 489 reward: 9.0\n",
      "episode: 490 reward: 9.0\n",
      "episode: 491 reward: 10.0\n",
      "episode: 492 reward: 9.0\n",
      "episode: 493 reward: 10.0\n",
      "episode: 494 reward: 10.0\n",
      "episode: 495 reward: 10.0\n",
      "episode: 496 reward: 11.0\n",
      "episode: 497 reward: 10.0\n",
      "episode: 498 reward: 10.0\n",
      "episode: 499 reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "class ActorCriticQ(ApproxVBase, ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hidden = 8\n",
    "        features = torch.nn.Linear(self.num_state, hidden)\n",
    "        model_q = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        model_policy = torch.nn.Sequential(\n",
    "            features,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, hidden),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, self.num_action),\n",
    "        ).to(self.device)\n",
    "        ApproxVBase.initialize(self, learning_rate_name=\"beta\", model=model_q)\n",
    "        ApproxPolicyBase.initialize(self, learning_rate_name=\"alpha\", model=model_policy)\n",
    "    \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        action, log_prob = self.policy(state)\n",
    "        for i in range(1000):\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _action, _log_prob = self.policy(_state)\n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.get_q(_state)[_action]\n",
    "    \n",
    "            # policy improvement\n",
    "            # 1. Sutton's book\n",
    "#             loss = (-log_prob) * self.gamma**i\n",
    "            # 2. David's slide\n",
    "            loss = (-log_prob) * self.get_q(state)[action]\n",
    "            self.update_policy(loss)\n",
    "            \n",
    "            # value evaluation\n",
    "#             td_error = td_target - self.get_q(state, action)\n",
    "#             self.Q[state, action] += self.alpha * td_error\n",
    "            current_q = self.get_q(state)[action]\n",
    "            self.update_q(td_target, current_q)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            action = _action\n",
    "            log_prob = _log_prob\n",
    "            if done: return total_reward\n",
    "\n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('CartPole-v1')\n",
    "# env = GridworldEnv2DState()\n",
    "# env = GridworldEnv()\n",
    "s = ActorCriticQ(env, \n",
    "               num_episodes=500,\n",
    "               policy=\"softmax_policy\",\n",
    "               alpha=0.007, \n",
    "               beta=0.1,\n",
    "               gamma=.99)\n",
    "s.train(True)\n",
    "# s.convert_Q_to_V()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurizes represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]\n",
    "state = env.reset()\n",
    "featurize_state(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class GaussianPolicyModel(nn.Module):\n",
    "    \"\"\"TODO add to device()\"\"\"\n",
    "    def __init__(self, num_input=2, num_output=1):\n",
    "        super().__init__()\n",
    "        hidden=8\n",
    "        # 0. feature\n",
    "        self.model_features = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input, hidden),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden, hidden)\n",
    "        )\n",
    "        # 1. mu\n",
    "        self.model_mu = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(hidden, hidden),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, num_output)\n",
    "        )\n",
    "        # 2. sigma\n",
    "        self.model_sigma = torch.nn.Sequential(\n",
    "#             torch.nn.Linear(hidden, hidden),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, num_output),\n",
    "            torch.nn.Softplus() # make it positive, be careful threshold=20???\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model_features(x)\n",
    "        mu = self.model_mu(features)\n",
    "        sigma = self.model_sigma(features)\n",
    "        return mu, sigma\n",
    "policy = GaussianPolicyModel(2)\n",
    "# optimizer_sigma = torch.optim.Adam(policy.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/epinyoanun/miniconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "episode: 0 reward: 69.55597572101787\n",
      "episode: 1 reward: 82.89091230924345\n",
      "episode: 2 reward: 82.91766994364545\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-016759607c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                gamma=.99)\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/EXIT/tutorial/machine_learning/github/jupyter/doc/reinforcement/EXIT_rl/EXITrl/base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_logged)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mtotal_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_logged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-016759607c86>\u001b[0m in \u001b[0;36m_loop\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/envs/classic_control/continuous_mountain_car.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyglet/window/cocoa/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyglet/gl/cocoa.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nscontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflushBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;34m\"\"\"Call the method with the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0;31m# Convert result to python type if it is a instance or class pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class REINFORCE_continuous(ApproxVBase, ApproxPolicyBase):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        hidden = 8\n",
    "        model_q = torch.nn.Sequential(\n",
    "            torch.nn.Linear(400, hidden),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(hidden, hidden),\n",
    "#             torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden, 1),\n",
    "        ).to(self.device)\n",
    "        \n",
    "        ApproxVBase.initialize(self, learning_rate_name=\"beta\", model=model_q)\n",
    "        ApproxPolicyBase.initialize(self, \n",
    "                                    learning_rate_name=\"alpha\", \n",
    "                                    model=GaussianPolicyModel(400, self.num_action))\n",
    "        \n",
    "    def _loop(self, episode) -> int:\n",
    "        done = False\n",
    "        total_reward, reward = 0, 0\n",
    "        state = self.env.reset()\n",
    "        state = featurize_state(state)\n",
    "        for i in range(1000):\n",
    "            action, log_prob, entropy = self.policy(state)\n",
    "#             print('\\raction:',action,'***********' , end=\"\")\n",
    "            _state, reward, done, _ = self.env.step(action)\n",
    "            _state = featurize_state(_state)\n",
    "            self.env.render()\n",
    "            \n",
    "            if done: \n",
    "                td_target = reward\n",
    "            else: \n",
    "                td_target = reward + self.gamma * self.get_v(_state)\n",
    "            estimate_v = self.get_v(state)\n",
    "            td_error = td_target - estimate_v\n",
    "            \n",
    "            self.update_v(td_target, self.get_v(state))\n",
    "            loss = (-log_prob) * td_error # using the td error as our advantage estimate\n",
    "            self.update_policy(loss)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = _state\n",
    "            if done: return total_reward\n",
    "\n",
    "try: env.close()\n",
    "except: pass\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "s = REINFORCE_continuous(env, \n",
    "               num_episodes=1000,\n",
    "               policy=\"gaussian_policy\",\n",
    "               alpha=0.0001, \n",
    "               beta=0.001,\n",
    "               gamma=.99)\n",
    "s.train(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
